# -*- coding: utf-8 -*-
"""Journal Paper-4-Scientific Report-Deepfake Face Detection-Revision R1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e7RjZHbuGjqax0TfXve5z8HdJgQ4r8Dl
"""

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images"
od.download(dataset_url)

!pip install tensorflow

"""# Pretrained Model Comparison Part 1"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from tensorflow.keras.layers import (Input, Dense, GlobalAveragePooling2D,
                                   BatchNormalization, Dropout)
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from tensorflow.keras.applications import (
    VGG16, VGG19, ResNet50, ResNet101,
    InceptionV3, MobileNetV2, DenseNet121, EfficientNetB0
)
from google.colab import drive
from sklearn import metrics

# Mount Google Drive
drive.mount('/content/drive')

# Create results directory in Drive
results_dir = '/content/drive/MyDrive/pretrained_comparison_results'
os.makedirs(results_dir, exist_ok=True)

print("Starting Pretrained Models Comparison")
print("="*50)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'
bs = 64
row, col = 224, 224

# Data generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    brightness_range=[0.9, 1.1]
)

training_set = train_datagen.flow_from_directory(
    path + '/train',
    class_mode='binary',
    shuffle=True,
    target_size=(row, col),
    batch_size=bs
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

validation_set = val_test_datagen.flow_from_directory(
    path + '/valid',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

test_set = val_test_datagen.flow_from_directory(
    path + '/test',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

# PRETRAINED MODEL CONFIGURATIONS
PRETRAINED_MODELS = {
    'VGG16': {
        'model': VGG16,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'VGG19': {
        'model': VGG19,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'ResNet50': {
        'model': ResNet50,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'ResNet101': {
        'model': ResNet101,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'InceptionV3': {
        'model': InceptionV3,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'MobileNetV2': {
        'model': MobileNetV2,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'DenseNet121': {
        'model': DenseNet121,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'EfficientNetB0': {
        'model': EfficientNetB0,
        'input_shape': (224, 224, 3),
        'preprocess': None
    }
}

def create_model_with_pretrained(pretrained_config, model_name, num_classes=2, freeze_base=True):
    """
    Create a model using pretrained backbone with the same classification head
    as the original attention model
    """
    input_shape = pretrained_config['input_shape']

    # Load pretrained model without top layers
    base_model = pretrained_config['model'](
        weights='imagenet',
        include_top=False,
        input_shape=input_shape
    )

    # Freeze or unfreeze base model
    base_model.trainable = not freeze_base

    # Build model with same classification head as attention model
    input_layer = Input(shape=input_shape)
    x = base_model(input_layer, training=False)

    # Same classification head from attention model
    x = GlobalAveragePooling2D()(x)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)

    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=output, name=f'{model_name}_Model')
    return model

def save_results(model_name, history, test_results, model_params):
    """Save all results to Google Drive"""
    model_dir = f'{results_dir}/{model_name.lower()}'
    os.makedirs(model_dir, exist_ok=True)

    # Save history
    history_dict = {
        'accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy'],
        'loss': history.history['loss'],
        'val_loss': history.history['val_loss']
    }

    with open(f'{model_dir}/history.pkl', 'wb') as f:
        pickle.dump(history_dict, f)

    # Save test results and metrics
    results = {
        'model_name': model_name,
        'test_accuracy': test_results['test_accuracy'],
        'test_loss': test_results['test_loss'],
        'best_val_acc': max(history.history['val_accuracy']),
        'best_val_loss': min(history.history['val_loss']),
        'total_params': model_params,
        'trainable_params': test_results.get('trainable_params', 0),
        'roc_auc': test_results.get('roc_auc', 0),
        'precision': test_results.get('precision', 0),
        'recall': test_results.get('recall', 0),
        'f1_score': test_results.get('f1_score', 0)
    }

    with open(f'{model_dir}/results.json', 'w') as f:
        json.dump(results, f, indent=2)

    print(f"Results saved to {model_dir}/")

def evaluate_model_detailed(model, test_set):
    """Detailed evaluation with additional metrics"""
    # Get test generator without shuffle for proper evaluation
    test_generator = val_test_datagen.flow_from_directory(
        path + '/test',
        target_size=(224, 224),
        batch_size=1,
        color_mode='rgb',
        shuffle=False,
        class_mode='binary'
    )

    # Basic evaluation
    test_loss, test_acc = model.evaluate(test_set, verbose=0)

    # Detailed predictions for additional metrics
    y_pred = model.predict(test_generator, verbose=1)
    y_test = test_generator.classes
    y_pred_binary = np.argmax(y_pred, axis=1)

    # Calculate additional metrics
    roc_auc = metrics.roc_auc_score(y_test, y_pred[:, 1])
    precision = metrics.precision_score(y_test, y_pred_binary, average='weighted')
    recall = metrics.recall_score(y_test, y_pred_binary, average='weighted')
    f1_score = metrics.f1_score(y_test, y_pred_binary, average='weighted')

    # Count trainable parameters
    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])

    return {
        'test_accuracy': test_acc,
        'test_loss': test_loss,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'trainable_params': trainable_params
    }

# Store all results for comparison
all_results = []

# Train each pretrained model
for model_name, config in PRETRAINED_MODELS.items():
    print(f"\n{'='*80}")
    print(f"Training Model: {model_name}")
    print(f"{'='*80}\n")

    # Create model directory
    model_dir = f'{results_dir}/{model_name.lower()}'
    os.makedirs(model_dir, exist_ok=True)

    try:
        # Create model
        model = create_model_with_pretrained(config, model_name, freeze_base=True)

        print(f"Model: {model_name}")
        print(f"Total Parameters: {model.count_params():,}")
        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
        print(f"Trainable Parameters: {trainable_params:,}")
        model.summary()

        # Setup callbacks (same for all models)
        checkpoint = ModelCheckpoint(
            filepath=f'{model_dir}/best_model.keras',
            save_best_only=True,
            verbose=1,
            mode='min',
            monitor='val_loss'
        )

        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=4,
            verbose=1,
            min_lr=1e-7
        )

        csv_logger = CSVLogger(f'{model_dir}/training.log')

        early_stopping = EarlyStopping(
            monitor='val_accuracy',
            min_delta=0.001,
            patience=7,
            verbose=1,
            mode='max',
            restore_best_weights=True
        )

        callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

        # Compile model (same optimizer and settings for all)
        optimizer = Adam(learning_rate=1e-4)
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        # Train model (same epochs for all)
        print(f"\nStarting training for {model_name}...")
        history = model.fit(
            training_set,
            validation_data=validation_set,
            epochs=20,
            callbacks=callbacks,
            verbose=1
        )

        # Evaluate model
        print(f"\nEvaluating {model_name}...")
        test_results = evaluate_model_detailed(model, test_set)

        print(f"\n{model_name} Results:")
        print(f"Test Accuracy: {test_results['test_accuracy']:.4f}")
        print(f"Test Loss: {test_results['test_loss']:.4f}")
        print(f"ROC-AUC: {test_results['roc_auc']:.4f}")
        print(f"Precision: {test_results['precision']:.4f}")
        print(f"Recall: {test_results['recall']:.4f}")
        print(f"F1-Score: {test_results['f1_score']:.4f}")

        # Save results
        save_results(model_name, history, test_results, model.count_params())

        # Store for comparison
        all_results.append({
            'model_name': model_name,
            'test_accuracy': test_results['test_accuracy'],
            'test_loss': test_results['test_loss'],
            'roc_auc': test_results['roc_auc'],
            'precision': test_results['precision'],
            'recall': test_results['recall'],
            'f1_score': test_results['f1_score'],
            'total_params': model.count_params(),
            'trainable_params': test_results['trainable_params'],
            'best_val_acc': max(history.history['val_accuracy']),
            'best_val_loss': min(history.history['val_loss'])
        })

        print(f"\n{model_name} training completed and results saved!")

        # Clear memory
        del model
        tf.keras.backend.clear_session()

    except Exception as e:
        print(f"Error training {model_name}: {str(e)}")
        continue

# Create comparison summary
print(f"\n{'='*80}")
print("COMPARISON SUMMARY")
print(f"{'='*80}\n")

comparison_df = pd.DataFrame(all_results)
comparison_df = comparison_df.sort_values('test_accuracy', ascending=False)

print(comparison_df.to_string(index=False))

# Save comparison summary
comparison_df.to_csv(f'{results_dir}/comparison_summary.csv', index=False)
with open(f'{results_dir}/comparison_summary.json', 'w') as f:
    json.dump(all_results, f, indent=2)

print(f"\n\nAll models trained and compared successfully!")
print(f"Results saved in: {results_dir}/")
print(f"Comparison summary saved as: comparison_summary.csv and comparison_summary.json")

"""# Ptrained Model comparison Part 2"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from tensorflow.keras.layers import (Input, Dense, GlobalAveragePooling2D,
                                   BatchNormalization, Dropout)
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from tensorflow.keras.applications import (
    VGG16, VGG19, ResNet50, ResNet101,
    InceptionV3, MobileNetV2, DenseNet121, EfficientNetB0
)
from google.colab import drive
from sklearn import metrics

# Mount Google Drive
drive.mount('/content/drive')

# Create results directory in Drive
results_dir = '/content/drive/MyDrive/pretrained_comparison_results'
os.makedirs(results_dir, exist_ok=True)

print("Starting Pretrained Models Comparison")
print("="*50)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'
bs = 64
row, col = 224, 224

# Data generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    brightness_range=[0.9, 1.1]
)

training_set = train_datagen.flow_from_directory(
    path + '/train',
    class_mode='binary',
    shuffle=True,
    target_size=(row, col),
    batch_size=bs
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

validation_set = val_test_datagen.flow_from_directory(
    path + '/valid',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

test_set = val_test_datagen.flow_from_directory(
    path + '/test',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

# PRETRAINED MODEL CONFIGURATIONS
PRETRAINED_MODELS = {
    'InceptionV3': {
        'model': InceptionV3,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'MobileNetV2': {
        'model': MobileNetV2,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'DenseNet121': {
        'model': DenseNet121,
        'input_shape': (224, 224, 3),
        'preprocess': None
    },
    'EfficientNetB0': {
        'model': EfficientNetB0,
        'input_shape': (224, 224, 3),
        'preprocess': None
    }
}

def create_model_with_pretrained(pretrained_config, model_name, num_classes=2, freeze_base=True):
    """
    Create a model using pretrained backbone with the same classification head
    as the original attention model
    """
    input_shape = pretrained_config['input_shape']

    # Load pretrained model without top layers
    base_model = pretrained_config['model'](
        weights='imagenet',
        include_top=False,
        input_shape=input_shape
    )

    # Freeze or unfreeze base model
    base_model.trainable = not freeze_base

    # Build model with same classification head as attention model
    input_layer = Input(shape=input_shape)
    x = base_model(input_layer, training=False)

    # Same classification head from attention model
    x = GlobalAveragePooling2D()(x)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)

    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=output, name=f'{model_name}_Model')
    return model

def save_results(model_name, history, test_results, model_params):
    """Save all results to Google Drive"""
    model_dir = f'{results_dir}/{model_name.lower()}'
    os.makedirs(model_dir, exist_ok=True)

    # Save history
    history_dict = {
        'accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy'],
        'loss': history.history['loss'],
        'val_loss': history.history['val_loss']
    }

    with open(f'{model_dir}/history.pkl', 'wb') as f:
        pickle.dump(history_dict, f)

    # Save test results and metrics
    results = {
        'model_name': model_name,
        'test_accuracy': test_results['test_accuracy'],
        'test_loss': test_results['test_loss'],
        'best_val_acc': max(history.history['val_accuracy']),
        'best_val_loss': min(history.history['val_loss']),
        'total_params': model_params,
        'trainable_params': test_results.get('trainable_params', 0),
        'roc_auc': test_results.get('roc_auc', 0),
        'precision': test_results.get('precision', 0),
        'recall': test_results.get('recall', 0),
        'f1_score': test_results.get('f1_score', 0)
    }

    with open(f'{model_dir}/results.json', 'w') as f:
        json.dump(results, f, indent=2)

    print(f"Results saved to {model_dir}/")

def evaluate_model_detailed(model, test_set):
    """Detailed evaluation with additional metrics"""
    # Get test generator without shuffle for proper evaluation
    test_generator = val_test_datagen.flow_from_directory(
        path + '/test',
        target_size=(224, 224),
        batch_size=1,
        color_mode='rgb',
        shuffle=False,
        class_mode='binary'
    )

    # Basic evaluation
    test_loss, test_acc = model.evaluate(test_set, verbose=0)

    # Detailed predictions for additional metrics
    y_pred = model.predict(test_generator, verbose=1)
    y_test = test_generator.classes
    y_pred_binary = np.argmax(y_pred, axis=1)

    # Calculate additional metrics
    roc_auc = metrics.roc_auc_score(y_test, y_pred[:, 1])
    precision = metrics.precision_score(y_test, y_pred_binary, average='weighted')
    recall = metrics.recall_score(y_test, y_pred_binary, average='weighted')
    f1_score = metrics.f1_score(y_test, y_pred_binary, average='weighted')

    # Count trainable parameters
    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])

    return {
        'test_accuracy': test_acc,
        'test_loss': test_loss,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'trainable_params': trainable_params
    }

# Store all results for comparison
all_results = []

# Train each pretrained model
for model_name, config in PRETRAINED_MODELS.items():
    print(f"\n{'='*80}")
    print(f"Training Model: {model_name}")
    print(f"{'='*80}\n")

    # Create model directory
    model_dir = f'{results_dir}/{model_name.lower()}'
    os.makedirs(model_dir, exist_ok=True)

    try:
        # Create model
        model = create_model_with_pretrained(config, model_name, freeze_base=True)

        print(f"Model: {model_name}")
        print(f"Total Parameters: {model.count_params():,}")
        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
        print(f"Trainable Parameters: {trainable_params:,}")
        model.summary()

        # Setup callbacks (same for all models)
        checkpoint = ModelCheckpoint(
            filepath=f'{model_dir}/best_model.keras',
            save_best_only=True,
            verbose=1,
            mode='min',
            monitor='val_loss'
        )

        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=4,
            verbose=1,
            min_lr=1e-7
        )

        csv_logger = CSVLogger(f'{model_dir}/training.log')

        early_stopping = EarlyStopping(
            monitor='val_accuracy',
            min_delta=0.001,
            patience=7,
            verbose=1,
            mode='max',
            restore_best_weights=True
        )

        callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

        # Compile model (same optimizer and settings for all)
        optimizer = Adam(learning_rate=1e-4)
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        # Train model (same epochs for all)
        print(f"\nStarting training for {model_name}...")
        history = model.fit(
            training_set,
            validation_data=validation_set,
            epochs=20,
            callbacks=callbacks,
            verbose=1
        )

        # Evaluate model
        print(f"\nEvaluating {model_name}...")
        test_results = evaluate_model_detailed(model, test_set)

        print(f"\n{model_name} Results:")
        print(f"Test Accuracy: {test_results['test_accuracy']:.4f}")
        print(f"Test Loss: {test_results['test_loss']:.4f}")
        print(f"ROC-AUC: {test_results['roc_auc']:.4f}")
        print(f"Precision: {test_results['precision']:.4f}")
        print(f"Recall: {test_results['recall']:.4f}")
        print(f"F1-Score: {test_results['f1_score']:.4f}")

        # Save results
        save_results(model_name, history, test_results, model.count_params())

        # Store for comparison
        all_results.append({
            'model_name': model_name,
            'test_accuracy': test_results['test_accuracy'],
            'test_loss': test_results['test_loss'],
            'roc_auc': test_results['roc_auc'],
            'precision': test_results['precision'],
            'recall': test_results['recall'],
            'f1_score': test_results['f1_score'],
            'total_params': model.count_params(),
            'trainable_params': test_results['trainable_params'],
            'best_val_acc': max(history.history['val_accuracy']),
            'best_val_loss': min(history.history['val_loss'])
        })

        print(f"\n{model_name} training completed and results saved!")

        # Clear memory
        del model
        tf.keras.backend.clear_session()

    except Exception as e:
        print(f"Error training {model_name}: {str(e)}")
        continue

# Create comparison summary
print(f"\n{'='*80}")
print("COMPARISON SUMMARY")
print(f"{'='*80}\n")

comparison_df = pd.DataFrame(all_results)
comparison_df = comparison_df.sort_values('test_accuracy', ascending=False)

print(comparison_df.to_string(index=False))

# Save comparison summary
comparison_df.to_csv(f'{results_dir}/comparison_summary.csv', index=False)
with open(f'{results_dir}/comparison_summary.json', 'w') as f:
    json.dump(all_results, f, indent=2)

print(f"\n\nAll models trained and compared successfully!")
print(f"Results saved in: {results_dir}/")
print(f"Comparison summary saved as: comparison_summary.csv and comparison_summary.json")

"""# Ptrained Test

VGG16
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/pretrained_comparison_results/vgg16/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""vgg19"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/pretrained_comparison_results/vgg19/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""resnet50"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/pretrained_comparison_results/resnet50/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""inceptionv3"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/pretrained_comparison_results/inceptionv3/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""MobileNetv2"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/pretrained_comparison_results/mobilenetv2/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""Proposed Model"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/enhanced/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""Ablation study stage 3"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/attention/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""Stage 2"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/batchnorm/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""stage 1"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/baseline/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""# Baseline Model"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from tensorflow.keras.layers import (Input, Dense, Flatten, Conv2D, MaxPooling2D,
                                   BatchNormalization, Dropout)
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from google.colab import drive
from sklearn import metrics

# Mount Google Drive
drive.mount('/content/drive')

# Create results directory in Drive
results_dir = '/content/drive/MyDrive/ablation_study_results'
os.makedirs(results_dir, exist_ok=True)
os.makedirs(f'{results_dir}/baseline', exist_ok=True)

print("Starting Baseline Model Training")
print("="*50)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'
bs = 64
row, col = 224, 224

# Data generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    brightness_range=[0.9, 1.1]
)

training_set = train_datagen.flow_from_directory(
    path + '/train',
    class_mode='binary',
    shuffle=True,
    target_size=(row, col),
    batch_size=bs
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

validation_set = val_test_datagen.flow_from_directory(
    path + '/valid',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

test_set = val_test_datagen.flow_from_directory(
    path + '/test',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

def baseline_model(input_shape=(224, 224, 3), num_classes=2):
    """Baseline model - simple CNN architecture"""
    input_layer = Input(shape=input_shape)

    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)

    x = Flatten()(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)

    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=output, name='BaselineModel')
    return model

def save_results(model_name, history, test_results, model_params):
    """Save all results to Google Drive"""
    # Save history
    history_dict = {
        'accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy'],
        'loss': history.history['loss'],
        'val_loss': history.history['val_loss']
    }

    with open(f'{results_dir}/{model_name.lower()}/history.pkl', 'wb') as f:
        pickle.dump(history_dict, f)

    # Save test results and metrics
    results = {
        'model_name': model_name,
        'test_accuracy': test_results['test_accuracy'],
        'test_loss': test_results['test_loss'],
        'best_val_acc': max(history.history['val_accuracy']),
        'best_val_loss': min(history.history['val_loss']),
        'total_params': model_params,
        'roc_auc': test_results.get('roc_auc', 0),
        'precision': test_results.get('precision', 0),
        'recall': test_results.get('recall', 0),
        'f1_score': test_results.get('f1_score', 0)
    }

    with open(f'{results_dir}/{model_name.lower()}/results.json', 'w') as f:
        json.dump(results, f, indent=2)

    print(f"Results saved to {results_dir}/{model_name.lower()}/")

def evaluate_model_detailed(model, test_set):
    """Detailed evaluation with additional metrics"""
    # Get test generator without shuffle for proper evaluation
    test_generator = val_test_datagen.flow_from_directory(
        path + '/test',
        target_size=(224, 224),
        batch_size=1,
        color_mode='rgb',
        shuffle=False,
        class_mode='binary'
    )

    # Basic evaluation
    test_loss, test_acc = model.evaluate(test_set, verbose=0)

    # Detailed predictions for additional metrics
    y_pred = model.predict(test_generator, verbose=1)
    y_test = test_generator.classes
    y_pred_binary = np.argmax(y_pred, axis=1)

    # Calculate additional metrics
    roc_auc = metrics.roc_auc_score(y_test, y_pred[:, 1])
    precision = metrics.precision_score(y_test, y_pred_binary, average='weighted')
    recall = metrics.recall_score(y_test, y_pred_binary, average='weighted')
    f1_score = metrics.f1_score(y_test, y_pred_binary, average='weighted')

    return {
        'test_accuracy': test_acc,
        'test_loss': test_loss,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score
    }

# Create and train baseline model
model = baseline_model()
model_name = "Baseline"

print(f"Model: {model_name}")
print(f"Total Parameters: {model.count_params():,}")
model.summary()

# Setup callbacks
checkpoint = ModelCheckpoint(
    filepath=f'{results_dir}/baseline/best_model.keras',
    save_best_only=True,
    verbose=1,
    mode='min',
    monitor='val_loss'
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=4,
    verbose=1,
    min_lr=1e-7
)

csv_logger = CSVLogger(f'{results_dir}/baseline/training.log')

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    min_delta=0.001,
    patience=7,
    verbose=1,
    mode='max',
    restore_best_weights=True
)

callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

# Compile model
optimizer = Adam(learning_rate=1e-4)
model.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train model
print("Starting training...")
history = model.fit(
    training_set,
    validation_data=validation_set,
    epochs=20,  # Full epochs for actual training
    callbacks=callbacks,
    verbose=1
)

# Evaluate model
print("Evaluating model...")
test_results = evaluate_model_detailed(model, test_set)

print(f"\nBaseline Model Results:")
print(f"Test Accuracy: {test_results['test_accuracy']:.4f}")
print(f"Test Loss: {test_results['test_loss']:.4f}")
print(f"ROC-AUC: {test_results['roc_auc']:.4f}")
print(f"Precision: {test_results['precision']:.4f}")
print(f"Recall: {test_results['recall']:.4f}")
print(f"F1-Score: {test_results['f1_score']:.4f}")

# Save results
save_results(model_name, history, test_results, model.count_params())

print(f"\n{model_name} model training completed and results saved!")
print(f"Results saved in: {results_dir}/baseline/")

"""# BatchNorm Model"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from tensorflow.keras.layers import (Input, Dense, Flatten, Conv2D, MaxPooling2D,
                                   BatchNormalization, Dropout)
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from google.colab import drive
from sklearn import metrics

# Mount Google Drive
drive.mount('/content/drive')

# Create results directory in Drive
results_dir = '/content/drive/MyDrive/ablation_study_results'
os.makedirs(results_dir, exist_ok=True)
os.makedirs(f'{results_dir}/batchnorm', exist_ok=True)

print("Starting BatchNorm Model Training")
print("="*50)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'
bs = 64
row, col = 224, 224

# Data generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    brightness_range=[0.9, 1.1]
)

training_set = train_datagen.flow_from_directory(
    path + '/train',
    class_mode='binary',
    shuffle=True,
    target_size=(row, col),
    batch_size=bs
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

validation_set = val_test_datagen.flow_from_directory(
    path + '/valid',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

test_set = val_test_datagen.flow_from_directory(
    path + '/test',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

def model_with_batch_norm(input_shape=(224, 224, 3), num_classes=2):
    """Model with batch normalization added"""
    input_layer = Input(shape=input_shape)

    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Flatten()(x)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)

    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=output, name='BatchNormModel')
    return model

def save_results(model_name, history, test_results, model_params):
    """Save all results to Google Drive"""
    # Save history
    history_dict = {
        'accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy'],
        'loss': history.history['loss'],
        'val_loss': history.history['val_loss']
    }

    with open(f'{results_dir}/{model_name.lower()}/history.pkl', 'wb') as f:
        pickle.dump(history_dict, f)

    # Save test results and metrics
    results = {
        'model_name': model_name,
        'test_accuracy': test_results['test_accuracy'],
        'test_loss': test_results['test_loss'],
        'best_val_acc': max(history.history['val_accuracy']),
        'best_val_loss': min(history.history['val_loss']),
        'total_params': model_params,
        'roc_auc': test_results.get('roc_auc', 0),
        'precision': test_results.get('precision', 0),
        'recall': test_results.get('recall', 0),
        'f1_score': test_results.get('f1_score', 0)
    }

    with open(f'{results_dir}/{model_name.lower()}/results.json', 'w') as f:
        json.dump(results, f, indent=2)

    print(f"Results saved to {results_dir}/{model_name.lower()}/")

def evaluate_model_detailed(model, test_set):
    """Detailed evaluation with additional metrics"""
    # Get test generator without shuffle for proper evaluation
    test_generator = val_test_datagen.flow_from_directory(
        path + '/test',
        target_size=(224, 224),
        batch_size=1,
        color_mode='rgb',
        shuffle=False,
        class_mode='binary'
    )

    # Basic evaluation
    test_loss, test_acc = model.evaluate(test_set, verbose=0)

    # Detailed predictions for additional metrics
    y_pred = model.predict(test_generator, verbose=1)
    y_test = test_generator.classes
    y_pred_binary = np.argmax(y_pred, axis=1)

    # Calculate additional metrics
    roc_auc = metrics.roc_auc_score(y_test, y_pred[:, 1])
    precision = metrics.precision_score(y_test, y_pred_binary, average='weighted')
    recall = metrics.recall_score(y_test, y_pred_binary, average='weighted')
    f1_score = metrics.f1_score(y_test, y_pred_binary, average='weighted')

    return {
        'test_accuracy': test_acc,
        'test_loss': test_loss,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score
    }

# Create and train BatchNorm model
model = model_with_batch_norm()
model_name = "BatchNorm"

print(f"Model: {model_name}")
print(f"Total Parameters: {model.count_params():,}")
model.summary()

# Setup callbacks
checkpoint = ModelCheckpoint(
    filepath=f'{results_dir}/batchnorm/best_model.keras',
    save_best_only=True,
    verbose=1,
    mode='min',
    monitor='val_loss'
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=4,
    verbose=1,
    min_lr=1e-7
)

csv_logger = CSVLogger(f'{results_dir}/batchnorm/training.log')

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    min_delta=0.001,
    patience=7,
    verbose=1,
    mode='max',
    restore_best_weights=True
)

callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

# Compile model
optimizer = Adam(learning_rate=1e-4)
model.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train model
print("Starting training...")
history = model.fit(
    training_set,
    validation_data=validation_set,
    epochs=20,  # Full epochs for actual training
    callbacks=callbacks,
    verbose=1
)

# Evaluate model
print("Evaluating model...")
test_results = evaluate_model_detailed(model, test_set)

print(f"\nBatchNorm Model Results:")
print(f"Test Accuracy: {test_results['test_accuracy']:.4f}")
print(f"Test Loss: {test_results['test_loss']:.4f}")
print(f"ROC-AUC: {test_results['roc_auc']:.4f}")
print(f"Precision: {test_results['precision']:.4f}")
print(f"Recall: {test_results['recall']:.4f}")
print(f"F1-Score: {test_results['f1_score']:.4f}")

# Save results
save_results(model_name, history, test_results, model.count_params())

print(f"\n{model_name} model training completed and results saved!")
print(f"Results saved in: {results_dir}/batchnorm/")

"""# Attention Model"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from tensorflow.keras.layers import (Input, Dense, Flatten, Conv2D, MaxPooling2D,
                                   BatchNormalization, Dropout, Reshape, Concatenate,
                                   LeakyReLU, GlobalAveragePooling2D, Multiply, Add,
                                   DepthwiseConv2D, SeparableConv2D, Activation)
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from google.colab import drive
from sklearn import metrics

# Mount Google Drive
drive.mount('/content/drive')

# Create results directory in Drive
results_dir = '/content/drive/MyDrive/ablation_study_results'
os.makedirs(results_dir, exist_ok=True)
os.makedirs(f'{results_dir}/attention', exist_ok=True)

print("Starting Attention Model Training")
print("="*50)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'
bs = 64
row, col = 224, 224

# Data generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    brightness_range=[0.9, 1.1]
)

training_set = train_datagen.flow_from_directory(
    path + '/train',
    class_mode='binary',
    shuffle=True,
    target_size=(row, col),
    batch_size=bs
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

validation_set = val_test_datagen.flow_from_directory(
    path + '/valid',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

test_set = val_test_datagen.flow_from_directory(
    path + '/test',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

# ATTENTION MECHANISMS AND CUSTOM BLOCKS
def squeeze_excite_block(input_tensor, ratio=16):
    """Squeeze-and-Excitation block for channel attention"""
    filters = input_tensor.shape[-1]

    # Squeeze: Global average pooling
    squeeze = GlobalAveragePooling2D()(input_tensor)

    # Excitation: FC -> ReLU -> FC -> Sigmoid
    excitation = Dense(filters // ratio, activation='relu')(squeeze)
    excitation = Dense(filters, activation='sigmoid')(excitation)

    # Scale the input
    excitation = Reshape((1, 1, filters))(excitation)
    scaled = Multiply()([input_tensor, excitation])

    return scaled

def conv_block_with_attention(x, filters, kernel_size=3, use_se=True, use_bn=True, dropout_rate=0.0):
    """Enhanced convolutional block with optional attention and batch normalization"""
    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)

    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)

    if use_se:
        x = squeeze_excite_block(x)

    x = MaxPooling2D((2, 2))(x)

    if dropout_rate > 0:
        x = Dropout(dropout_rate)(x)

    return x

def residual_block_enhanced(x, filters, use_se=True):
    """Enhanced residual block with SE attention"""
    shortcut = Conv2D(filters, (1, 1), padding='same')(x)
    shortcut = BatchNormalization()(shortcut)

    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Conv2D(filters, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)

    if use_se:
        x = squeeze_excite_block(x)

    x = Add()([x, shortcut])
    x = Activation('relu')(x)

    return x

def model_with_attention(input_shape=(224, 224, 3), num_classes=2):
    """Model with Squeeze-and-Excitation attention blocks"""
    input_layer = Input(shape=input_shape)

    x = conv_block_with_attention(input_layer, 64, use_se=True, use_bn=True, dropout_rate=0.1)
    x = conv_block_with_attention(x, 128, use_se=True, use_bn=True, dropout_rate=0.2)
    x = conv_block_with_attention(x, 256, use_se=True, use_bn=True, dropout_rate=0.3)

    # Enhanced residual block
    x = residual_block_enhanced(x, 512, use_se=True)
    x = MaxPooling2D((2, 2))(x)

    # Global average pooling instead of flatten for better generalization
    x = GlobalAveragePooling2D()(x)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)

    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=output, name='AttentionModel')
    return model

def save_results(model_name, history, test_results, model_params):
    """Save all results to Google Drive"""
    # Save history
    history_dict = {
        'accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy'],
        'loss': history.history['loss'],
        'val_loss': history.history['val_loss']
    }

    with open(f'{results_dir}/{model_name.lower()}/history.pkl', 'wb') as f:
        pickle.dump(history_dict, f)

    # Save test results and metrics
    results = {
        'model_name': model_name,
        'test_accuracy': test_results['test_accuracy'],
        'test_loss': test_results['test_loss'],
        'best_val_acc': max(history.history['val_accuracy']),
        'best_val_loss': min(history.history['val_loss']),
        'total_params': model_params,
        'roc_auc': test_results.get('roc_auc', 0),
        'precision': test_results.get('precision', 0),
        'recall': test_results.get('recall', 0),
        'f1_score': test_results.get('f1_score', 0)
    }

    with open(f'{results_dir}/{model_name.lower()}/results.json', 'w') as f:
        json.dump(results, f, indent=2)

    print(f"Results saved to {results_dir}/{model_name.lower()}/")

def evaluate_model_detailed(model, test_set):
    """Detailed evaluation with additional metrics"""
    # Get test generator without shuffle for proper evaluation
    test_generator = val_test_datagen.flow_from_directory(
        path + '/test',
        target_size=(224, 224),
        batch_size=1,
        color_mode='rgb',
        shuffle=False,
        class_mode='binary'
    )

    # Basic evaluation
    test_loss, test_acc = model.evaluate(test_set, verbose=0)

    # Detailed predictions for additional metrics
    y_pred = model.predict(test_generator, verbose=1)
    y_test = test_generator.classes
    y_pred_binary = np.argmax(y_pred, axis=1)

    # Calculate additional metrics
    roc_auc = metrics.roc_auc_score(y_test, y_pred[:, 1])
    precision = metrics.precision_score(y_test, y_pred_binary, average='weighted')
    recall = metrics.recall_score(y_test, y_pred_binary, average='weighted')
    f1_score = metrics.f1_score(y_test, y_pred_binary, average='weighted')

    return {
        'test_accuracy': test_acc,
        'test_loss': test_loss,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score
    }

# Create and train Attention model
model = model_with_attention()
model_name = "Attention"

print(f"Model: {model_name}")
print(f"Total Parameters: {model.count_params():,}")
model.summary()

# Setup callbacks
checkpoint = ModelCheckpoint(
    filepath=f'{results_dir}/attention/best_model.keras',
    save_best_only=True,
    verbose=1,
    mode='min',
    monitor='val_loss'
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=4,
    verbose=1,
    min_lr=1e-7
)

csv_logger = CSVLogger(f'{results_dir}/attention/training.log')

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    min_delta=0.001,
    patience=7,
    verbose=1,
    mode='max',
    restore_best_weights=True
)

callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

# Compile model
optimizer = Adam(learning_rate=1e-4)  # Lower learning rate for attention model
model.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train model
print("Starting training...")
history = model.fit(
    training_set,
    validation_data=validation_set,
    epochs=20,  # Full epochs for actual training
    callbacks=callbacks,
    verbose=1
)

# Evaluate model
print("Evaluating model...")
test_results = evaluate_model_detailed(model, test_set)

print(f"\nAttention Model Results:")
print(f"Test Accuracy: {test_results['test_accuracy']:.4f}")
print(f"Test Loss: {test_results['test_loss']:.4f}")
print(f"ROC-AUC: {test_results['roc_auc']:.4f}")
print(f"Precision: {test_results['precision']:.4f}")
print(f"Recall: {test_results['recall']:.4f}")
print(f"F1-Score: {test_results['f1_score']:.4f}")

# Save results
save_results(model_name, history, test_results, model.count_params())

print(f"\n{model_name} model training completed and results saved!")
print(f"Results saved in: {results_dir}/attention/")

"""# Enhanced Model"""

import os
import numpy as np
import pandas as pd
import pickle
import json
from tensorflow.keras.layers import (Input, Dense, Flatten, Conv2D, MaxPooling2D,
                                   BatchNormalization, Dropout, Reshape, Concatenate,
                                   LeakyReLU, GlobalAveragePooling2D, Multiply, Add,
                                   DepthwiseConv2D, SeparableConv2D, Activation)
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from google.colab import drive
from sklearn import metrics

# Mount Google Drive
drive.mount('/content/drive')

# Create results directory in Drive
results_dir = '/content/drive/MyDrive/ablation_study_results'
os.makedirs(results_dir, exist_ok=True)
os.makedirs(f'{results_dir}/enhanced_full', exist_ok=True)

print("Starting Enhanced Full Model Training")
print("="*50)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'
bs = 64
row, col = 224, 224

# Data generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    brightness_range=[0.9, 1.1]
)

training_set = train_datagen.flow_from_directory(
    path + '/train',
    class_mode='binary',
    shuffle=True,
    target_size=(row, col),
    batch_size=bs
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

validation_set = val_test_datagen.flow_from_directory(
    path + '/valid',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

test_set = val_test_datagen.flow_from_directory(
    path + '/test',
    class_mode='binary',
    shuffle=False,
    target_size=(row, col),
    batch_size=bs
)

# ATTENTION MECHANISMS AND CUSTOM BLOCKS
def squeeze_excite_block(input_tensor, ratio=16):
    """Squeeze-and-Excitation block for channel attention"""
    filters = input_tensor.shape[-1]

    # Squeeze: Global average pooling
    squeeze = GlobalAveragePooling2D()(input_tensor)

    # Excitation: FC -> ReLU -> FC -> Sigmoid
    excitation = Dense(filters // ratio, activation='relu')(squeeze)
    excitation = Dense(filters, activation='sigmoid')(excitation)

    # Scale the input
    excitation = Reshape((1, 1, filters))(excitation)
    scaled = Multiply()([input_tensor, excitation])

    return scaled

def conv_block_with_attention(x, filters, kernel_size=3, use_se=True, use_bn=True, dropout_rate=0.0):
    """Enhanced convolutional block with optional attention and batch normalization"""
    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)

    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)

    if use_se:
        x = squeeze_excite_block(x)

    x = MaxPooling2D((2, 2))(x)

    if dropout_rate > 0:
        x = Dropout(dropout_rate)(x)

    return x

def residual_block_enhanced(x, filters, use_se=True):
    """Enhanced residual block with SE attention"""
    shortcut = Conv2D(filters, (1, 1), padding='same')(x)
    shortcut = BatchNormalization()(shortcut)

    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Conv2D(filters, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)

    if use_se:
        x = squeeze_excite_block(x)

    x = Add()([x, shortcut])
    x = Activation('relu')(x)

    return x

def enhanced_model_full(input_shape=(224, 224, 3), num_classes=2):
    """Full enhanced model with all improvements"""
    input_layer = Input(shape=input_shape)

    # Initial feature extraction with larger kernels
    x = Conv2D(64, (7, 7), strides=2, padding='same', activation='relu')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)

    # Progressive feature learning with attention
    x = conv_block_with_attention(x, 128, use_se=True, use_bn=True, dropout_rate=0.1)
    x = conv_block_with_attention(x, 256, use_se=True, use_bn=True, dropout_rate=0.2)

    # Multiple residual blocks with attention
    x = residual_block_enhanced(x, 512, use_se=True)
    x = residual_block_enhanced(x, 512, use_se=True)
    x = MaxPooling2D((2, 2))(x)

    # High-level feature extraction
    x = residual_block_enhanced(x, 1024, use_se=True)

    # Multi-scale feature fusion
    # Branch 1: Global context
    branch1 = GlobalAveragePooling2D()(x)
    branch1 = Dense(256, activation='relu')(branch1)

    # Branch 2: Local details with different pooling
    branch2 = MaxPooling2D((2, 2))(x)
    branch2 = Flatten()(branch2)
    branch2 = Dense(256, activation='relu')(branch2)

    # Combine branches
    combined = Concatenate()([branch1, branch2])
    combined = BatchNormalization()(combined)
    combined = Dropout(0.5)(combined)

    # Final classification layers
    x = Dense(512, activation='relu')(combined)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)

    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=output, name='EnhancedFullModel')
    return model

def save_results(model_name, history, test_results, model_params):
    """Save all results to Google Drive"""
    # Save history
    history_dict = {
        'accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy'],
        'loss': history.history['loss'],
        'val_loss': history.history['val_loss']
    }

    with open(f'{results_dir}/{model_name.lower()}/history.pkl', 'wb') as f:
        pickle.dump(history_dict, f)

    # Save test results and metrics
    results = {
        'model_name': model_name,
        'test_accuracy': test_results['test_accuracy'],
        'test_loss': test_results['test_loss'],
        'best_val_acc': max(history.history['val_accuracy']),
        'best_val_loss': min(history.history['val_loss']),
        'total_params': model_params,
        'roc_auc': test_results.get('roc_auc', 0),
        'precision': test_results.get('precision', 0),
        'recall': test_results.get('recall', 0),
        'f1_score': test_results.get('f1_score', 0)
    }

    with open(f'{results_dir}/{model_name.lower()}/results.json', 'w') as f:
        json.dump(results, f, indent=2)

    print(f"Results saved to {results_dir}/{model_name.lower()}/")

def evaluate_model_detailed(model, test_set):
    """Detailed evaluation with additional metrics"""
    # Get test generator without shuffle for proper evaluation
    test_generator = val_test_datagen.flow_from_directory(
        path + '/test',
        target_size=(224, 224),
        batch_size=1,
        color_mode='rgb',
        shuffle=False,
        class_mode='binary'
    )

    # Basic evaluation
    test_loss, test_acc = model.evaluate(test_set, verbose=0)

    # Detailed predictions for additional metrics
    y_pred = model.predict(test_generator, verbose=1)
    y_test = test_generator.classes
    y_pred_binary = np.argmax(y_pred, axis=1)

    # Calculate additional metrics
    roc_auc = metrics.roc_auc_score(y_test, y_pred[:, 1])
    precision = metrics.precision_score(y_test, y_pred_binary, average='weighted')
    recall = metrics.recall_score(y_test, y_pred_binary, average='weighted')
    f1_score = metrics.f1_score(y_test, y_pred_binary, average='weighted')

    return {
        'test_accuracy': test_acc,
        'test_loss': test_loss,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score
    }

# Create and train Enhanced Full model
model = enhanced_model_full()
model_name = "Enhanced_Full"

print(f"Model: {model_name}")
print(f"Total Parameters: {model.count_params():,}")
model.summary()

# Setup callbacks
checkpoint = ModelCheckpoint(
    filepath=f'{results_dir}/enhanced_full/best_model.keras',
    save_best_only=True,
    verbose=1,
    mode='min',
    monitor='val_loss'
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=4,
    verbose=1,
    min_lr=1e-7
)

csv_logger = CSVLogger(f'{results_dir}/enhanced_full/training.log')

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    min_delta=0.001,
    patience=7,
    verbose=1,
    mode='max',
    restore_best_weights=True
)

callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

# Compile model
optimizer = Adam(learning_rate=1e-4)  # Lower learning rate for complex model
model.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train model
print("Starting training...")
history = model.fit(
    training_set,
    validation_data=validation_set,
    epochs=20,  # Full epochs for actual training
    callbacks=callbacks,
    verbose=1
)

# Evaluate model
print("Evaluating model...")
test_results = evaluate_model_detailed(model, test_set)

print(f"\nEnhanced Full Model Results:")
print(f"Test Accuracy: {test_results['test_accuracy']:.4f}")
print(f"Test Loss: {test_results['test_loss']:.4f}")
print(f"ROC-AUC: {test_results['roc_auc']:.4f}")
print(f"Precision: {test_results['precision']:.4f}")
print(f"Recall: {test_results['recall']:.4f}")
print(f"F1-Score: {test_results['f1_score']:.4f}")

# Save results
save_results(model_name, history, test_results, model.count_params())

print(f"\n{model_name} model training completed and results saved!")
print(f"Results saved in: {results_dir}/enhanced_full/")

"""# Part 1: Seeds (42,52)"""

import os
import numpy as np
import pandas as pd
import pickle
import json
import random
from tensorflow.keras.layers import (Input, Dense, Flatten, Conv2D, MaxPooling2D,
                                   BatchNormalization, Dropout, Reshape, Concatenate,
                                   LeakyReLU, GlobalAveragePooling2D, Multiply, Add,
                                   DepthwiseConv2D, SeparableConv2D, Activation)
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from google.colab import drive
from sklearn import metrics

# Mount Google Drive
drive.mount('/content/drive')

# Create results directory in Drive
results_dir = '/content/drive/MyDrive/ablation_study_results'
os.makedirs(results_dir, exist_ok=True)
os.makedirs(f'{results_dir}/enhanced_full', exist_ok=True)

print("="*70)
print("PART 1: Training Enhanced Full Model with Seeds 42 and 52")
print("="*70)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'
bs = 64
row, col = 224, 224

# SEED MANAGEMENT
def set_seeds(seed):
    """Set all random seeds for reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"✓ Set all seeds to: {seed}")

# ATTENTION MECHANISMS AND CUSTOM BLOCKS
def squeeze_excite_block(input_tensor, ratio=16):
    """Squeeze-and-Excitation block for channel attention"""
    filters = input_tensor.shape[-1]
    squeeze = GlobalAveragePooling2D()(input_tensor)
    excitation = Dense(filters // ratio, activation='relu')(squeeze)
    excitation = Dense(filters, activation='sigmoid')(excitation)
    excitation = Reshape((1, 1, filters))(excitation)
    scaled = Multiply()([input_tensor, excitation])
    return scaled

def conv_block_with_attention(x, filters, kernel_size=3, use_se=True, use_bn=True, dropout_rate=0.0):
    """Enhanced convolutional block with optional attention and batch normalization"""
    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)
    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)
    if use_se:
        x = squeeze_excite_block(x)
    x = MaxPooling2D((2, 2))(x)
    if dropout_rate > 0:
        x = Dropout(dropout_rate)(x)
    return x

def residual_block_enhanced(x, filters, use_se=True):
    """Enhanced residual block with SE attention"""
    shortcut = Conv2D(filters, (1, 1), padding='same')(x)
    shortcut = BatchNormalization()(shortcut)
    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Conv2D(filters, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)
    if use_se:
        x = squeeze_excite_block(x)
    x = Add()([x, shortcut])
    x = Activation('relu')(x)
    return x

def enhanced_model_full(input_shape=(224, 224, 3), num_classes=2):
    """Full enhanced model with all improvements"""
    input_layer = Input(shape=input_shape)

    # Initial featured extraction with larger kernels
    x = Conv2D(64, (7, 7), strides=2, padding='same', activation='relu')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)

    # Progressive feature learning with attention
    x = conv_block_with_attention(x, 128, use_se=True, use_bn=True, dropout_rate=0.1)
    x = conv_block_with_attention(x, 256, use_se=True, use_bn=True, dropout_rate=0.2)

    # Multiple residual blocks with attention
    x = residual_block_enhanced(x, 512, use_se=True)
    x = residual_block_enhanced(x, 512, use_se=True)
    x = MaxPooling2D((2, 2))(x)

    # High-level feature extraction
    x = residual_block_enhanced(x, 1024, use_se=True)

    # Multi-scale feature fusion
    branch1 = GlobalAveragePooling2D()(x)
    branch1 = Dense(256, activation='relu')(branch1)

    branch2 = MaxPooling2D((2, 2))(x)
    branch2 = Flatten()(branch2)
    branch2 = Dense(256, activation='relu')(branch2)

    combined = Concatenate()([branch1, branch2])
    combined = BatchNormalization()(combined)
    combined = Dropout(0.5)(combined)

    # Final classification layers
    x = Dense(512, activation='relu')(combined)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)

    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=output, name='EnhancedFullModel')
    return model

def evaluate_model_detailed(model, test_set, path):
    """Detailed evaluation with additional metrics"""
    test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(
        path + '/test',
        target_size=(224, 224),
        batch_size=1,
        color_mode='rgb',
        shuffle=False,
        class_mode='binary'
    )

    # Basic evaluation
    test_loss, test_acc = model.evaluate(test_set, verbose=0)

    # Detailed predictions
    y_pred = model.predict(test_generator, verbose=0)
    y_test = test_generator.classes
    y_pred_binary = np.argmax(y_pred, axis=1)

    # Calculate metrics
    roc_auc = metrics.roc_auc_score(y_test, y_pred[:, 1])
    precision = metrics.precision_score(y_test, y_pred_binary, average='weighted')
    recall = metrics.recall_score(y_test, y_pred_binary, average='weighted')
    f1_score = metrics.f1_score(y_test, y_pred_binary, average='weighted')

    # Confusion matrix
    cm = metrics.confusion_matrix(y_test, y_pred_binary)
    tn, fp, fn, tp = cm.ravel()

    # Specificity
    specificity = tn / (tn + fp)

    return {
        'test_accuracy': test_acc,
        'test_loss': test_loss,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'specificity': specificity,
        'true_positives': int(tp),
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn)
    }

def save_individual_run_results(model_name, seed, history, test_results, model_params):
    """Save results for individual run"""
    history_dict = {
        'accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy'],
        'loss': history.history['loss'],
        'val_loss': history.history['val_loss']
    }

    with open(f'{results_dir}/{model_name.lower()}/history_seed_{seed}.pkl', 'wb') as f:
        pickle.dump(history_dict, f)

    results = {
        'seed': seed,
        'model_name': model_name,
        'test_accuracy': test_results['test_accuracy'],
        'test_loss': test_results['test_loss'],
        'best_val_acc': max(history.history['val_accuracy']),
        'best_val_loss': min(history.history['val_loss']),
        'total_params': model_params,
        'roc_auc': test_results['roc_auc'],
        'precision': test_results['precision'],
        'recall': test_results['recall'],
        'f1_score': test_results['f1_score'],
        'specificity': test_results['specificity'],
        'true_positives': test_results['true_positives'],
        'true_negatives': test_results['true_negatives'],
        'false_positives': test_results['false_positives'],
        'false_negatives': test_results['false_negatives']
    }

    with open(f'{results_dir}/{model_name.lower()}/results_seed_{seed}.json', 'w') as f:
        json.dump(results, f, indent=2)

    return results

# ============================================================================
# MAIN TRAINING LOOP - PART 1: SEEDS 42 AND 52
# ============================================================================

SEEDS_PART1 = [42, 52]
model_name = "Enhanced_Full"
all_results = []

# Data generator templates
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    brightness_range=[0.9, 1.1]
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

# Loop through seeds 42 and 52
for run_idx, seed in enumerate(SEEDS_PART1):
    print(f"\n{'='*70}")
    print(f"TRAINING RUN {run_idx + 1}/{len(SEEDS_PART1)} WITH SEED {seed}")
    print(f"{'='*70}\n")

    # Set seed
    set_seeds(seed)

    # Create data generators
    training_set = train_datagen.flow_from_directory(
        path + '/train',
        class_mode='binary',
        shuffle=True,
        target_size=(row, col),
        batch_size=bs,
        seed=seed
    )

    validation_set = val_test_datagen.flow_from_directory(
        path + '/valid',
        class_mode='binary',
        shuffle=False,
        target_size=(row, col),
        batch_size=bs
    )

    test_set = val_test_datagen.flow_from_directory(
        path + '/test',
        class_mode='binary',
        shuffle=False,
        target_size=(row, col),
        batch_size=bs
    )

    # Create model
    model = enhanced_model_full()

    if run_idx == 0:
        print(f"Model: {model_name}")
        print(f"Total Parameters: {model.count_params():,}")
        model.summary()

    # Setup callbacks
    checkpoint = ModelCheckpoint(
        filepath=f'{results_dir}/enhanced_full/best_model_seed_{seed}.keras',
        save_best_only=True,
        verbose=1,
        mode='min',
        monitor='val_loss'
    )

    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=4,
        verbose=1,
        min_lr=1e-7
    )

    csv_logger = CSVLogger(f'{results_dir}/enhanced_full/training_seed_{seed}.log')

    early_stopping = EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.001,
        patience=7,
        verbose=1,
        mode='max',
        restore_best_weights=True
    )

    callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

    # Compile model
    optimizer = Adam(learning_rate=1e-4)
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # Train model
    print(f"\n⏳ Starting training for seed {seed}...")
    history = model.fit(
        training_set,
        validation_data=validation_set,
        epochs=20,
        callbacks=callbacks,
        verbose=1
    )

    # Evaluate model
    print(f"\n📊 Evaluating model for seed {seed}...")
    test_results = evaluate_model_detailed(model, test_set, path)

    # Print results
    print(f"\n{'='*70}")
    print(f"RESULTS FOR SEED {seed}")
    print(f"{'='*70}")
    print(f"Test Accuracy:    {test_results['test_accuracy']*100:.2f}%")
    print(f"Test Loss:        {test_results['test_loss']:.4f}")
    print(f"ROC-AUC:          {test_results['roc_auc']:.4f}")
    print(f"Precision:        {test_results['precision']*100:.2f}%")
    print(f"Recall:           {test_results['recall']*100:.2f}%")
    print(f"Specificity:      {test_results['specificity']*100:.2f}%")
    print(f"F1-Score:         {test_results['f1_score']*100:.2f}%")
    print(f"Best Val Acc:     {max(history.history['val_accuracy'])*100:.2f}%")
    print(f"{'='*70}")

    # Save results
    run_results = save_individual_run_results(
        model_name, seed, history, test_results, model.count_params()
    )
    all_results.append(run_results)

    # Clear memory
    del model
    tf.keras.backend.clear_session()

# Save Part 1 results summary
part1_summary = {
    'part': 1,
    'seeds': SEEDS_PART1,
    'results': all_results
}

with open(f'{results_dir}/enhanced_full/part1_summary.json', 'w') as f:
    json.dump(part1_summary, f, indent=2)

print(f"\n{'='*70}")
print("✅ PART 1 COMPLETED SUCCESSFULLY!")
print(f"{'='*70}")
print(f"Seeds trained: {SEEDS_PART1}")
print(f"Results saved in: {results_dir}/enhanced_full/")
print("\nFiles saved:")
print(f"  - results_seed_42.json")
print(f"  - results_seed_52.json")
print(f"  - best_model_seed_42.keras")
print(f"  - best_model_seed_52.keras")
print(f"  - part1_summary.json")
print(f"\n⚠️  IMPORTANT: Run Part 2 to train seeds 62 and 72")
print(f"{'='*70}")

"""# Part 2: Seeds (62,72)"""

import os
import numpy as np
import pandas as pd
import pickle
import json
import random
from tensorflow.keras.layers import (Input, Dense, Flatten, Conv2D, MaxPooling2D,
                                   BatchNormalization, Dropout, Reshape, Concatenate,
                                   LeakyReLU, GlobalAveragePooling2D, Multiply, Add,
                                   DepthwiseConv2D, SeparableConv2D, Activation)
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from google.colab import drive
from sklearn import metrics

# Mount Google Drive
drive.mount('/content/drive')

# Use existing results directory
results_dir = '/content/drive/MyDrive/ablation_study_results'
os.makedirs(f'{results_dir}/enhanced_full', exist_ok=True)

print("="*70)
print("PART 2: Training Enhanced Full Model with Seeds 62 and 72")
print("="*70)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'
bs = 64
row, col = 224, 224

# SEED MANAGEMENT
def set_seeds(seed):
    """Set all random seeds for reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"✓ Set all seeds to: {seed}")

# ATTENTION MECHANISMS AND CUSTOM BLOCKS
def squeeze_excite_block(input_tensor, ratio=16):
    """Squeeze-and-Excitation block for channel attention"""
    filters = input_tensor.shape[-1]
    squeeze = GlobalAveragePooling2D()(input_tensor)
    excitation = Dense(filters // ratio, activation='relu')(squeeze)
    excitation = Dense(filters, activation='sigmoid')(excitation)
    excitation = Reshape((1, 1, filters))(excitation)
    scaled = Multiply()([input_tensor, excitation])
    return scaled

def conv_block_with_attention(x, filters, kernel_size=3, use_se=True, use_bn=True, dropout_rate=0.0):
    """Enhanced convolutional block with optional attention and batch normalization"""
    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)
    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)
    if use_se:
        x = squeeze_excite_block(x)
    x = MaxPooling2D((2, 2))(x)
    if dropout_rate > 0:
        x = Dropout(dropout_rate)(x)
    return x

def residual_block_enhanced(x, filters, use_se=True):
    """Enhanced residual block with SE attention"""
    shortcut = Conv2D(filters, (1, 1), padding='same')(x)
    shortcut = BatchNormalization()(shortcut)
    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Conv2D(filters, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)
    if use_se:
        x = squeeze_excite_block(x)
    x = Add()([x, shortcut])
    x = Activation('relu')(x)
    return x

def enhanced_model_full(input_shape=(224, 224, 3), num_classes=2):
    """Full enhanced model with all improvements"""
    input_layer = Input(shape=input_shape)

    # Initial feature extraction with larger kernels
    x = Conv2D(64, (7, 7), strides=2, padding='same', activation='relu')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)

    # Progressive feature learning with attention
    x = conv_block_with_attention(x, 128, use_se=True, use_bn=True, dropout_rate=0.1)
    x = conv_block_with_attention(x, 256, use_se=True, use_bn=True, dropout_rate=0.2)

    # Multiple residual blocks with attention
    x = residual_block_enhanced(x, 512, use_se=True)
    x = residual_block_enhanced(x, 512, use_se=True)
    x = MaxPooling2D((2, 2))(x)

    # High-level feature extraction
    x = residual_block_enhanced(x, 1024, use_se=True)

    # Multi-scale feature fusion
    branch1 = GlobalAveragePooling2D()(x)
    branch1 = Dense(256, activation='relu')(branch1)

    branch2 = MaxPooling2D((2, 2))(x)
    branch2 = Flatten()(branch2)
    branch2 = Dense(256, activation='relu')(branch2)

    combined = Concatenate()([branch1, branch2])
    combined = BatchNormalization()(combined)
    combined = Dropout(0.5)(combined)

    # Final classification layers
    x = Dense(512, activation='relu')(combined)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)

    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=output, name='EnhancedFullModel')
    return model

def evaluate_model_detailed(model, test_set, path):
    """Detailed evaluation with additional metrics"""
    test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(
        path + '/test',
        target_size=(224, 224),
        batch_size=1,
        color_mode='rgb',
        shuffle=False,
        class_mode='binary'
    )

    # Basic evaluation
    test_loss, test_acc = model.evaluate(test_set, verbose=0)

    # Detailed predictions
    y_pred = model.predict(test_generator, verbose=0)
    y_test = test_generator.classes
    y_pred_binary = np.argmax(y_pred, axis=1)

    # Calculate metrics
    roc_auc = metrics.roc_auc_score(y_test, y_pred[:, 1])
    precision = metrics.precision_score(y_test, y_pred_binary, average='weighted')
    recall = metrics.recall_score(y_test, y_pred_binary, average='weighted')
    f1_score = metrics.f1_score(y_test, y_pred_binary, average='weighted')

    # Confusion matrix
    cm = metrics.confusion_matrix(y_test, y_pred_binary)
    tn, fp, fn, tp = cm.ravel()

    # Specificity
    specificity = tn / (tn + fp)

    return {
        'test_accuracy': test_acc,
        'test_loss': test_loss,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'specificity': specificity,
        'true_positives': int(tp),
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn)
    }

def save_individual_run_results(model_name, seed, history, test_results, model_params):
    """Save results for individual run"""
    history_dict = {
        'accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy'],
        'loss': history.history['loss'],
        'val_loss': history.history['val_loss']
    }

    with open(f'{results_dir}/{model_name.lower()}/history_seed_{seed}.pkl', 'wb') as f:
        pickle.dump(history_dict, f)

    results = {
        'seed': seed,
        'model_name': model_name,
        'test_accuracy': test_results['test_accuracy'],
        'test_loss': test_results['test_loss'],
        'best_val_acc': max(history.history['val_accuracy']),
        'best_val_loss': min(history.history['val_loss']),
        'total_params': model_params,
        'roc_auc': test_results['roc_auc'],
        'precision': test_results['precision'],
        'recall': test_results['recall'],
        'f1_score': test_results['f1_score'],
        'specificity': test_results['specificity'],
        'true_positives': test_results['true_positives'],
        'true_negatives': test_results['true_negatives'],
        'false_positives': test_results['false_positives'],
        'false_negatives': test_results['false_negatives']
    }

    with open(f'{results_dir}/{model_name.lower()}/results_seed_{seed}.json', 'w') as f:
        json.dump(results, f, indent=2)

    return results

# ============================================================================
# MAIN TRAINING LOOP - PART 2: SEEDS 62 AND 72
# ============================================================================

SEEDS_PART2 = [62, 72]
model_name = "Enhanced_Full"
all_results = []

# Data generator templates
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    brightness_range=[0.9, 1.1]
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

# Loop through seeds 62 and 72
for run_idx, seed in enumerate(SEEDS_PART2):
    print(f"\n{'='*70}")
    print(f"TRAINING RUN {run_idx + 1}/{len(SEEDS_PART2)} WITH SEED {seed}")
    print(f"{'='*70}\n")

    # Set seed
    set_seeds(seed)

    # Create data generators
    training_set = train_datagen.flow_from_directory(
        path + '/train',
        class_mode='binary',
        shuffle=True,
        target_size=(row, col),
        batch_size=bs,
        seed=seed
    )

    validation_set = val_test_datagen.flow_from_directory(
        path + '/valid',
        class_mode='binary',
        shuffle=False,
        target_size=(row, col),
        batch_size=bs
    )

    test_set = val_test_datagen.flow_from_directory(
        path + '/test',
        class_mode='binary',
        shuffle=False,
        target_size=(row, col),
        batch_size=bs
    )

    # Create model
    model = enhanced_model_full()

    if run_idx == 0:
        print(f"Model: {model_name}")
        print(f"Total Parameters: {model.count_params():,}")
        model.summary()

    # Setup callbacks
    checkpoint = ModelCheckpoint(
        filepath=f'{results_dir}/enhanced_full/best_model_seed_{seed}.keras',
        save_best_only=True,
        verbose=1,
        mode='min',
        monitor='val_loss'
    )

    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=4,
        verbose=1,
        min_lr=1e-7
    )

    csv_logger = CSVLogger(f'{results_dir}/enhanced_full/training_seed_{seed}.log')

    early_stopping = EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.001,
        patience=7,
        verbose=1,
        mode='max',
        restore_best_weights=True
    )

    callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

    # Compile model
    optimizer = Adam(learning_rate=1e-4)
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # Train model
    print(f"\n⏳ Starting training for seed {seed}...")
    history = model.fit(
        training_set,
        validation_data=validation_set,
        epochs=20,
        callbacks=callbacks,
        verbose=1
    )

    # Evaluate model
    print(f"\n📊 Evaluating model for seed {seed}...")
    test_results = evaluate_model_detailed(model, test_set, path)

    # Print results
    print(f"\n{'='*70}")
    print(f"RESULTS FOR SEED {seed}")
    print(f"{'='*70}")
    print(f"Test Accuracy:    {test_results['test_accuracy']*100:.2f}%")
    print(f"Test Loss:        {test_results['test_loss']:.4f}")
    print(f"ROC-AUC:          {test_results['roc_auc']:.4f}")
    print(f"Precision:        {test_results['precision']*100:.2f}%")
    print(f"Recall:           {test_results['recall']*100:.2f}%")
    print(f"Specificity:      {test_results['specificity']*100:.2f}%")
    print(f"F1-Score:         {test_results['f1_score']*100:.2f}%")
    print(f"Best Val Acc:     {max(history.history['val_accuracy'])*100:.2f}%")
    print(f"{'='*70}")

    # Save results
    run_results = save_individual_run_results(
        model_name, seed, history, test_results, model.count_params()
    )
    all_results.append(run_results)

    # Clear memory
    del model
    tf.keras.backend.clear_session()

# Save Part 2 results summary
part2_summary = {
    'part': 2,
    'seeds': SEEDS_PART2,
    'results': all_results
}

with open(f'{results_dir}/enhanced_full/part2_summary.json', 'w') as f:
    json.dump(part2_summary, f, indent=2)

print(f"\n{'='*70}")
print("✅ PART 2 COMPLETED SUCCESSFULLY!")
print(f"{'='*70}")
print(f"Seeds trained: {SEEDS_PART2}")
print(f"Results saved in: {results_dir}/enhanced_full/")
print("\nFiles saved:")
print(f"  - results_seed_62.json")
print(f"  - results_seed_72.json")
print(f"  - best_model_seed_62.keras")
print(f"  - best_model_seed_72.keras")
print(f"  - part2_summary.json")
print(f"\n⚠️  IMPORTANT: Run Part 3 to analyze all results (seeds 42, 52, 62, 72)")
print(f"{'='*70}")

"""# Seed 3"""

import os
import numpy as np
import pandas as pd
import pickle
import json
import random
from tensorflow.keras.layers import (Input, Dense, Flatten, Conv2D, MaxPooling2D,
                                   BatchNormalization, Dropout, Reshape, Concatenate,
                                   LeakyReLU, GlobalAveragePooling2D, Multiply, Add,
                                   DepthwiseConv2D, SeparableConv2D, Activation)
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from google.colab import drive
from sklearn import metrics

# Mount Google Drive
drive.mount('/content/drive')

# Use existing results directory
results_dir = '/content/drive/MyDrive/ablation_study_results'
os.makedirs(f'{results_dir}/enhanced_full', exist_ok=True)

print("="*70)
print("PART 2: Training Enhanced Full Model with Seeds 62 and 72")
print("="*70)

# Download and setup data
!pip install opendatasets --upgrade --quiet
import opendatasets as od
dataset_url = "https://www.kaggle.com/xhlulu/140k-real-and-fake-faces"
od.download(dataset_url)

path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'
bs = 64
row, col = 224, 224

# SEED MANAGEMENT
def set_seeds(seed):
    """Set all random seeds for reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"✓ Set all seeds to: {seed}")

# ATTENTION MECHANISMS AND CUSTOM BLOCKS
def squeeze_excite_block(input_tensor, ratio=16):
    """Squeeze-and-Excitation block for channel attention"""
    filters = input_tensor.shape[-1]
    squeeze = GlobalAveragePooling2D()(input_tensor)
    excitation = Dense(filters // ratio, activation='relu')(squeeze)
    excitation = Dense(filters, activation='sigmoid')(excitation)
    excitation = Reshape((1, 1, filters))(excitation)
    scaled = Multiply()([input_tensor, excitation])
    return scaled

def conv_block_with_attention(x, filters, kernel_size=3, use_se=True, use_bn=True, dropout_rate=0.0):
    """Enhanced convolutional block with optional attention and batch normalization"""
    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)
    x = Conv2D(filters, kernel_size, padding='same', activation='relu')(x)
    if use_bn:
        x = BatchNormalization()(x)
    if use_se:
        x = squeeze_excite_block(x)
    x = MaxPooling2D((2, 2))(x)
    if dropout_rate > 0:
        x = Dropout(dropout_rate)(x)
    return x

def residual_block_enhanced(x, filters, use_se=True):
    """Enhanced residual block with SE attention"""
    shortcut = Conv2D(filters, (1, 1), padding='same')(x)
    shortcut = BatchNormalization()(shortcut)
    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = Conv2D(filters, (3, 3), padding='same')(x)
    x = BatchNormalization()(x)
    if use_se:
        x = squeeze_excite_block(x)
    x = Add()([x, shortcut])
    x = Activation('relu')(x)
    return x

def enhanced_model_full(input_shape=(224, 224, 3), num_classes=2):
    """Full enhanced model with all improvements"""
    input_layer = Input(shape=input_shape)

    # Initial feature extraction with larger kernels
    x = Conv2D(64, (7, 7), strides=2, padding='same', activation='relu')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)

    # Progressive feature learning with attention
    x = conv_block_with_attention(x, 128, use_se=True, use_bn=True, dropout_rate=0.1)
    x = conv_block_with_attention(x, 256, use_se=True, use_bn=True, dropout_rate=0.2)

    # Multiple residual blocks with attention
    x = residual_block_enhanced(x, 512, use_se=True)
    x = residual_block_enhanced(x, 512, use_se=True)
    x = MaxPooling2D((2, 2))(x)

    # High-level feature extraction
    x = residual_block_enhanced(x, 1024, use_se=True)

    # Multi-scale feature fusion
    branch1 = GlobalAveragePooling2D()(x)
    branch1 = Dense(256, activation='relu')(branch1)

    branch2 = MaxPooling2D((2, 2))(x)
    branch2 = Flatten()(branch2)
    branch2 = Dense(256, activation='relu')(branch2)

    combined = Concatenate()([branch1, branch2])
    combined = BatchNormalization()(combined)
    combined = Dropout(0.5)(combined)

    # Final classification layers
    x = Dense(512, activation='relu')(combined)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)

    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=output, name='EnhancedFullModel')
    return model

def evaluate_model_detailed(model, test_set, path):
    """Detailed evaluation with additional metrics"""
    test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(
        path + '/test',
        target_size=(224, 224),
        batch_size=1,
        color_mode='rgb',
        shuffle=False,
        class_mode='binary'
    )

    # Basic evaluation
    test_loss, test_acc = model.evaluate(test_set, verbose=0)

    # Detailed predictions
    y_pred = model.predict(test_generator, verbose=0)
    y_test = test_generator.classes
    y_pred_binary = np.argmax(y_pred, axis=1)

    # Calculate metrics
    roc_auc = metrics.roc_auc_score(y_test, y_pred[:, 1])
    precision = metrics.precision_score(y_test, y_pred_binary, average='weighted')
    recall = metrics.recall_score(y_test, y_pred_binary, average='weighted')
    f1_score = metrics.f1_score(y_test, y_pred_binary, average='weighted')

    # Confusion matrix
    cm = metrics.confusion_matrix(y_test, y_pred_binary)
    tn, fp, fn, tp = cm.ravel()

    # Specificity
    specificity = tn / (tn + fp)

    return {
        'test_accuracy': test_acc,
        'test_loss': test_loss,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'specificity': specificity,
        'true_positives': int(tp),
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn)
    }

def save_individual_run_results(model_name, seed, history, test_results, model_params):
    """Save results for individual run"""
    history_dict = {
        'accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy'],
        'loss': history.history['loss'],
        'val_loss': history.history['val_loss']
    }

    with open(f'{results_dir}/{model_name.lower()}/history_seed_{seed}.pkl', 'wb') as f:
        pickle.dump(history_dict, f)

    results = {
        'seed': seed,
        'model_name': model_name,
        'test_accuracy': test_results['test_accuracy'],
        'test_loss': test_results['test_loss'],
        'best_val_acc': max(history.history['val_accuracy']),
        'best_val_loss': min(history.history['val_loss']),
        'total_params': model_params,
        'roc_auc': test_results['roc_auc'],
        'precision': test_results['precision'],
        'recall': test_results['recall'],
        'f1_score': test_results['f1_score'],
        'specificity': test_results['specificity'],
        'true_positives': test_results['true_positives'],
        'true_negatives': test_results['true_negatives'],
        'false_positives': test_results['false_positives'],
        'false_negatives': test_results['false_negatives']
    }

    with open(f'{results_dir}/{model_name.lower()}/results_seed_{seed}.json', 'w') as f:
        json.dump(results, f, indent=2)

    return results

# ============================================================================
# MAIN TRAINING LOOP - PART 2: SEEDS 62 AND 72
# ============================================================================

SEEDS_PART2 = [62, 72]
model_name = "Enhanced_Full"
all_results = []

# Data generator templates
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    brightness_range=[0.9, 1.1]
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

# Loop through seeds 62 and 72
for run_idx, seed in enumerate(SEEDS_PART2):
    print(f"\n{'='*70}")
    print(f"TRAINING RUN {run_idx + 1}/{len(SEEDS_PART2)} WITH SEED {seed}")
    print(f"{'='*70}\n")

    # Set seed
    set_seeds(seed)

    # Create data generators
    training_set = train_datagen.flow_from_directory(
        path + '/train',
        class_mode='binary',
        shuffle=True,
        target_size=(row, col),
        batch_size=bs,
        seed=seed
    )

    validation_set = val_test_datagen.flow_from_directory(
        path + '/valid',
        class_mode='binary',
        shuffle=False,
        target_size=(row, col),
        batch_size=bs
    )

    test_set = val_test_datagen.flow_from_directory(
        path + '/test',
        class_mode='binary',
        shuffle=False,
        target_size=(row, col),
        batch_size=bs
    )

    # Create model
    model = enhanced_model_full()

    if run_idx == 0:
        print(f"Model: {model_name}")
        print(f"Total Parameters: {model.count_params():,}")
        model.summary()

    # Setup callbacks
    checkpoint = ModelCheckpoint(
        filepath=f'{results_dir}/enhanced_full/best_model_seed_{seed}.keras',
        save_best_only=True,
        verbose=1,
        mode='min',
        monitor='val_loss'
    )

    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=4,
        verbose=1,
        min_lr=1e-7
    )

    csv_logger = CSVLogger(f'{results_dir}/enhanced_full/training_seed_{seed}.log')

    early_stopping = EarlyStopping(
        monitor='val_accuracy',
        min_delta=0.001,
        patience=7,
        verbose=1,
        mode='max',
        restore_best_weights=True
    )

    callbacks = [checkpoint, reduce_lr, early_stopping, csv_logger]

    # Compile model
    optimizer = Adam(learning_rate=1e-4)
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # Train model
    print(f"\n⏳ Starting training for seed {seed}...")
    history = model.fit(
        training_set,
        validation_data=validation_set,
        epochs=20,
        callbacks=callbacks,
        verbose=1
    )

    # Evaluate model
    print(f"\n📊 Evaluating model for seed {seed}...")
    test_results = evaluate_model_detailed(model, test_set, path)

    # Print results
    print(f"\n{'='*70}")
    print(f"RESULTS FOR SEED {seed}")
    print(f"{'='*70}")
    print(f"Test Accuracy:    {test_results['test_accuracy']*100:.2f}%")
    print(f"Test Loss:        {test_results['test_loss']:.4f}")
    print(f"ROC-AUC:          {test_results['roc_auc']:.4f}")
    print(f"Precision:        {test_results['precision']*100:.2f}%")
    print(f"Recall:           {test_results['recall']*100:.2f}%")
    print(f"Specificity:      {test_results['specificity']*100:.2f}%")
    print(f"F1-Score:         {test_results['f1_score']*100:.2f}%")
    print(f"Best Val Acc:     {max(history.history['val_accuracy'])*100:.2f}%")
    print(f"{'='*70}")

    # Save results
    run_results = save_individual_run_results(
        model_name, seed, history, test_results, model.count_params()
    )
    all_results.append(run_results)

    # Clear memory
    del model
    tf.keras.backend.clear_session()

# Save Part 2 results summary
part2_summary = {
    'part': 2,
    'seeds': SEEDS_PART2,
    'results': all_results
}

with open(f'{results_dir}/enhanced_full/part2_summary.json', 'w') as f:
    json.dump(part2_summary, f, indent=2)

print(f"\n{'='*70}")
print("✅ PART 2 COMPLETED SUCCESSFULLY!")
print(f"{'='*70}")
print(f"Seeds trained: {SEEDS_PART2}")
print(f"Results saved in: {results_dir}/enhanced_full/")
print("\nFiles saved:")
print(f"  - results_seed_62.json")
print(f"  - results_seed_72.json")
print(f"  - best_model_seed_62.keras")
print(f"  - best_model_seed_72.keras")
print(f"  - part2_summary.json")
print(f"\n⚠️  IMPORTANT: Run Part 3 to analyze all results (seeds 42, 52, 62, 72)")
print(f"{'='*70}")

"""# Part 3: Complete Analysis and Statistical Comparison

Seed 42
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/enhanced_full/best_model_seed_42.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""seed 52"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/enhanced_full/best_model_seed_52.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""seed 62"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score, precision_recall_curve)
import json
import time

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/enhanced_full/best_model_seed_62.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/model_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*70)
print("LOADING TRAINED MODEL AND TESTING ON DATASET")
print("="*70)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Display model summary
print("\nModel Architecture:")
model.summary()

# Setup test data generator
print(f"\nLoading test data from: {DATA_PATH}")
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    DATA_PATH,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary',
    shuffle=False  # Important: don't shuffle for proper evaluation
)

print(f"✓ Found {test_generator.samples} test images")
print(f"✓ Classes: {test_generator.class_indices}")

# =================================================================
# SINGLE IMAGE INFERENCE TIME TEST
# =================================================================
print("\n" + "="*70)
print("SINGLE IMAGE INFERENCE TIME ANALYSIS")
print("="*70)

# Get a single batch for inference timing
single_batch = next(iter(test_generator))
single_image = single_batch[0][0:1]  # Take only the first image

# Warm-up prediction (to avoid cold-start overhead)
_ = model.predict(single_image, verbose=0)

# Measure inference time for a single image (multiple runs for accuracy)
num_runs = 100
inference_times = []

print(f"\nRunning {num_runs} inference iterations on a single image...")
for i in range(num_runs):
    start_time = time.time()
    _ = model.predict(single_image, verbose=0)
    end_time = time.time()
    inference_times.append((end_time - start_time) * 1000)  # Convert to milliseconds

avg_inference_time = np.mean(inference_times)
std_inference_time = np.std(inference_times)
min_inference_time = np.min(inference_times)
max_inference_time = np.max(inference_times)

print(f"\n{'Single Image Inference Time Statistics':^70}")
print("-"*70)
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Standard Deviation:     {std_inference_time:.2f} ms")
print(f"Minimum Time:           {min_inference_time:.2f} ms")
print(f"Maximum Time:           {max_inference_time:.2f} ms")
print(f"FPS (Frames Per Second): {1000/avg_inference_time:.2f}")
print("-"*70)

# Reset generator for full testing
test_generator.reset()

# =================================================================
# FULL DATASET PREDICTIONS
# =================================================================
print("\n" + "="*70)
print("GENERATING PREDICTIONS ON FULL TEST SET")
print("="*70)

y_true = test_generator.classes

# Time the full dataset prediction
print(f"\nPredicting on {test_generator.samples} test images...")
start_time_full = time.time()
y_pred_proba = model.predict(test_generator, verbose=1)
end_time_full = time.time()

total_time = end_time_full - start_time_full
avg_time_per_image_batch = (total_time / test_generator.samples) * 1000

print(f"\nFull Dataset Inference Statistics:")
print(f"Total Time: {total_time:.2f} seconds")
print(f"Average Time per Image: {avg_time_per_image_batch:.2f} ms")
print(f"Throughput: {test_generator.samples/total_time:.2f} images/second")

y_pred_binary = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
print("\n" + "="*70)
print("CLASSIFICATION RESULTS")
print("="*70)

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred_binary)
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Classification Report
print("\n" + "-"*70)
print("DETAILED CLASSIFICATION REPORT")
print("-"*70)
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred_binary,
                               target_names=class_names,
                               digits=4)
print(report)

# Save classification report
report_dict = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   output_dict=True)
with open(f'{RESULTS_DIR}/classification_report.json', 'w') as f:
    json.dump(report_dict, f, indent=2)
print(f"✓ Classification report saved to {RESULTS_DIR}/classification_report.json")

# Confusion Matrix
print("\n" + "-"*70)
print("CONFUSION MATRIX")
print("-"*70)
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"\nTrue Negatives:  {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives:  {cm[1,1]}")

# Calculate per-class metrics
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
sensitivity = tp / (tp + fn)
print(f"\nSensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')
print(f"✓ Confusion matrix plot saved to {RESULTS_DIR}/confusion_matrix.png")
plt.close()

# ROC Curve
print("\n" + "-"*70)
print("ROC CURVE ANALYSIS")
print("-"*70)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
roc_auc = auc(fpr, tpr)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/roc_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ ROC curve saved to {RESULTS_DIR}/roc_curve.png")
plt.close()

# Precision-Recall Curve
print("\n" + "-"*70)
print("PRECISION-RECALL CURVE")
print("-"*70)
precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba[:, 1])
pr_auc = auc(recall, precision)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/precision_recall_curve.png', dpi=300, bbox_inches='tight')
print(f"✓ Precision-Recall curve saved to {RESULTS_DIR}/precision_recall_curve.png")
plt.close()

# Prediction Distribution
print("\n" + "-"*70)
print("PREDICTION DISTRIBUTION")
print("-"*70)
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba[y_true==0, 1], bins=50, alpha=0.5, label=class_names[0], color='blue')
plt.hist(y_pred_proba[y_true==1, 1], bins=50, alpha=0.5, label=class_names[1], color='red')
plt.xlabel('Predicted Probability (Class 1)')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities by True Class')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/prediction_distribution.png', dpi=300, bbox_inches='tight')
print(f"✓ Prediction distribution plot saved to {RESULTS_DIR}/prediction_distribution.png")
plt.close()

# =================================================================
# INFERENCE TIME VISUALIZATION
# =================================================================
print("\n" + "-"*70)
print("CREATING INFERENCE TIME VISUALIZATION")
print("-"*70)

# Create inference time visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Histogram of inference times
ax1.hist(inference_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
ax1.axvline(avg_inference_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_inference_time:.2f} ms')
ax1.set_xlabel('Inference Time (ms)', fontsize=12, fontweight='bold')
ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')
ax1.set_title('Distribution of Single Image Inference Times', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(alpha=0.3)

# Plot 2: Summary metrics
metrics_text = f"""
Single Image Inference Statistics
{'─'*35}

Average Time:    {avg_inference_time:.2f} ms
Std Deviation:   {std_inference_time:.2f} ms
Min Time:        {min_inference_time:.2f} ms
Max Time:        {max_inference_time:.2f} ms
FPS:             {1000/avg_inference_time:.2f}

{'─'*35}
Batch Processing Statistics
{'─'*35}

Total Images:    {test_generator.samples}
Total Time:      {total_time:.2f} s
Throughput:      {test_generator.samples/total_time:.2f} img/s
"""

ax2.text(0.1, 0.5, metrics_text, fontsize=11, verticalalignment='center',
         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2.axis('off')

plt.tight_layout()
plt.savefig(f'{RESULTS_DIR}/inference_time_analysis.png', dpi=300, bbox_inches='tight')
print(f"✓ Inference time analysis saved to {RESULTS_DIR}/inference_time_analysis.png")
plt.close()

# Save detailed results
print("\n" + "="*70)
print("SAVING DETAILED RESULTS")
print("="*70)

# Create detailed results DataFrame
results_df = pd.DataFrame({
    'true_label': y_true,
    'predicted_label': y_pred_binary,
    'probability_class_0': y_pred_proba[:, 0],
    'probability_class_1': y_pred_proba[:, 1],
    'correct_prediction': y_true == y_pred_binary
})

# Add filenames if available
if hasattr(test_generator, 'filenames'):
    results_df['filename'] = test_generator.filenames

results_df.to_csv(f'{RESULTS_DIR}/detailed_predictions.csv', index=False)
print(f"✓ Detailed predictions saved to {RESULTS_DIR}/detailed_predictions.csv")

# Summary statistics
print("\n" + "-"*70)
print("PREDICTION STATISTICS")
print("-"*70)
print(f"Total samples: {len(y_true)}")
print(f"Correct predictions: {sum(y_true == y_pred_binary)} ({sum(y_true == y_pred_binary)/len(y_true)*100:.2f}%)")
print(f"Incorrect predictions: {sum(y_true != y_pred_binary)} ({sum(y_true != y_pred_binary)/len(y_true)*100:.2f}%)")

# Analyze misclassifications
misclassified = results_df[results_df['correct_prediction'] == False]
print(f"\nMisclassified samples: {len(misclassified)}")
if len(misclassified) > 0:
    print("\nMisclassification breakdown:")
    print(f"  False Positives (predicted 1, actually 0): {fp}")
    print(f"  False Negatives (predicted 0, actually 1): {fn}")

    # Save misclassified samples
    misclassified.to_csv(f'{RESULTS_DIR}/misclassified_samples.csv', index=False)
    print(f"✓ Misclassified samples saved to {RESULTS_DIR}/misclassified_samples.csv")

# Create summary report
summary = {
    'model_path': MODEL_PATH,
    'test_samples': int(len(y_true)),
    'accuracy': float(accuracy),
    'roc_auc': float(roc_auc),
    'precision_recall_auc': float(pr_auc),
    'sensitivity': float(sensitivity),
    'specificity': float(specificity),
    'inference_time': {
        'single_image_avg_ms': float(avg_inference_time),
        'single_image_std_ms': float(std_inference_time),
        'single_image_min_ms': float(min_inference_time),
        'single_image_max_ms': float(max_inference_time),
        'fps': float(1000/avg_inference_time),
        'full_dataset_total_sec': float(total_time),
        'full_dataset_throughput_img_per_sec': float(test_generator.samples/total_time)
    },
    'confusion_matrix': {
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'true_positives': int(tp)
    },
    'class_distribution': {
        class_names[0]: int(sum(y_true == 0)),
        class_names[1]: int(sum(y_true == 1))
    }
}

with open(f'{RESULTS_DIR}/test_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)
print(f"✓ Test summary saved to {RESULTS_DIR}/test_summary.json")

print("\n" + "="*70)
print("TESTING COMPLETE!")
print("="*70)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - classification_report.json")
print("  - confusion_matrix.png")
print("  - roc_curve.png")
print("  - precision_recall_curve.png")
print("  - prediction_distribution.png")
print("  - inference_time_analysis.png")
print("  - detailed_predictions.csv")
print("  - misclassified_samples.csv")
print("  - test_summary.json")

"""# Robustness Anlysis

# Part 1
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import img_to_array, array_to_img
from PIL import Image, ImageEnhance
import cv2
from io import BytesIO
import random

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/enhanced/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/robustness_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224

print("="*80)
print("MODEL ROBUSTNESS TESTING WITH POST-PROCESSING ATTACKS")
print("="*80)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Get class names and all image paths
class_names = ['fake', 'real']  # Adjust based on your folder structure
all_images = []

for class_name in class_names:
    class_path = os.path.join(DATA_PATH, class_name)
    if os.path.exists(class_path):
        for img_file in os.listdir(class_path):
            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                all_images.append({
                    'path': os.path.join(class_path, img_file),
                    'label': class_name,
                    'filename': img_file
                })

# Randomly select 15 samples
random.seed(42)
selected_samples = random.sample(all_images, min(15, len(all_images)))

print(f"\n✓ Selected {len(selected_samples)} random samples for testing")

# =================================================================
# DEFINE POST-PROCESSING ATTACK FUNCTIONS
# =================================================================

def apply_gaussian_noise(img_array, mean=0, std=25):
    """Add Gaussian noise to image"""
    noise = np.random.normal(mean, std, img_array.shape)
    noisy_img = img_array + noise
    return np.clip(noisy_img, 0, 255).astype(np.uint8)

def apply_jpeg_compression(img_array, quality=30):
    """Apply JPEG compression"""
    img_pil = array_to_img(img_array.astype('uint8'))
    buffer = BytesIO()
    img_pil.save(buffer, format='JPEG', quality=quality)
    buffer.seek(0)
    compressed_img = Image.open(buffer)
    return img_to_array(compressed_img)

def apply_resize_attack(img_array, scale=0.5):
    """Resize image down and back up"""
    h, w = img_array.shape[:2]
    small = cv2.resize(img_array, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_LINEAR)
    resized = cv2.resize(small, (w, h), interpolation=cv2.INTER_LINEAR)
    return resized

def apply_blur(img_array, kernel_size=7):
    """Apply Gaussian blur"""
    return cv2.GaussianBlur(img_array, (kernel_size, kernel_size), 0)

def apply_brightness(img_array, factor=1.5):
    """Adjust brightness"""
    img_pil = array_to_img(img_array.astype('uint8'))
    enhancer = ImageEnhance.Brightness(img_pil)
    bright_img = enhancer.enhance(factor)
    return img_to_array(bright_img)

def apply_contrast(img_array, factor=1.5):
    """Adjust contrast"""
    img_pil = array_to_img(img_array.astype('uint8'))
    enhancer = ImageEnhance.Contrast(img_pil)
    contrast_img = enhancer.enhance(factor)
    return img_to_array(contrast_img)

def apply_zoom(img_array, zoom_factor=1.2):
    """Apply zoom (crop center and resize)"""
    h, w = img_array.shape[:2]
    crop_h, crop_w = int(h/zoom_factor), int(w/zoom_factor)
    start_h, start_w = (h-crop_h)//2, (w-crop_w)//2
    cropped = img_array[start_h:start_h+crop_h, start_w:start_w+crop_w]
    zoomed = cv2.resize(cropped, (w, h), interpolation=cv2.INTER_LINEAR)
    return zoomed

# =================================================================
# PREDICTION FUNCTION
# =================================================================

def predict_image(img_array, model):
    """Preprocess and predict image"""
    # Normalize
    img_normalized = img_array / 255.0
    # Add batch dimension
    img_batch = np.expand_dims(img_normalized, axis=0)
    # Predict
    pred_proba = model.predict(img_batch, verbose=0)
    pred_class = np.argmax(pred_proba)
    confidence = pred_proba[0][pred_class]
    return class_names[pred_class], confidence

# =================================================================
# PROCESS EACH SAMPLE WITH ALL ATTACKS
# =================================================================

print("\n" + "="*80)
print("APPLYING POST-PROCESSING ATTACKS AND TESTING")
print("="*80)

results = []

for idx, sample in enumerate(selected_samples, 1):
    print(f"\nProcessing sample {idx}/15: {sample['filename']}")

    # Load original image
    img = image.load_img(sample['path'], target_size=(IMG_HEIGHT, IMG_WIDTH))
    img_array = img_to_array(img)

    actual_label = sample['label']

    # Test original image
    pred_label, confidence = predict_image(img_array, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Original',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Gaussian Noise
    noisy_img = apply_gaussian_noise(img_array.copy())
    pred_label, confidence = predict_image(noisy_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Gaussian Noise',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply JPEG Compression
    compressed_img = apply_jpeg_compression(img_array.copy())
    pred_label, confidence = predict_image(compressed_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'JPEG Compression',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Image Resizing
    resized_img = apply_resize_attack(img_array.copy())
    pred_label, confidence = predict_image(resized_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Image Resizing',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Blur
    blurred_img = apply_blur(img_array.copy())
    pred_label, confidence = predict_image(blurred_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Blur',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Brightness
    bright_img = apply_brightness(img_array.copy())
    pred_label, confidence = predict_image(bright_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Brightness',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Contrast
    contrast_img = apply_contrast(img_array.copy())
    pred_label, confidence = predict_image(contrast_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Contrast',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Zoom
    zoomed_img = apply_zoom(img_array.copy())
    pred_label, confidence = predict_image(zoomed_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Zoom',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

# =================================================================
# CREATE RESULTS DATAFRAME AND DISPLAY
# =================================================================

print("\n" + "="*80)
print("ROBUSTNESS TEST RESULTS")
print("="*80)

df_results = pd.DataFrame(results)

# Display results
print("\n")
print(df_results.to_string(index=False))

# Save to CSV
csv_path = f'{RESULTS_DIR}/robustness_test_results.csv'
df_results.to_csv(csv_path, index=False)
print(f"\n✓ Results saved to: {csv_path}")

# =================================================================
# SUMMARY STATISTICS
# =================================================================

print("\n" + "="*80)
print("SUMMARY STATISTICS")
print("="*80)

# Calculate accuracy for each attack type
attack_summary = df_results.groupby('Attack Type').agg({
    'Correct': lambda x: (x == '✓').sum(),
    'Sl. No': 'count'
}).rename(columns={'Sl. No': 'Total'})

attack_summary['Accuracy (%)'] = (attack_summary['Correct'] / attack_summary['Total'] * 100).round(2)
attack_summary = attack_summary.reset_index()

print("\nAccuracy by Attack Type:")
print(attack_summary.to_string(index=False))

# Save summary
summary_path = f'{RESULTS_DIR}/attack_summary.csv'
attack_summary.to_csv(summary_path, index=False)
print(f"\n✓ Summary saved to: {summary_path}")

# =================================================================
# VISUALIZATION
# =================================================================

print("\n" + "="*80)
print("CREATING VISUALIZATIONS")
print("="*80)

# Create bar plot of accuracy by attack type
plt.figure(figsize=(12, 6), dpi=300)
plt.bar(attack_summary['Attack Type'], attack_summary['Accuracy (%)'],
        color='steelblue', edgecolor='black', alpha=0.7)
plt.axhline(y=100, color='green', linestyle='--', linewidth=2, label='Perfect Accuracy')
plt.xlabel('Attack Type', fontsize=12, fontweight='bold')
plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')
plt.title('Model Robustness: Accuracy Under Different Post-Processing Attacks',
          fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 105)
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()

plot_path = f'{RESULTS_DIR}/robustness_accuracy_plot.png'
plt.savefig(plot_path, dpi=300, bbox_inches='tight')
print(f"✓ Accuracy plot saved to: {plot_path}")
plt.close()

# Create heatmap showing which samples failed under which attacks
pivot_correct = df_results.pivot_table(
    index='Test Sample Name',
    columns='Attack Type',
    values='Correct',
    aggfunc=lambda x: 1 if x.iloc[0] == '✓' else 0
)

plt.figure(figsize=(14, 10), dpi=300)
sns.heatmap(pivot_correct, cmap='RdYlGn', cbar_kws={'label': 'Correct (1) / Incorrect (0)'},
            linewidths=0.5, linecolor='gray', vmin=0, vmax=1)
plt.title('Model Predictions Across Samples and Attacks', fontsize=14, fontweight='bold')
plt.xlabel('Attack Type', fontsize=12, fontweight='bold')
plt.ylabel('Test Sample', fontsize=12, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()

heatmap_path = f'{RESULTS_DIR}/robustness_heatmap.png'
plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')
print(f"✓ Heatmap saved to: {heatmap_path}")
plt.close()

print("\n" + "="*80)
print("ROBUSTNESS TESTING COMPLETE!")
print("="*80)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - robustness_test_results.csv (detailed results)")
print("  - attack_summary.csv (accuracy by attack type)")
print("  - robustness_accuracy_plot.png")
print("  - robustness_heatmap.png")

"""# Part 2"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import img_to_array, array_to_img
from PIL import Image, ImageEnhance
import cv2
from io import BytesIO
import random

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/enhanced/best_model.keras'
DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'
RESULTS_DIR = '/content/drive/MyDrive/robustness_test_results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224

print("="*80)
print("MODEL ROBUSTNESS TESTING WITH POST-PROCESSING ATTACKS")
print("="*80)

# Load the trained model
print(f"\nLoading model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# Get class names and all image paths
class_names = ['fake', 'real']  # Adjust based on your folder structure
all_images = []

for class_name in class_names:
    class_path = os.path.join(DATA_PATH, class_name)
    if os.path.exists(class_path):
        for img_file in os.listdir(class_path):
            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                all_images.append({
                    'path': os.path.join(class_path, img_file),
                    'label': class_name,
                    'filename': img_file
                })

# Randomly select 15 samples
random.seed(42)
selected_samples = random.sample(all_images, min(30000, len(all_images)))

print(f"\n✓ Selected {len(selected_samples)} random samples for testing")

# =================================================================
# DEFINE POST-PROCESSING ATTACK FUNCTIONS
# =================================================================

def apply_gaussian_noise(img_array, mean=0, std=25):
    """Add Gaussian noise to image"""
    noise = np.random.normal(mean, std, img_array.shape)
    noisy_img = img_array + noise
    return np.clip(noisy_img, 0, 255).astype(np.uint8)

def apply_jpeg_compression(img_array, quality=30):
    """Apply JPEG compression"""
    img_pil = array_to_img(img_array.astype('uint8'))
    buffer = BytesIO()
    img_pil.save(buffer, format='JPEG', quality=quality)
    buffer.seek(0)
    compressed_img = Image.open(buffer)
    return img_to_array(compressed_img)

def apply_resize_attack(img_array, scale=0.5):
    """Resize image down and back up"""
    h, w = img_array.shape[:2]
    small = cv2.resize(img_array, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_LINEAR)
    resized = cv2.resize(small, (w, h), interpolation=cv2.INTER_LINEAR)
    return resized

def apply_blur(img_array, kernel_size=7):
    """Apply Gaussian blur"""
    return cv2.GaussianBlur(img_array, (kernel_size, kernel_size), 0)

def apply_brightness(img_array, factor=1.5):
    """Adjust brightness"""
    img_pil = array_to_img(img_array.astype('uint8'))
    enhancer = ImageEnhance.Brightness(img_pil)
    bright_img = enhancer.enhance(factor)
    return img_to_array(bright_img)

def apply_contrast(img_array, factor=1.5):
    """Adjust contrast"""
    img_pil = array_to_img(img_array.astype('uint8'))
    enhancer = ImageEnhance.Contrast(img_pil)
    contrast_img = enhancer.enhance(factor)
    return img_to_array(contrast_img)

def apply_zoom(img_array, zoom_factor=1.2):
    """Apply zoom (crop center and resize)"""
    h, w = img_array.shape[:2]
    crop_h, crop_w = int(h/zoom_factor), int(w/zoom_factor)
    start_h, start_w = (h-crop_h)//2, (w-crop_w)//2
    cropped = img_array[start_h:start_h+crop_h, start_w:start_w+crop_w]
    zoomed = cv2.resize(cropped, (w, h), interpolation=cv2.INTER_LINEAR)
    return zoomed

# =================================================================
# PREDICTION FUNCTION
# =================================================================

def predict_image(img_array, model):
    """Preprocess and predict image"""
    # Normalize
    img_normalized = img_array / 255.0
    # Add batch dimension
    img_batch = np.expand_dims(img_normalized, axis=0)
    # Predict
    pred_proba = model.predict(img_batch, verbose=0)
    pred_class = np.argmax(pred_proba)
    confidence = pred_proba[0][pred_class]
    return class_names[pred_class], confidence

# =================================================================
# PROCESS EACH SAMPLE WITH ALL ATTACKS
# =================================================================

print("\n" + "="*80)
print("APPLYING POST-PROCESSING ATTACKS AND TESTING")
print("="*80)

results = []

for idx, sample in enumerate(selected_samples, 1):
    print(f"\nProcessing sample {idx}/15: {sample['filename']}")

    # Load original image
    img = image.load_img(sample['path'], target_size=(IMG_HEIGHT, IMG_WIDTH))
    img_array = img_to_array(img)

    actual_label = sample['label']

    # Test original image
    pred_label, confidence = predict_image(img_array, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Original',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Gaussian Noise
    noisy_img = apply_gaussian_noise(img_array.copy())
    pred_label, confidence = predict_image(noisy_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Gaussian Noise',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply JPEG Compression
    compressed_img = apply_jpeg_compression(img_array.copy())
    pred_label, confidence = predict_image(compressed_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'JPEG Compression',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Image Resizing
    resized_img = apply_resize_attack(img_array.copy())
    pred_label, confidence = predict_image(resized_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Image Resizing',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Blur
    blurred_img = apply_blur(img_array.copy())
    pred_label, confidence = predict_image(blurred_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Blur',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Brightness
    bright_img = apply_brightness(img_array.copy())
    pred_label, confidence = predict_image(bright_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Brightness',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Contrast
    contrast_img = apply_contrast(img_array.copy())
    pred_label, confidence = predict_image(contrast_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Contrast',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

    # Apply Zoom
    zoomed_img = apply_zoom(img_array.copy())
    pred_label, confidence = predict_image(zoomed_img, model)
    results.append({
        'Sl. No': idx,
        'Test Sample Name': sample['filename'],
        'Actual Label': actual_label,
        'Attack Type': 'Zoom',
        'Predicted Label': pred_label,
        'Confidence Score': f"{confidence:.4f}",
        'Correct': '✓' if pred_label == actual_label else '✗'
    })

# =================================================================
# CREATE RESULTS DATAFRAME AND DISPLAY
# =================================================================

print("\n" + "="*80)
print("ROBUSTNESS TEST RESULTS")
print("="*80)

df_results = pd.DataFrame(results)

# Display results
print("\n")
print(df_results.to_string(index=False))

# Save to CSV
csv_path = f'{RESULTS_DIR}/robustness_test_results.csv'
df_results.to_csv(csv_path, index=False)
print(f"\n✓ Results saved to: {csv_path}")

# =================================================================
# SUMMARY STATISTICS
# =================================================================

print("\n" + "="*80)
print("SUMMARY STATISTICS")
print("="*80)

# Calculate accuracy for each attack type
attack_summary = df_results.groupby('Attack Type').agg({
    'Correct': lambda x: (x == '✓').sum(),
    'Sl. No': 'count'
}).rename(columns={'Sl. No': 'Total'})

attack_summary['Accuracy (%)'] = (attack_summary['Correct'] / attack_summary['Total'] * 100).round(2)
attack_summary = attack_summary.reset_index()

print("\nAccuracy by Attack Type:")
print(attack_summary.to_string(index=False))

# Save summary
summary_path = f'{RESULTS_DIR}/attack_summary.csv'
attack_summary.to_csv(summary_path, index=False)
print(f"\n✓ Summary saved to: {summary_path}")

# =================================================================
# VISUALIZATION
# =================================================================

print("\n" + "="*80)
print("CREATING VISUALIZATIONS")
print("="*80)

# Create bar plot of accuracy by attack type
plt.figure(figsize=(12, 6), dpi=300)
plt.bar(attack_summary['Attack Type'], attack_summary['Accuracy (%)'],
        color='steelblue', edgecolor='black', alpha=0.7)
plt.axhline(y=100, color='green', linestyle='--', linewidth=2, label='Perfect Accuracy')
plt.xlabel('Attack Type', fontsize=12, fontweight='bold')
plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')
plt.title('Model Robustness: Accuracy Under Different Post-Processing Attacks',
          fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 105)
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()

plot_path = f'{RESULTS_DIR}/robustness_accuracy_plot.png'
plt.savefig(plot_path, dpi=300, bbox_inches='tight')
print(f"✓ Accuracy plot saved to: {plot_path}")
plt.close()

# Create heatmap showing which samples failed under which attacks
pivot_correct = df_results.pivot_table(
    index='Test Sample Name',
    columns='Attack Type',
    values='Correct',
    aggfunc=lambda x: 1 if x.iloc[0] == '✓' else 0
)

plt.figure(figsize=(14, 10), dpi=300)
sns.heatmap(pivot_correct, cmap='RdYlGn', cbar_kws={'label': 'Correct (1) / Incorrect (0)'},
            linewidths=0.5, linecolor='gray', vmin=0, vmax=1)
plt.title('Model Predictions Across Samples and Attacks', fontsize=14, fontweight='bold')
plt.xlabel('Attack Type', fontsize=12, fontweight='bold')
plt.ylabel('Test Sample', fontsize=12, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()

heatmap_path = f'{RESULTS_DIR}/robustness_heatmap.png'
plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')
print(f"✓ Heatmap saved to: {heatmap_path}")
plt.close()

print("\n" + "="*80)
print("ROBUSTNESS TESTING COMPLETE!")
print("="*80)
print(f"\nAll results saved to: {RESULTS_DIR}/")
print("\nGenerated files:")
print("  - robustness_test_results.csv (detailed results)")
print("  - attack_summary.csv (accuracy by attack type)")
print("  - robustness_accuracy_plot.png")
print("  - robustness_heatmap.png")

"""# Cross Dataset Generalization"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, accuracy_score,
                            precision_score, recall_score, f1_score)
import json
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Configuration
MODEL_PATH = '/content/drive/MyDrive/ablation_study_results/enhanced_full/best_model.keras'
RESULTS_BASE_DIR = '/content/drive/MyDrive/cross_dataset_results'
os.makedirs(RESULTS_BASE_DIR, exist_ok=True)

# Image parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

print("="*80)
print("CROSS-DATASET GENERALIZATION TESTING")
print("="*80)

# Load the trained model
print(f"\nLoading trained model from: {MODEL_PATH}")
model = load_model(MODEL_PATH)
print("✓ Model loaded successfully!")

# =================================================================
# DOWNLOAD ADDITIONAL DATASETS FROM KAGGLE
# =================================================================

print("\n" + "="*80)
print("DOWNLOADING ADDITIONAL DATASETS")
print("="*80)

!pip install opendatasets --upgrade --quiet
import opendatasets as od

# Download Dataset 1: Manjilkarki Deepfake Dataset
print("\n[1/5] Downloading Manjilkarki Deepfake Dataset...")
manjil_url = "https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images"
od.download(manjil_url)
print("✓ Manjilkarki dataset downloaded!")

# Download Dataset 2: Photoshopped Faces
print("\n[2/5] Downloading Photoshopped Faces Dataset...")
photoshop_url = "https://www.kaggle.com/datasets/tbourton/photoshopped-faces"
od.download(photoshop_url)
print("✓ Photoshopped Faces dataset downloaded!")

# Download Dataset 3: Real and Fake Face Detection
print("\n[3/5] Downloading Real and Fake Face Detection Dataset...")
ciplab_url = "https://www.kaggle.com/datasets/ciplab/real-and-fake-face-detection"
od.download(ciplab_url)
print("✓ Real and Fake Face Detection dataset downloaded!")

# Download Dataset 4: CASIA 2.0 Image Tampering Detection
print("\n[4/5] Downloading CASIA 2.0 Dataset...")
casia_url = "https://www.kaggle.com/datasets/divg07/casia-20-image-tampering-detection-dataset"
od.download(casia_url)
print("✓ CASIA 2.0 dataset downloaded!")

# Download Dataset 5: 140k Real and Fake Faces (Original - if not already present)
print("\n[5/5] Downloading 140k Real and Fake Faces (Original Dataset)...")
original_url = "https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces"
od.download(original_url)
print("✓ Original dataset downloaded!")

print("\n" + "="*80)
print("ALL DATASETS DOWNLOADED SUCCESSFULLY!")
print("="*80)

# =================================================================
# ACTUAL DATASET PATHS - CORRECTLY MAPPED
# =================================================================

manual_datasets = {
    # Original training dataset for comparison
    # Structure: /content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test/{fake, real}
    'Original_Dataset': '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test',

    # Dataset 1: Manjilkarki Dataset (Downloaded via opendatasets)
    # Structure: /content/deepfake-and-real-images/Dataset/Test/{Fake, Real}
    'Manjilkarki_Dataset': '/content/deepfake-and-real-images/Dataset/Test',

    # Dataset 2: Photoshopped Faces Dataset
    # Structure: /content/photoshopped-faces/{original, photoshopped} OR /content/photoshopped-faces/test/{original, photoshopped}
    # Try both possible paths:
    'Photoshopped_Faces': '/content/photoshopped-faces',
    # Alternative path if above doesn't work:
    # 'Photoshopped_Faces': '/content/photoshopped-faces/test',

    # Dataset 3: Real and Fake Face Detection (CIPLab)
    # Structure: /content/real-and-fake-face-detection/test/{fake, real} OR /content/real-and-fake-face-detection/{fake, real}
    'CIPLab_Dataset': '/content/real-and-fake-face-detection',
    # Alternative paths:
    # 'CIPLab_Dataset': '/content/real-and-fake-face-detection/test',
    # 'CIPLab_Dataset': '/content/real-and-fake-face-detection/real-vs-fake',

    # Dataset 4: CASIA 2.0 Image Tampering Detection
    # Structure: /content/casia-20-image-tampering-detection-dataset/CASIA2/{Au, Tp}
    # Au = Authentic (Real), Tp = Tampered (Fake)
    'CASIA_2.0': '/content/casia-20-image-tampering-detection-dataset/CASIA2',
    # Alternative paths:
    # 'CASIA_2.0': '/content/casia-20-image-tampering-detection-dataset',

    # Additional datasets you can add:

    # Celeb-DF v2 Dataset
    # Download: od.download("https://www.kaggle.com/datasets/yuezhengling/celeb-df")
    # Structure: /content/celeb-df/{fake, real} OR /content/celeb-df/test/{fake, real}
    # 'Celeb-DF': '/content/celeb-df/test',

    # FaceForensics++ Dataset
    # Download: od.download("https://www.kaggle.com/datasets/sorokin/faceforensics")
    # Structure: /content/faceforensics/{manipulated, original} OR /content/faceforensics/test/{0, 1}
    # 'FaceForensics++': '/content/faceforensics/test',

    # DFDC (DeepFake Detection Challenge) - Preview
    # Download: od.download("https://www.kaggle.com/c/deepfake-detection-challenge")
    # Structure: /content/deepfake-detection-challenge/test/{fake, real}
    # 'DFDC_Preview': '/content/deepfake-detection-challenge/test',

    # Wild Deepfake Dataset
    # Download: od.download("https://www.kaggle.com/datasets/brijesh89/wild-deepfake-dataset")
    # Structure: /content/wild-deepfake-dataset/{fake, real}
    # 'Wild_Deepfake': '/content/wild-deepfake-dataset',
}

print("\n" + "="*80)
print("DATASET PATHS CONFIGURED")
print("="*80)
print("\nConfigured datasets:")
for name, path in manual_datasets.items():
    exists = "✓" if os.path.exists(path) else "✗"
    print(f"{exists} {name}: {path}")

# =================================================================
# TEST FUNCTION
# =================================================================

def test_on_dataset(dataset_name, dataset_path, model, results_dir):
    """
    Test model on a specific dataset and save results
    """
    print(f"\n{'='*80}")
    print(f"TESTING ON: {dataset_name}")
    print(f"{'='*80}")

    if not os.path.exists(dataset_path):
        print(f"⚠ Dataset path not found: {dataset_path}")
        print("Please download the dataset and update the path.")
        return None

    # Create results directory for this dataset
    dataset_results_dir = os.path.join(results_dir, dataset_name.lower().replace(' ', '_').replace('.', '_'))
    os.makedirs(dataset_results_dir, exist_ok=True)

    # Setup test data generator
    print(f"\nLoading test data from: {dataset_path}")
    test_datagen = ImageDataGenerator(rescale=1./255)

    # Try to load the dataset - handle different structures
    test_generator = None
    possible_subdirs = [
        dataset_path,
        os.path.join(dataset_path, 'test'),
        os.path.join(dataset_path, 'Test'),
        os.path.join(dataset_path, 'val'),
    ]

    for try_path in possible_subdirs:
        if os.path.exists(try_path):
            try:
                # Check if this path has subdirectories (classes)
                subdirs = [d for d in os.listdir(try_path) if os.path.isdir(os.path.join(try_path, d))]
                if len(subdirs) >= 2:  # Must have at least 2 classes
                    print(f"  Trying path: {try_path}")
                    print(f"  Found classes: {subdirs}")

                    test_generator = test_datagen.flow_from_directory(
                        try_path,
                        target_size=(IMG_HEIGHT, IMG_WIDTH),
                        batch_size=BATCH_SIZE,
                        class_mode='binary',
                        shuffle=False
                    )
                    print(f"✓ Successfully loaded from: {try_path}")
                    break
            except Exception as e:
                print(f"  ✗ Failed to load from {try_path}: {str(e)[:100]}")
                continue

    if test_generator is None:
        print(f"⚠ Could not load dataset from any expected path")
        print(f"Please check the dataset structure.")
        print(f"Expected structure: dataset_path/{{class1, class2}}/image_files")
        return None

    print(f"✓ Found {test_generator.samples} test images")
    print(f"✓ Classes: {test_generator.class_indices}")

    # Get predictions
    print("\nGenerating predictions...")
    y_true = test_generator.classes
    y_pred_proba = model.predict(test_generator, verbose=1)
    y_pred_binary = np.argmax(y_pred_proba, axis=1)

    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred_binary)
    precision = precision_score(y_true, y_pred_binary, average='binary', zero_division=0)
    recall = recall_score(y_true, y_pred_binary, average='binary', zero_division=0)
    f1 = f1_score(y_true, y_pred_binary, average='binary', zero_division=0)

    print(f"\n{'='*60}")
    print(f"RESULTS FOR {dataset_name}")
    print(f"{'='*60}")
    print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-Score:  {f1:.4f}")

    # Classification Report
    class_names = list(test_generator.class_indices.keys())
    report = classification_report(y_true, y_pred_binary,
                                   target_names=class_names,
                                   digits=4,
                                   zero_division=0)
    print("\nClassification Report:")
    print(report)

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred_binary)
    print("\nConfusion Matrix:")
    print(cm)

    # Plot Confusion Matrix
    plt.figure(figsize=(8, 6), dpi=300)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f'Confusion Matrix - {dataset_name}', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=12, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{dataset_results_dir}/confusion_matrix.png', dpi=300, bbox_inches='tight')
    print(f"✓ Confusion matrix saved")
    plt.close()

    # ROC Curve
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6), dpi=300)
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')
    plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')
    plt.title(f'ROC Curve - {dataset_name}', fontsize=14, fontweight='bold')
    plt.legend(loc="lower right", fontsize=11)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{dataset_results_dir}/roc_curve.png', dpi=300, bbox_inches='tight')
    print(f"✓ ROC curve saved")
    plt.close()

    # Save results
    results = {
        'dataset_name': dataset_name,
        'total_samples': int(len(y_true)),
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'roc_auc': float(roc_auc),
        'confusion_matrix': cm.tolist()
    }

    with open(f'{dataset_results_dir}/results.json', 'w') as f:
        json.dump(results, f, indent=2)

    return results

# =================================================================
# TEST ON ALL AVAILABLE DATASETS
# =================================================================

print("\n" + "="*80)
print("STARTING CROSS-DATASET TESTING")
print("="*80)

all_results = []

for dataset_name, dataset_path in manual_datasets.items():
    if os.path.exists(dataset_path):
        result = test_on_dataset(dataset_name, dataset_path, model, RESULTS_BASE_DIR)
        if result:
            all_results.append(result)
    else:
        print(f"\n⚠ Skipping {dataset_name} - Path not found: {dataset_path}")
        print(f"   Expected path: {dataset_path}")

# =================================================================
# COMPARATIVE ANALYSIS
# =================================================================

if len(all_results) > 1:
    print("\n" + "="*80)
    print("CROSS-DATASET COMPARATIVE ANALYSIS")
    print("="*80)

    # Create comparison DataFrame
    comparison_df = pd.DataFrame([{
        'Dataset': r['dataset_name'],
        'Samples': r['total_samples'],
        'Accuracy': f"{r['accuracy']:.4f}",
        'Precision': f"{r['precision']:.4f}",
        'Recall': f"{r['recall']:.4f}",
        'F1-Score': f"{r['f1_score']:.4f}",
        'ROC-AUC': f"{r['roc_auc']:.4f}"
    } for r in all_results])

    print("\n")
    print(comparison_df.to_string(index=False))

    # Save comparison
    comparison_df.to_csv(f'{RESULTS_BASE_DIR}/cross_dataset_comparison.csv', index=False)
    print(f"\n✓ Comparison saved to: {RESULTS_BASE_DIR}/cross_dataset_comparison.csv")

    # Create comparison visualization
    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']

    fig, axes = plt.subplots(2, 3, figsize=(18, 10), dpi=300)
    axes = axes.flatten()

    for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
        ax = axes[idx]
        datasets = [r['dataset_name'] for r in all_results]
        values = [r[metric] for r in all_results]

        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(datasets)))
        bars = ax.bar(datasets, values, color=colors, edgecolor='black', alpha=0.7)

        ax.set_ylabel(metric_name, fontsize=12, fontweight='bold')
        ax.set_ylim(0, 1.1)
        ax.set_title(f'{metric_name} Across Datasets', fontsize=13, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)

        # Add value labels on bars
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{value:.3f}',
                   ha='center', va='bottom', fontsize=10, fontweight='bold')

        ax.set_xticklabels(datasets, rotation=45, ha='right')

    # Remove extra subplot
    fig.delaxes(axes[5])

    plt.suptitle('Cross-Dataset Generalization Performance',
                 fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig(f'{RESULTS_BASE_DIR}/cross_dataset_comparison.png', dpi=300, bbox_inches='tight')
    print(f"✓ Comparison plot saved to: {RESULTS_BASE_DIR}/cross_dataset_comparison.png")
    plt.close()

    # Generalization Gap Analysis
    print("\n" + "="*80)
    print("GENERALIZATION GAP ANALYSIS")
    print("="*80)

    if 'Original_Dataset' in [r['dataset_name'] for r in all_results]:
        original_acc = next(r['accuracy'] for r in all_results if r['dataset_name'] == 'Original_Dataset')

        print(f"\nOriginal Dataset Accuracy: {original_acc:.4f} ({original_acc*100:.2f}%)")
        print("\nGeneralization Gaps:")
        print(f"{'Dataset':<30} {'Gap':<15} {'Percentage'}")
        print("-"*60)
        for result in all_results:
            if result['dataset_name'] != 'Original_Dataset':
                gap = original_acc - result['accuracy']
                print(f"{result['dataset_name']:<30} {gap:+.4f} {' '*8} {gap*100:+.2f}%")

elif len(all_results) == 1:
    print(f"\n⚠ Only one dataset tested: {all_results[0]['dataset_name']}")
    print("Please download and add more datasets for cross-dataset comparison.")
else:
    print("\n⚠ No datasets were successfully tested.")
    print("Please check dataset paths and try again.")

print("\n" + "="*80)
print("CROSS-DATASET TESTING COMPLETE!")
print("="*80)
print(f"\nAll results saved to: {RESULTS_BASE_DIR}/")

"""# Test Samples"""

import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import random
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Paths
results_dir = '/content/drive/MyDrive/ablation_study_results'
model_path = f'{results_dir}/enhanced/best_model.keras'
test_path = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'

# Create the directory for saving plots if it doesn't exist
plot_save_dir = f'{results_dir}/enhanced_full'
os.makedirs(plot_save_dir, exist_ok=True)


# Load the trained model
print("Loading trained model...")
model = load_model(model_path)
print("Model loaded successfully!")

# Class labels
class_labels = {0: 'Fake', 1: 'Real'}

def predict_single_image(img_path, model):
    """Predict a single image and return results"""
    # Load and preprocess image
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = img_array / 255.0  # Rescale to match training
    img_array = np.expand_dims(img_array, axis=0)

    # Make prediction
    prediction = model.predict(img_array, verbose=0)
    predicted_class = np.argmax(prediction[0])
    confidence = prediction[0][predicted_class] * 100

    return predicted_class, confidence, prediction[0]

def test_random_images(num_images=10):
    """Test random images from test set and display results"""

    # Get image paths from both classes
    fake_dir = os.path.join(test_path, 'fake')
    real_dir = os.path.join(test_path, 'real')

    fake_images = [os.path.join(fake_dir, img) for img in os.listdir(fake_dir)
                   if img.lower().endswith(('.png', '.jpg', '.jpeg'))]
    real_images = [os.path.join(real_dir, img) for img in os.listdir(real_dir)
                   if img.lower().endswith(('.png', '.jpg', '.jpeg'))]

    # Select random images (half from each class)
    num_per_class = num_images // 2
    selected_fake = random.sample(fake_images, min(num_per_class, len(fake_images)))
    selected_real = random.sample(real_images, min(num_per_class, len(real_images)))

    all_selected = selected_fake + selected_real
    actual_labels = [0] * len(selected_fake) + [1] * len(selected_real)

    # Shuffle together
    combined = list(zip(all_selected, actual_labels))
    random.shuffle(combined)
    all_selected, actual_labels = zip(*combined)

    # Create figure
    num_cols = 5
    num_rows = (len(all_selected) + num_cols - 1) // num_cols
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))
    axes = axes.flatten() if num_rows > 1 else [axes] if num_rows == 1 else axes

    results = []
    correct_predictions = 0

    print(f"\n{'='*80}")
    print(f"Testing {len(all_selected)} random images from test set")
    print(f"{'='*80}\n")

    for idx, (img_path, actual_label) in enumerate(zip(all_selected, actual_labels)):
        # Predict
        predicted_class, confidence, full_pred = predict_single_image(img_path, model)

        # Check if correct
        is_correct = (predicted_class == actual_label)
        if is_correct:
            correct_predictions += 1

        # Store results
        results.append({
            'image': os.path.basename(img_path),
            'actual': class_labels[actual_label],
            'predicted': class_labels[predicted_class],
            'confidence': confidence,
            'fake_prob': full_pred[0] * 100,
            'real_prob': full_pred[1] * 100,
            'correct': is_correct
        })

        # Display image
        img = image.load_img(img_path, target_size=(224, 224))
        axes[idx].imshow(img)
        axes[idx].axis('off')

        # Title with color coding
        title_color = 'green' if is_correct else 'red'
        title = f"Actual: {class_labels[actual_label]}\n"
        title += f"Predicted: {class_labels[predicted_class]}\n"
        title += f"Confidence: {confidence:.1f}%"
        axes[idx].set_title(title, fontsize=10, color=title_color, weight='bold')

        # Print detailed results
        status = "✓ CORRECT" if is_correct else "✗ WRONG"
        print(f"Image {idx + 1}: {os.path.basename(img_path)}")
        print(f"  Actual Label:     {class_labels[actual_label]}")
        print(f"  Predicted Label:  {class_labels[predicted_class]}")
        print(f"  Confidence:       {confidence:.2f}%")
        print(f"  Fake Probability: {full_pred[0] * 100:.2f}%")
        print(f"  Real Probability: {full_pred[1] * 100:.2f}%")
        print(f"  Status:           {status}")
        print(f"{'-'*80}\n")

    # Hide unused subplots
    for idx in range(len(all_selected), len(axes)):
        axes[idx].axis('off')

    plt.tight_layout()
    plt.savefig(f'{plot_save_dir}/test_predictions.png', dpi=150, bbox_inches='tight')
    plt.show()

    # Summary statistics
    accuracy = (correct_predictions / len(all_selected)) * 100
    print(f"\n{'='*80}")
    print(f"SUMMARY")
    print(f"{'='*80}")
    print(f"Total Images Tested:     {len(all_selected)}")
    print(f"Correct Predictions:     {correct_predictions}")
    print(f"Wrong Predictions:       {len(all_selected) - correct_predictions}")
    print(f"Accuracy:                {accuracy:.2f}%")
    print(f"{'='*80}\n")

    return results

def test_specific_images(image_paths, actual_labels):
    """Test specific images by providing paths and their actual labels"""

    print(f"\n{'='*80}")
    print(f"Testing {len(image_paths)} specific images")
    print(f"{'='*80}\n")

    # Create figure
    num_cols = min(5, len(image_paths))
    num_rows = (len(image_paths) + num_cols - 1) // num_cols
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows))

    if len(image_paths) == 1:
        axes = [axes]
    else:
        axes = axes.flatten() if num_rows > 1 else axes

    results = []
    correct_predictions = 0

    for idx, (img_path, actual_label) in enumerate(zip(image_paths, actual_labels)):
        # Predict
        predicted_class, confidence, full_pred = predict_single_image(img_path, model)

        # Check if correct
        is_correct = (predicted_class == actual_label)
        if is_correct:
            correct_predictions += 1

        # Store results
        results.append({
            'image': os.path.basename(img_path),
            'actual': class_labels[actual_label],
            'predicted': class_labels[predicted_class],
            'confidence': confidence,
            'fake_prob': full_pred[0] * 100,
            'real_prob': full_pred[1] * 100,
            'correct': is_correct
        })

        # Display image
        img = image.load_img(img_path, target_size=(224, 224))
        axes[idx].imshow(img)
        axes[idx].axis('off')

        # Title with color coding
        title_color = 'green' if is_correct else 'red'
        title = f"Actual: {class_labels[actual_label]}\n"
        title += f"Predicted: {class_labels[predicted_class]}\n"
        title += f"Confidence: {confidence:.1f}%"
        axes[idx].set_title(title, fontsize=10, color=title_color, weight='bold')

        # Print detailed results
        status = "✓ CORRECT" if is_correct else "✗ WRONG"
        print(f"Image {idx + 1}: {os.path.basename(img_path)}")
        print(f"  Actual Label:     {class_labels[actual_label]}")
        print(f"  Predicted Label:  {class_labels[predicted_class]}")
        print(f"  Confidence:       {confidence:.2f}%")
        print(f"  Fake Probability: {full_pred[0] * 100:.2f}%")
        print(f"  Real Probability: {full_pred[1] * 100:.2f}%")
        print(f"  Status:           {status}")
        print(f"{'-'*80}\n")

    plt.tight_layout()
    plt.show()

    # Summary
    accuracy = (correct_predictions / len(image_paths)) * 100
    print(f"\n{'='*80}")
    print(f"SUMMARY")
    print(f"{'='*80}")
    print(f"Total Images Tested:     {len(image_paths)}")
    print(f"Correct Predictions:     {correct_predictions}")
    print(f"Wrong Predictions:       {len(image_paths) - correct_predictions}")
    print(f"Accuracy:                {accuracy:.2f}%")
    print(f"{'='*80}\n")

    return results

# ===== USAGE EXAMPLES =====

# Option 1: Test random images from test set
print("\n📸 Testing random images from test set...")
results = test_random_images(num_images=10)

# Option 2: Test specific images (uncomment and modify paths as needed)
"""
specific_paths = [
    '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test/fake/image1.jpg',
    '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test/real/image2.jpg',
]
specific_labels = [0, 1]  # 0 for Fake, 1 for Real
results = test_specific_images(specific_paths, specific_labels)
"""

print("\n✅ Testing complete! Results saved to:", f'{plot_save_dir}/test_predictions.png')

import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import random
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Paths
results_dir = '/content/drive/MyDrive/ablation_study_results'
model_path = f'{results_dir}/enhanced/best_model.keras'
test_path = '/content/deepfake-and-real-images/Dataset/Test'

# Create the directory for saving plots if it doesn't exist
plot_save_dir = f'{results_dir}/enhanced_full'
os.makedirs(plot_save_dir, exist_ok=True)


# Load the trained model
print("Loading trained model...")
model = load_model(model_path)
print("Model loaded successfully!")

# Class labels
class_labels = {0: 'Fake', 1: 'Real'}

def predict_single_image(img_path, model):
    """Predict a single image and return results"""
    # Load and preprocess image
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = img_array / 255.0  # Rescale to match training
    img_array = np.expand_dims(img_array, axis=0)

    # Make prediction
    prediction = model.predict(img_array, verbose=0)
    predicted_class = np.argmax(prediction[0])
    confidence = prediction[0][predicted_class] * 100

    return predicted_class, confidence, prediction[0]

def test_random_images(num_images=10):
    """Test random images from test set and display results"""

    # Get image paths from both classes
    fake_dir = os.path.join(test_path, 'Fake')
    real_dir = os.path.join(test_path, 'Real')

    fake_images = [os.path.join(fake_dir, img) for img in os.listdir(fake_dir)
                   if img.lower().endswith(('.png', '.jpg', '.jpeg'))]
    real_images = [os.path.join(real_dir, img) for img in os.listdir(real_dir)
                   if img.lower().endswith(('.png', '.jpg', '.jpeg'))]

    # Select random images (half from each class)
    num_per_class = num_images // 2
    selected_fake = random.sample(fake_images, min(num_per_class, len(fake_images)))
    selected_real = random.sample(real_images, min(num_per_class, len(real_images)))

    all_selected = selected_fake + selected_real
    actual_labels = [0] * len(selected_fake) + [1] * len(selected_real)

    # Shuffle together
    combined = list(zip(all_selected, actual_labels))
    random.shuffle(combined)
    all_selected, actual_labels = zip(*combined)

    # Create figure
    num_cols = 5
    num_rows = (len(all_selected) + num_cols - 1) // num_cols
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))
    axes = axes.flatten() if num_rows > 1 else [axes] if num_rows == 1 else axes

    results = []
    correct_predictions = 0

    print(f"\n{'='*80}")
    print(f"Testing {len(all_selected)} random images from test set")
    print(f"{'='*80}\n")

    for idx, (img_path, actual_label) in enumerate(zip(all_selected, actual_labels)):
        # Predict
        predicted_class, confidence, full_pred = predict_single_image(img_path, model)

        # Check if correct
        is_correct = (predicted_class == actual_label)
        if is_correct:
            correct_predictions += 1

        # Store results
        results.append({
            'image': os.path.basename(img_path),
            'actual': class_labels[actual_label],
            'predicted': class_labels[predicted_class],
            'confidence': confidence,
            'fake_prob': full_pred[0] * 100,
            'real_prob': full_pred[1] * 100,
            'correct': is_correct
        })

        # Display image
        img = image.load_img(img_path, target_size=(224, 224))
        axes[idx].imshow(img)
        axes[idx].axis('off')

        # Title with color coding
        title_color = 'green' if is_correct else 'red'
        title = f"Actual: {class_labels[actual_label]}\n"
        title += f"Predicted: {class_labels[predicted_class]}\n"
        title += f"Confidence: {confidence:.1f}%"
        axes[idx].set_title(title, fontsize=10, color=title_color, weight='bold')

        # Print detailed results
        status = "✓ CORRECT" if is_correct else "✗ WRONG"
        print(f"Image {idx + 1}: {os.path.basename(img_path)}")
        print(f"  Actual Label:     {class_labels[actual_label]}")
        print(f"  Predicted Label:  {class_labels[predicted_class]}")
        print(f"  Confidence:       {confidence:.2f}%")
        print(f"  Fake Probability: {full_pred[0] * 100:.2f}%")
        print(f"  Real Probability: {full_pred[1] * 100:.2f}%")
        print(f"  Status:           {status}")
        print(f"{'-'*80}\n")

    # Hide unused subplots
    for idx in range(len(all_selected), len(axes)):
        axes[idx].axis('off')

    plt.tight_layout()
    plt.savefig(f'{plot_save_dir}/test_predictions.png', dpi=150, bbox_inches='tight')
    plt.show()

    # Summary statistics
    accuracy = (correct_predictions / len(all_selected)) * 100
    print(f"\n{'='*80}")
    print(f"SUMMARY")
    print(f"{'='*80}")
    print(f"Total Images Tested:     {len(all_selected)}")
    print(f"Correct Predictions:     {correct_predictions}")
    print(f"Wrong Predictions:       {len(all_selected) - correct_predictions}")
    print(f"Accuracy:                {accuracy:.2f}%")
    print(f"{'='*80}\n")

    return results

def test_specific_images(image_paths, actual_labels):
    """Test specific images by providing paths and their actual labels"""

    print(f"\n{'='*80}")
    print(f"Testing {len(image_paths)} specific images")
    print(f"{'='*80}\n")

    # Create figure
    num_cols = min(5, len(image_paths))
    num_rows = (len(image_paths) + num_cols - 1) // num_cols
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows))

    if len(image_paths) == 1:
        axes = [axes]
    else:
        axes = axes.flatten() if num_rows > 1 else axes

    results = []
    correct_predictions = 0

    for idx, (img_path, actual_label) in enumerate(zip(image_paths, actual_labels)):
        # Predict
        predicted_class, confidence, full_pred = predict_single_image(img_path, model)

        # Check if correct
        is_correct = (predicted_class == actual_label)
        if is_correct:
            correct_predictions += 1

        # Store results
        results.append({
            'image': os.path.basename(img_path),
            'actual': class_labels[actual_label],
            'predicted': class_labels[predicted_class],
            'confidence': confidence,
            'fake_prob': full_pred[0] * 100,
            'real_prob': full_pred[1] * 100,
            'correct': is_correct
        })

        # Display image
        img = image.load_img(img_path, target_size=(224, 224))
        axes[idx].imshow(img)
        axes[idx].axis('off')

        # Title with color coding
        title_color = 'green' if is_correct else 'red'
        title = f"Actual: {class_labels[actual_label]}\n"
        title += f"Predicted: {class_labels[predicted_class]}\n"
        title += f"Confidence: {confidence:.1f}%"
        axes[idx].set_title(title, fontsize=10, color=title_color, weight='bold')

        # Print detailed results
        status = "✓ CORRECT" if is_correct else "✗ WRONG"
        print(f"Image {idx + 1}: {os.path.basename(img_path)}")
        print(f"  Actual Label:     {class_labels[actual_label]}")
        print(f"  Predicted Label:  {class_labels[predicted_class]}")
        print(f"  Confidence:       {confidence:.2f}%")
        print(f"  Fake Probability: {full_pred[0] * 100:.2f}%")
        print(f"  Real Probability: {full_pred[1] * 100:.2f}%")
        print(f"  Status:           {status}")
        print(f"{'-'*80}\n")

    plt.tight_layout()
    plt.show()

    # Summary
    accuracy = (correct_predictions / len(image_paths)) * 100
    print(f"\n{'='*80}")
    print(f"SUMMARY")
    print(f"{'='*80}")
    print(f"Total Images Tested:     {len(image_paths)}")
    print(f"Correct Predictions:     {correct_predictions}")
    print(f"Wrong Predictions:       {len(image_paths) - correct_predictions}")
    print(f"Accuracy:                {accuracy:.2f}%")
    print(f"{'='*80}\n")

    return results

# ===== USAGE EXAMPLES =====

# Option 1: Test random images from test set
print("\n📸 Testing random images from test set...")
results = test_random_images(num_images=10)

# Option 2: Test specific images (uncomment and modify paths as needed)
"""
specific_paths = [
    '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test/fake/image1.jpg',
    '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test/real/image2.jpg',
]
specific_labels = [0, 1]  # 0 for Fake, 1 for Real
results = test_specific_images(specific_paths, specific_labels)
"""

print("\n✅ Testing complete! Results saved to:", f'{plot_save_dir}/test_predictions.png')

"""# All Comparison Results"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import json
from google.colab import drive
from matplotlib.colors import LinearSegmentedColormap

# Mount Google Drive
drive.mount('/content/drive')

# Results directory
results_dir = '/content/drive/MyDrive/ablation_study_results'

def load_all_results():
    """Load results from all models"""
    models = ['Stage 1', 'Stage 2', 'Stage 3', 'Proposed Model']
    model_names = ['Stage 1', 'Stage 2', 'Stage 3', 'Proposed Model']

    results = {}
    histories = {}

    for model_folder, model_name in zip(models, model_names):
        try:
            # Load results
            results_file = f'{results_dir}/{model_folder}/results.json'
            if os.path.exists(results_file):
                with open(results_file, 'r') as f:
                    results[model_name] = json.load(f)

            # Load history
            history_file = f'{results_dir}/{model_folder}/history.pkl'
            if os.path.exists(history_file):
                with open(history_file, 'rb') as f:
                    histories[model_name] = pickle.load(f)

            print(f"✓ Loaded results for {model_name}")
        except Exception as e:
            print(f"✗ Failed to load {model_name}: {str(e)}")

    return results, histories

def create_all_individual_plots(results, histories):
    """Create all individual plots"""
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

    print("🎨 Creating all individual plots...")
    print("="*60)

    # ==================== PLOT 1: Test Accuracy Comparison ====================
    print("📊 Creating Plot 1: Test Accuracy Comparison...")
    models = list(results.keys())
    test_accs = [results[model]['test_accuracy'] for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, test_accs, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('Test Accuracy Comparison', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Test Accuracy', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')

    for bar, acc in zip(bars, test_accs):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{acc:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=12)

    plt.ylim(min(test_accs) - 0.02, max(test_accs) + 0.03)
    plt.tight_layout()
    plt.savefig(f"{results_dir}/01_test_accuracy_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 2: ROC-AUC Comparison ====================
    print("📊 Creating Plot 2: ROC-AUC Comparison...")
    roc_aucs = [results[model]['roc_auc'] for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, roc_aucs, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('ROC-AUC Score Comparison', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('ROC-AUC Score', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')

    for bar, auc in zip(bars, roc_aucs):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{auc:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=12)

    plt.ylim(min(roc_aucs) - 0.02, max(roc_aucs) + 0.03)
    plt.tight_layout()
    plt.savefig(f"{results_dir}/02_roc_auc_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 3: Training History Accuracy ====================
    print("📊 Creating Plot 3: Training History - Accuracy...")
    plt.figure(figsize=(14, 8))

    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['accuracy']) + 1)
        plt.plot(epochs, history['accuracy'], label=f'{name} (Train)',
                color=colors[i], linestyle='-', alpha=0.9, linewidth=2.5, marker='o', markersize=4)
        plt.plot(epochs, history['val_accuracy'], label=f'{name} (Val)',
                color=colors[i], linestyle='--', alpha=0.9, linewidth=2.5, marker='s', markersize=4)

    plt.title('Training & Validation Accuracy History', fontsize=18, fontweight='bold', pad=20)
    plt.xlabel('Epochs', fontsize=14)
    plt.ylabel('Accuracy', fontsize=14)
    plt.legend(fontsize=12, loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/03_training_accuracy_history.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 4: Training History Loss ====================
    print("📊 Creating Plot 4: Training History - Loss...")
    plt.figure(figsize=(14, 8))

    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['loss']) + 1)
        plt.plot(epochs, history['loss'], label=f'{name} (Train)',
                color=colors[i], linestyle='-', alpha=0.9, linewidth=2.5, marker='o', markersize=4)
        plt.plot(epochs, history['val_loss'], label=f'{name} (Val)',
                color=colors[i], linestyle='--', alpha=0.9, linewidth=2.5, marker='s', markersize=4)

    plt.title('Training & Validation Loss History', fontsize=18, fontweight='bold', pad=20)
    plt.xlabel('Epochs', fontsize=14)
    plt.ylabel('Loss', fontsize=14)
    plt.legend(fontsize=12, loc='upper right')
    plt.grid(True, alpha=0.3)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/04_training_loss_history.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 5: Model Complexity ====================
    print("📊 Creating Plot 5: Model Complexity...")
    param_counts = [results[model]['total_params'] for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, param_counts, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('Model Complexity Comparison\n(Total Parameters)', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Number of Parameters', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')

    # Format y-axis
    ax = plt.gca()
    if max(param_counts) >= 1e6:
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))
    elif max(param_counts) >= 1e3:
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e3:.0f}K'))

    for bar, count in zip(bars, param_counts):
        if count >= 1e6:
            label = f'{count/1e6:.2f}M'
        elif count >= 1e3:
            label = f'{count/1e3:.0f}K'
        else:
            label = f'{count:,}'

        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(param_counts)*0.02,
                label, ha='center', va='bottom', fontweight='bold', fontsize=11)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/05_model_complexity_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 6: Precision, Recall, F1 ====================
    print("📊 Creating Plot 6: Precision, Recall & F1-Score...")
    precision_scores = [results[model]['precision'] for model in models]
    recall_scores = [results[model]['recall'] for model in models]
    f1_scores = [results[model]['f1_score'] for model in models]

    x = np.arange(len(models))
    width = 0.25

    plt.figure(figsize=(14, 8))

    bars1 = plt.bar(x - width, precision_scores, width, label='Precision', alpha=0.8,
                    color='#FF9999', edgecolor='black', linewidth=1)
    bars2 = plt.bar(x, recall_scores, width, label='Recall', alpha=0.8,
                    color='#66B2FF', edgecolor='black', linewidth=1)
    bars3 = plt.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8,
                    color='#99FF99', edgecolor='black', linewidth=1)

    for bars, scores in zip([bars1, bars2, bars3], [precision_scores, recall_scores, f1_scores]):
        for bar, score in zip(bars, scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)

    plt.xlabel('Model Architecture', fontsize=14)
    plt.ylabel('Score', fontsize=14)
    plt.title('Precision, Recall & F1-Score Comparison', fontsize=18, fontweight='bold', pad=20)
    plt.xticks(x, models, rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.legend(fontsize=12, loc='upper left')
    plt.grid(True, alpha=0.3, axis='y')

    all_scores = precision_scores + recall_scores + f1_scores
    plt.ylim(min(all_scores) - 0.05, max(all_scores) + 0.08)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/06_precision_recall_f1_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 7: Accuracy vs Complexity ====================
    print("📊 Creating Plot 7: Accuracy vs Complexity...")
    plt.figure(figsize=(12, 8))

    scatter = plt.scatter(param_counts, test_accs, c=colors[:len(models)], s=200,
                         alpha=0.8, edgecolors='black', linewidth=2)

    for i, model in enumerate(models):
        plt.annotate(model, (param_counts[i], test_accs[i]),
                    xytext=(10, 10), textcoords='offset points',
                    fontsize=12, fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor=colors[i], alpha=0.3))

    plt.title('Test Accuracy vs Model Complexity', fontsize=18, fontweight='bold', pad=20)
    plt.xlabel('Number of Parameters', fontsize=14)
    plt.ylabel('Test Accuracy', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    ax = plt.gca()
    if max(param_counts) >= 1e6:
        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))
        plt.xlabel('Number of Parameters (Millions)', fontsize=14)
    elif max(param_counts) >= 1e3:
        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e3:.0f}K'))
        plt.xlabel('Number of Parameters (Thousands)', fontsize=14)

    z = np.polyfit(param_counts, test_accs, 1)
    p = np.poly1d(z)
    plt.plot(param_counts, p(param_counts), "r--", alpha=0.6, linewidth=2, label='Trend Line')
    plt.legend(fontsize=12)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/07_accuracy_vs_complexity_scatter.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 8: Performance Improvement ====================
    print("📊 Creating Plot 8: Performance Improvement...")
    baseline_acc = results['Baseline']['test_accuracy']
    improvements = [(results[model]['test_accuracy'] - baseline_acc) * 100 for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, improvements, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('Test Accuracy Improvement over Baseline (%)', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Improvement (%)', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Baseline Performance')

    for i, (bar, imp) in enumerate(zip(bars, improvements)):
        if imp > 0:
            bar.set_color('#4CAF50')
        elif imp < 0:
            bar.set_color('#F44336')
        else:
            bar.set_color('#9E9E9E')

        plt.text(bar.get_x() + bar.get_width()/2,
                bar.get_height() + (0.2 if imp >= 0 else -0.3),
                f'{imp:+.2f}%', ha='center', va='bottom' if imp >= 0 else 'top',
                fontweight='bold', fontsize=12)

    plt.legend(fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{results_dir}/08_performance_improvement_analysis.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 9: Parameter Efficiency ====================
    print("📊 Creating Plot 9: Parameter Efficiency...")
    efficiency = [results[model]['test_accuracy'] / (results[model]['total_params'] / 1e6)
                  for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, efficiency, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('Parameter Efficiency: Accuracy per Million Parameters', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Efficiency Score\n(Accuracy / Million Parameters)', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')

    for bar, eff in zip(bars, efficiency):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{eff:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)

    max_efficiency_idx = efficiency.index(max(efficiency))
    bars[max_efficiency_idx].set_color('#FFD700')
    bars[max_efficiency_idx].set_edgecolor('#FF8C00')
    bars[max_efficiency_idx].set_linewidth(3)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/09_parameter_efficiency_analysis.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 10: Performance Heatmap ====================
    print("📊 Creating Plot 10: Performance Heatmap...")
    metrics_for_heatmap = ['test_accuracy', 'roc_auc', 'precision', 'recall', 'f1_score', 'best_val_acc']
    metric_labels = ['Test Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score', 'Best Val Acc']

    heatmap_data = []
    model_names = []

    for model_name in results.keys():
        model_names.append(model_name)
        row = [results[model_name][metric] for metric in metrics_for_heatmap]
        heatmap_data.append(row)

    heatmap_array = np.array(heatmap_data)
    normalized_data = np.zeros_like(heatmap_array)
    for i in range(heatmap_array.shape[1]):
        col = heatmap_array[:, i]
        normalized_data[:, i] = (col - col.min()) / (col.max() - col.min())

    plt.figure(figsize=(14, 10))

    colors_heatmap = ['#d73027', '#fc8d59', '#fee08b', '#d9ef8b', '#91cf60', '#4d9221']
    custom_cmap = LinearSegmentedColormap.from_list('custom', colors_heatmap, N=100)

    sns.heatmap(normalized_data,
                xticklabels=metric_labels,
                yticklabels=model_names,
                annot=heatmap_array,
                fmt='.4f',
                cmap=custom_cmap,
                center=0.5,
                square=True,
                linewidths=1,
                cbar_kws={"shrink": .8, "label": "Normalized Performance (0-1 scale)"},
                annot_kws={"fontsize": 11, "fontweight": "bold"})

    plt.title('Model Performance Heatmap\n(Normalized Colors with Actual Values)',
              fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Models', fontsize=14, fontweight='bold')
    plt.xlabel('Performance Metrics', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(rotation=0, fontsize=12)

    for i, metric in enumerate(metrics_for_heatmap):
        best_model_idx = np.argmax(heatmap_array[:, i])
        plt.text(i + 0.7, best_model_idx + 0.3, '★', fontsize=20, color='red', fontweight='bold')

    plt.tight_layout()
    plt.savefig(f"{results_dir}/10_comprehensive_performance_heatmap.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 11: Radar Chart ====================
    print("📊 Creating Plot 11: Overall Performance Radar Chart...")
    metrics = ['Test Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']
    N = len(metrics)
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))

    for i, model in enumerate(models):
        values = [
            results[model]['test_accuracy'],
            results[model]['roc_auc'],
            results[model]['precision'],
            results[model]['recall'],
            results[model]['f1_score']
        ]
        values += values[:1]

        ax.plot(angles, values, 'o-', linewidth=3, label=model, color=colors[i], alpha=0.8, markersize=8)
        ax.fill(angles, values, alpha=0.15, color=colors[i])

        for angle, value, metric in zip(angles[:-1], values[:-1], metrics):
            ax.text(angle, value + 0.02, f'{value:.3f}', ha='center', va='center',
                   fontsize=9, fontweight='bold', color=colors[i],
                   bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(metrics, fontsize=14, fontweight='bold')
    ax.set_ylim(0, 1)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=12)
    ax.grid(True, alpha=0.3)

    plt.title('Overall Performance Comparison\n(Radar Chart)', fontsize=18, fontweight='bold', pad=30)
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=12)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/11_overall_performance_radar_chart.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 12: Convergence Analysis ====================
    print("📊 Creating Plot 12: Training Convergence Analysis...")
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))

    # Learning Rate Analysis
    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['accuracy']) + 1)
        acc_improvements = np.diff([0] + history['accuracy'])
        ax1.plot(epochs, acc_improvements, label=name, color=colors[i],
                marker='o', markersize=4, linewidth=2, alpha=0.8)

    ax1.set_title('Training Accuracy Improvement per Epoch', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy Improvement')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)

    # Overfitting Analysis
    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['accuracy']) + 1)
        overfitting_gap = np.array(history['accuracy']) - np.array(history['val_accuracy'])
        ax2.plot(epochs, overfitting_gap, label=name, color=colors[i],
                marker='s', markersize=4, linewidth=2, alpha=0.8)

    ax2.set_title('Overfitting Analysis\n(Training - Validation Accuracy)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Overfitting Gap')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.axhline(y=0, color='green', linestyle='--', alpha=0.7, label='Perfect Fit')
    ax2.axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='Overfitting Threshold')

    # Loss Convergence Stability
    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['val_loss']) + 1)
        window_size = min(3, len(history['val_loss']))
        if len(history['val_loss']) >= window_size:
            moving_avg = np.convolve(history['val_loss'], np.ones(window_size)/window_size, mode='valid')
            moving_epochs = range(window_size, len(history['val_loss']) + 1)
            ax3.plot(moving_epochs, moving_avg, label=f'{name} (Smoothed)',
                    color=colors[i], linewidth=3, alpha=0.8)

        ax3.plot(epochs, history['val_loss'], color=colors[i],
                alpha=0.3, linewidth=1, linestyle='--')

    ax3.set_title('Validation Loss Convergence\n(Smoothed Trends)', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Validation Loss')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # Training Efficiency
    best_val_reached = {}
    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['val_accuracy']) + 1)
        cumulative_best = []
        current_best = 0

        for val_acc in history['val_accuracy']:
            current_best = max(current_best, val_acc)
            cumulative_best.append(current_best)

        ax4.plot(epochs, cumulative_best, label=name, color=colors[i],
                marker='d', markersize=4, linewidth=2.5, alpha=0.8)

        best_val_reached[name] = current_best

    ax4.set_title('Training Efficiency\n(Best Validation Accuracy Achieved)', fontsize=14, fontweight='bold')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Best Validation Accuracy')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    for i, (name, best_val) in enumerate(best_val_reached.items()):
        ax4.annotate(f'{best_val:.4f}',
                    xy=(len(histories[name]['val_accuracy']), best_val),
                    xytext=(10, 0), textcoords='offset points',
                    fontsize=9, fontweight='bold', color=colors[i])

    plt.tight_layout()
    plt.savefig(f"{results_dir}/12_training_convergence_analysis.png", dpi=300, bbox_inches='tight')
    plt.show()

    print("✅ All individual plots created successfully!")
    print("="*60)
    print(f"📁 All plots saved in: {results_dir}")
    print("\n📋 Generated Plots:")
    print("   01_test_accuracy_comparison.png")
    print("   02_roc_auc_comparison.png")
    print("   03_training_accuracy_history.png")
    print("   04_training_loss_history.png")
    print("   05_model_complexity_comparison.png")
    print("   06_precision_recall_f1_comparison.png")
    print("   07_accuracy_vs_complexity_scatter.png")
    print("   08_performance_improvement_analysis.png")
    print("   09_parameter_efficiency_analysis.png")
    print("   10_comprehensive_performance_heatmap.png")
    print("   11_overall_performance_radar_chart.png")
    print("   12_training_convergence_analysis.png")

def generate_summary_report(results, histories):
    """Generate a comprehensive summary report"""
    print("\n" + "="*80)
    print("📊 COMPREHENSIVE ABLATION STUDY SUMMARY")
    print("="*80)

    # Create summary DataFrame
    df = pd.DataFrame(results).T
    df = df.round(4)
    df.index.name = 'Model'

    # Add efficiency metric
    df['efficiency'] = df['test_accuracy'] / (df['total_params'] / 1e6)
    df['efficiency'] = df['efficiency'].round(2)

    print("\n🏆 PERFORMANCE SUMMARY TABLE:")
    print("-" * 80)
    print(df.to_string())
    print("-" * 80)

    # Key findings
    best_accuracy = df['test_accuracy'].max()
    best_model = df['test_accuracy'].idxmax()
    most_efficient = df['efficiency'].idxmax()
    best_roc_auc = df['roc_auc'].idxmax()

    print(f"\n🎯 KEY FINDINGS:")
    print(f"   • Best Overall Model: {best_model} (Accuracy: {best_accuracy:.4f})")
    print(f"   • Most Efficient Model: {most_efficient} (Efficiency: {df.loc[most_efficient, 'efficiency']:.2f})")
    print(f"   • Best ROC-AUC: {best_roc_auc} (ROC-AUC: {df.loc[best_roc_auc, 'roc_auc']:.4f})")

    # Improvement analysis
    baseline_acc = df.loc['Baseline', 'test_accuracy']
    print(f"\n📈 IMPROVEMENTS OVER BASELINE:")
    for model in df.index:
        if model != 'Baseline':
            improvement = (df.loc[model, 'test_accuracy'] - baseline_acc) * 100
            print(f"   • {model}: {improvement:+.2f}% improvement")

    # Parameter efficiency ranking
    print(f"\n⚡ PARAMETER EFFICIENCY RANKING:")
    efficiency_ranking = df.sort_values('efficiency', ascending=False)
    for i, model in enumerate(efficiency_ranking.index, 1):
        eff = efficiency_ranking.loc[model, 'efficiency']
        params = efficiency_ranking.loc[model, 'total_params']
        print(f"   {i}. {model}: {eff:.2f} (Acc/M params) - {params:,} total parameters")

    # Training convergence analysis
    if histories:
        print(f"\n🔍 TRAINING CONVERGENCE ANALYSIS:")
        for model_name, history in histories.items():
            epochs_trained = len(history['accuracy'])
            final_train_acc = history['accuracy'][-1]
            final_val_acc = history['val_accuracy'][-1]
            best_val_acc = max(history['val_accuracy'])
            best_epoch = history['val_accuracy'].index(best_val_acc) + 1
            overfitting = final_train_acc - final_val_acc

            print(f"\n   {model_name}:")
            print(f"     • Epochs Trained: {epochs_trained}")
            print(f"     • Final Train Accuracy: {final_train_acc:.4f}")
            print(f"     • Final Validation Accuracy: {final_val_acc:.4f}")
            print(f"     • Best Validation Accuracy: {best_val_acc:.4f} (Epoch {best_epoch})")
            print(f"     • Overfitting Indicator: {overfitting:.4f}")

            if overfitting > 0.05:
                print(f"     ⚠️  Potential overfitting detected!")
            elif overfitting < -0.02:
                print(f"     📈 Model may benefit from more training")
            else:
                print(f"     ✅ Good training convergence")

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    try:
        print("🚀 ABLATION STUDY - INDIVIDUAL PLOTS GENERATOR")
        print("="*80)
        print("Loading results from all models...")

        results, histories = load_all_results()

        if not results:
            print("❌ No results found! Make sure you've run all model training scripts first.")
            print("Expected directory structure:")
            print(f"  {results_dir}/")
            print("    ├── baseline/")
            print("    ├── batchnorm/")
            print("    ├── attention/")
            print("    └── enhanced_full/")
        else:
            print(f"✅ Successfully loaded results for {len(results)} models")

            # Create all individual plots
            create_all_individual_plots(results, histories)

            # Generate summary report
            generate_summary_report(results, histories)

            # Save summary report
            summary_path = f"{results_dir}/individual_plots_summary.txt"
            with open(summary_path, 'w') as f:
                f.write("INDIVIDUAL PLOTS GENERATED\n")
                f.write("="*50 + "\n\n")
                f.write("Generated 12 individual plots for comprehensive analysis\n")
                f.write("Each plot saved as high-resolution PNG file\n\n")
                f.write("Plot List:\n")
                for i in range(1, 13):
                    f.write(f"{i:02d}_plot_name.png\n")

            print(f"\n🎉 ANALYSIS COMPLETE!")
            print(f"📁 All {12} individual plots saved in: {results_dir}")
            print(f"📄 Summary report saved to: {summary_path}")

    except Exception as e:
        print(f"❌ Error during analysis: {str(e)}")
        print("Please check that all model training scripts have been run successfully.")

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import json
from google.colab import drive
from matplotlib.colors import LinearSegmentedColormap

# Mount Google Drive
drive.mount('/content/drive')

# Results directory
results_dir = '/content/drive/MyDrive/ablation_study_results'

def load_all_results():
    """Load results from all models"""
    models = ['baseline', 'batchnorm', 'attention', 'enhanced']
    model_names = ['Stage 1', 'Stage 2', 'Stage 3', 'Proposed Model']

    results = {}
    histories = {}

    for model_folder, model_name in zip(models, model_names):
        try:
            # Load results
            results_file = f'{results_dir}/{model_folder}/results.json'
            if os.path.exists(results_file):
                with open(results_file, 'r') as f:
                    results[model_name] = json.load(f)

            # Load history
            history_file = f'{results_dir}/{model_folder}/history.pkl'
            if os.path.exists(history_file):
                with open(history_file, 'rb') as f:
                    histories[model_name] = pickle.load(f)

            print(f"✓ Loaded results for {model_name}")
        except Exception as e:
            print(f"✗ Failed to load {model_name}: {str(e)}")

    return results, histories

def create_all_individual_plots(results, histories):
    """Create all individual plots with updated color scheme"""
    # NEW COLOR PALETTE - Each model gets a distinct, vibrant color
    colors = [
        '#E74C3C',  # Baseline - Red
        '#3498DB',  # BatchNorm - Blue
        '#2ECC71',  # Attention - Green
        '#9B59B6'   # Enhanced_Full - Purple
    ]

    print("🎨 Creating all individual plots with new color scheme...")
    print("="*60)

    # ==================== PLOT 1: Test Accuracy Comparison ====================
    print("📊 Creating Plot 1: Test Accuracy Comparison...")
    models = list(results.keys())
    test_accs = [results[model]['test_accuracy'] for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, test_accs, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('Test Accuracy Comparison', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Test Accuracy', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')

    for bar, acc in zip(bars, test_accs):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{acc:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=12)

    plt.ylim(min(test_accs) - 0.02, max(test_accs) + 0.03)
    plt.tight_layout()
    plt.savefig(f"{results_dir}/01_test_accuracy_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 2: ROC-AUC Comparison ====================
    print("📊 Creating Plot 2: ROC-AUC Comparison...")
    roc_aucs = [results[model]['roc_auc'] for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, roc_aucs, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('ROC-AUC Score Comparison', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('ROC-AUC Score', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')

    for bar, auc in zip(bars, roc_aucs):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{auc:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=12)

    plt.ylim(min(roc_aucs) - 0.02, max(roc_aucs) + 0.03)
    plt.tight_layout()
    plt.savefig(f"{results_dir}/02_roc_auc_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 3: Training History Accuracy ====================
    print("📊 Creating Plot 3: Training History - Accuracy...")
    plt.figure(figsize=(14, 8))

    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['accuracy']) + 1)
        plt.plot(epochs, history['accuracy'], label=f'{name} (Train)',
                color=colors[i], linestyle='-', alpha=0.9, linewidth=2.5, marker='o', markersize=4)
        plt.plot(epochs, history['val_accuracy'], label=f'{name} (Val)',
                color=colors[i], linestyle='--', alpha=0.9, linewidth=2.5, marker='s', markersize=4)

    plt.title('Training & Validation Accuracy History', fontsize=18, fontweight='bold', pad=20)
    plt.xlabel('Epochs', fontsize=14)
    plt.ylabel('Accuracy', fontsize=14)
    plt.legend(fontsize=12, loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/03_training_accuracy_history.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 4: Training History Loss ====================
    print("📊 Creating Plot 4: Training History - Loss...")
    plt.figure(figsize=(14, 8))

    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['loss']) + 1)
        plt.plot(epochs, history['loss'], label=f'{name} (Train)',
                color=colors[i], linestyle='-', alpha=0.9, linewidth=2.5, marker='o', markersize=4)
        plt.plot(epochs, history['val_loss'], label=f'{name} (Val)',
                color=colors[i], linestyle='--', alpha=0.9, linewidth=2.5, marker='s', markersize=4)

    plt.title('Training & Validation Loss History', fontsize=18, fontweight='bold', pad=20)
    plt.xlabel('Epochs', fontsize=14)
    plt.ylabel('Loss', fontsize=14)
    plt.legend(fontsize=12, loc='upper right')
    plt.grid(True, alpha=0.3)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/04_training_loss_history.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 5: Model Complexity ====================
    print("📊 Creating Plot 5: Model Complexity...")
    param_counts = [results[model]['total_params'] for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, param_counts, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('Model Complexity Comparison\n(Total Parameters)', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Number of Parameters', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')

    # Format y-axis
    ax = plt.gca()
    if max(param_counts) >= 1e6:
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))
    elif max(param_counts) >= 1e3:
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e3:.0f}K'))

    for bar, count in zip(bars, param_counts):
        if count >= 1e6:
            label = f'{count/1e6:.2f}M'
        elif count >= 1e3:
            label = f'{count/1e3:.0f}K'
        else:
            label = f'{count:,}'

        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(param_counts)*0.02,
                label, ha='center', va='bottom', fontweight='bold', fontsize=11)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/05_model_complexity_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 6: Precision, Recall, F1 ====================
    print("📊 Creating Plot 6: Precision, Recall & F1-Score...")
    precision_scores = [results[model]['precision'] for model in models]
    recall_scores = [results[model]['recall'] for model in models]
    f1_scores = [results[model]['f1_score'] for model in models]

    x = np.arange(len(models))
    width = 0.25

    plt.figure(figsize=(14, 8))

    # Updated colors for metrics bars
    bars1 = plt.bar(x - width, precision_scores, width, label='Precision', alpha=0.8,
                    color='#FF7F50', edgecolor='black', linewidth=1)  # Coral
    bars2 = plt.bar(x, recall_scores, width, label='Recall', alpha=0.8,
                    color='#20B2AA', edgecolor='black', linewidth=1)  # Light Sea Green
    bars3 = plt.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8,
                    color='#DDA0DD', edgecolor='black', linewidth=1)  # Plum

    for bars, scores in zip([bars1, bars2, bars3], [precision_scores, recall_scores, f1_scores]):
        for bar, score in zip(bars, scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)

    plt.xlabel('Model Architecture', fontsize=14)
    plt.ylabel('Score', fontsize=14)
    plt.title('Precision, Recall & F1-Score Comparison', fontsize=18, fontweight='bold', pad=20)
    plt.xticks(x, models, rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.legend(fontsize=12, loc='upper left')
    plt.grid(True, alpha=0.3, axis='y')

    all_scores = precision_scores + recall_scores + f1_scores
    plt.ylim(min(all_scores) - 0.05, max(all_scores) + 0.08)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/06_precision_recall_f1_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 7: Accuracy vs Complexity ====================
    print("📊 Creating Plot 7: Accuracy vs Complexity...")
    plt.figure(figsize=(12, 8))

    scatter = plt.scatter(param_counts, test_accs, c=colors[:len(models)], s=200,
                         alpha=0.8, edgecolors='black', linewidth=2)

    for i, model in enumerate(models):
        plt.annotate(model, (param_counts[i], test_accs[i]),
                    xytext=(10, 10), textcoords='offset points',
                    fontsize=12, fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor=colors[i], alpha=0.3))

    plt.title('Test Accuracy vs Model Complexity', fontsize=18, fontweight='bold', pad=20)
    plt.xlabel('Number of Parameters', fontsize=14)
    plt.ylabel('Test Accuracy', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)

    ax = plt.gca()
    if max(param_counts) >= 1e6:
        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))
        plt.xlabel('Number of Parameters (Millions)', fontsize=14)
    elif max(param_counts) >= 1e3:
        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e3:.0f}K'))
        plt.xlabel('Number of Parameters (Thousands)', fontsize=14)

    z = np.polyfit(param_counts, test_accs, 1)
    p = np.poly1d(z)
    plt.plot(param_counts, p(param_counts), "r--", alpha=0.6, linewidth=2, label='Trend Line')
    plt.legend(fontsize=12)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/07_accuracy_vs_complexity_scatter.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 8: Performance Improvement ====================
    print("📊 Creating Plot 8: Performance Improvement...")
    baseline_acc = results['Baseline']['test_accuracy']
    improvements = [(results[model]['test_accuracy'] - baseline_acc) * 100 for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, improvements, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('Test Accuracy Improvement over Baseline (%)', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Improvement (%)', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Baseline Performance')

    # Override colors based on improvement (positive/negative)
    for i, (bar, imp) in enumerate(zip(bars, improvements)):
        if imp > 0:
            bar.set_color('#27AE60')  # Green for positive improvement
        elif imp < 0:
            bar.set_color('#E74C3C')  # Red for negative
        else:
            bar.set_color(colors[i])   # Original color for baseline (0)

        plt.text(bar.get_x() + bar.get_width()/2,
                bar.get_height() + (0.2 if imp >= 0 else -0.3),
                f'{imp:+.2f}%', ha='center', va='bottom' if imp >= 0 else 'top',
                fontweight='bold', fontsize=12)

    plt.legend(fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{results_dir}/08_performance_improvement_analysis.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 9: Parameter Efficiency ====================
    print("📊 Creating Plot 9: Parameter Efficiency...")
    efficiency = [results[model]['test_accuracy'] / (results[model]['total_params'] / 1e6)
                  for model in models]

    plt.figure(figsize=(12, 8))
    bars = plt.bar(models, efficiency, color=colors[:len(models)], alpha=0.8, edgecolor='black', linewidth=1.5)

    plt.title('Parameter Efficiency: Accuracy per Million Parameters', fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Efficiency Score\n(Accuracy / Million Parameters)', fontsize=14)
    plt.xlabel('Model Architecture', fontsize=14)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')

    for bar, eff in zip(bars, efficiency):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{eff:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)

    max_efficiency_idx = efficiency.index(max(efficiency))
    bars[max_efficiency_idx].set_color('#FFD700')  # Gold for most efficient
    bars[max_efficiency_idx].set_edgecolor('#FF8C00')
    bars[max_efficiency_idx].set_linewidth(3)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/09_parameter_efficiency_analysis.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 10: Performance Heatmap ====================
    print("📊 Creating Plot 10: Performance Heatmap...")
    metrics_for_heatmap = ['test_accuracy', 'roc_auc', 'precision', 'recall', 'f1_score', 'best_val_acc']
    metric_labels = ['Test Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score', 'Best Val Acc']

    heatmap_data = []
    model_names = []

    for model_name in results.keys():
        model_names.append(model_name)
        row = [results[model_name][metric] for metric in metrics_for_heatmap]
        heatmap_data.append(row)

    heatmap_array = np.array(heatmap_data)
    normalized_data = np.zeros_like(heatmap_array)
    for i in range(heatmap_array.shape[1]):
        col = heatmap_array[:, i]
        normalized_data[:, i] = (col - col.min()) / (col.max() - col.min())

    plt.figure(figsize=(14, 10))

    # Updated heatmap colors - more vibrant gradient
    colors_heatmap = ['#E74C3C', '#F39C12', '#F1C40F', '#2ECC71', '#27AE60', '#1ABC9C']
    custom_cmap = LinearSegmentedColormap.from_list('custom', colors_heatmap, N=100)

    sns.heatmap(normalized_data,
                xticklabels=metric_labels,
                yticklabels=model_names,
                annot=heatmap_array,
                fmt='.4f',
                cmap=custom_cmap,
                center=0.5,
                square=True,
                linewidths=1,
                cbar_kws={"shrink": .8, "label": "Normalized Performance (0-1 scale)"},
                annot_kws={"fontsize": 11, "fontweight": "bold"})

    plt.title('Model Performance Heatmap\n(Normalized Colors with Actual Values)',
              fontsize=18, fontweight='bold', pad=20)
    plt.ylabel('Models', fontsize=14, fontweight='bold')
    plt.xlabel('Performance Metrics', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(rotation=0, fontsize=12)

    for i, metric in enumerate(metrics_for_heatmap):
        best_model_idx = np.argmax(heatmap_array[:, i])
        plt.text(i + 0.7, best_model_idx + 0.3, '★', fontsize=20, color='red', fontweight='bold')

    plt.tight_layout()
    plt.savefig(f"{results_dir}/10_comprehensive_performance_heatmap.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 11: Radar Chart ====================
    print("📊 Creating Plot 11: Overall Performance Radar Chart...")
    metrics = ['Test Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']
    N = len(metrics)
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))

    for i, model in enumerate(models):
        values = [
            results[model]['test_accuracy'],
            results[model]['roc_auc'],
            results[model]['precision'],
            results[model]['recall'],
            results[model]['f1_score']
        ]
        values += values[:1]

        ax.plot(angles, values, 'o-', linewidth=3, label=model, color=colors[i], alpha=0.8, markersize=8)
        ax.fill(angles, values, alpha=0.15, color=colors[i])

        for angle, value, metric in zip(angles[:-1], values[:-1], metrics):
            ax.text(angle, value + 0.02, f'{value:.3f}', ha='center', va='center',
                   fontsize=9, fontweight='bold', color=colors[i],
                   bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(metrics, fontsize=14, fontweight='bold')
    ax.set_ylim(0, 1)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=12)
    ax.grid(True, alpha=0.3)

    plt.title('Overall Performance Comparison\n(Radar Chart)', fontsize=18, fontweight='bold', pad=30)
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=12)

    plt.tight_layout()
    plt.savefig(f"{results_dir}/11_overall_performance_radar_chart.png", dpi=300, bbox_inches='tight')
    plt.show()

    # ==================== PLOT 12: Convergence Analysis ====================
    print("📊 Creating Plot 12: Training Convergence Analysis...")
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))

    # Learning Rate Analysis
    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['accuracy']) + 1)
        acc_improvements = np.diff([0] + history['accuracy'])
        ax1.plot(epochs, acc_improvements, label=name, color=colors[i],
                marker='o', markersize=4, linewidth=2, alpha=0.8)

    ax1.set_title('Training Accuracy Improvement per Epoch', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy Improvement')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)

    # Overfitting Analysis
    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['accuracy']) + 1)
        overfitting_gap = np.array(history['accuracy']) - np.array(history['val_accuracy'])
        ax2.plot(epochs, overfitting_gap, label=name, color=colors[i],
                marker='s', markersize=4, linewidth=2, alpha=0.8)

    ax2.set_title('Overfitting Analysis\n(Training - Validation Accuracy)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Overfitting Gap')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.axhline(y=0, color='green', linestyle='--', alpha=0.7, label='Perfect Fit')
    ax2.axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='Overfitting Threshold')

    # Loss Convergence Stability
    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['val_loss']) + 1)
        window_size = min(3, len(history['val_loss']))
        if len(history['val_loss']) >= window_size:
            moving_avg = np.convolve(history['val_loss'], np.ones(window_size)/window_size, mode='valid')
            moving_epochs = range(window_size, len(history['val_loss']) + 1)
            ax3.plot(moving_epochs, moving_avg, label=f'{name} (Smoothed)',
                    color=colors[i], linewidth=3, alpha=0.8)

        ax3.plot(epochs, history['val_loss'], color=colors[i],
                alpha=0.3, linewidth=1, linestyle='--')

    ax3.set_title('Validation Loss Convergence\n(Smoothed Trends)', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Validation Loss')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # Training Efficiency
    best_val_reached = {}
    for i, (name, history) in enumerate(histories.items()):
        epochs = range(1, len(history['val_accuracy']) + 1)
        cumulative_best = []
        current_best = 0

        for val_acc in history['val_accuracy']:
            current_best = max(current_best, val_acc)
            cumulative_best.append(current_best)

        ax4.plot(epochs, cumulative_best, label=name, color=colors[i],
                marker='d', markersize=4, linewidth=2.5, alpha=0.8)

        best_val_reached[name] = current_best

    ax4.set_title('Training Efficiency\n(Best Validation Accuracy Achieved)', fontsize=14, fontweight='bold')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Best Validation Accuracy')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    for i, (name, best_val) in enumerate(best_val_reached.items()):
        ax4.annotate(f'{best_val:.4f}',
                    xy=(len(histories[name]['val_accuracy']), best_val),
                    xytext=(10, 0), textcoords='offset points',
                    fontsize=9, fontweight='bold', color=colors[i])

    plt.tight_layout()
    plt.savefig(f"{results_dir}/12_training_convergence_analysis.png", dpi=300, bbox_inches='tight')
    plt.show()

    print("✅ All individual plots created successfully with new color scheme!")
    print("="*60)
    print(f"📁 All plots saved in: {results_dir}")
    print("\n🎨 New Color Scheme:")
    print("   • Baseline: Red (#E74C3C)")
    print("   • BatchNorm: Blue (#3498DB)")
    print("   • Attention: Green (#2ECC71)")
    print("   • Enhanced_Full: Purple (#9B59B6)")
    print("\n📋 Generated Plots:")
    print("   01_test_accuracy_comparison.png")
    print("   02_roc_auc_comparison.png")
    print("   03_training_accuracy_history.png")
    print("   04_training_loss_history.png")
    print("   05_model_complexity_comparison.png")
    print("   06_precision_recall_f1_comparison.png")
    print("   07_accuracy_vs_complexity_scatter.png")
    print("   08_performance_improvement_analysis.png")
    print("   09_parameter_efficiency_analysis.png")
    print("   10_comprehensive_performance_heatmap.png")
    print("   11_overall_performance_radar_chart.png")
    print("   12_training_convergence_analysis.png")

def generate_summary_report(results, histories):
    """Generate a comprehensive summary report"""
    print("\n" + "="*80)
    print("📊 COMPREHENSIVE ABLATION STUDY SUMMARY")
    print("="*80)

    # Create summary DataFrame
    df = pd.DataFrame(results).T
    df = df.round(4)
    df.index.name = 'Model'

    # Add efficiency metric
    df['efficiency'] = df['test_accuracy'] / (df['total_params'] / 1e6)
    df['efficiency'] = df['efficiency'].round(2)

    print("\n🏆 PERFORMANCE SUMMARY TABLE:")
    print("-" * 80)
    print(df.to_string())
    print("-" * 80)

    # Key findings
    best_accuracy = df['test_accuracy'].max()
    best_model = df['test_accuracy'].idxmax()
    most_efficient = df['efficiency'].idxmax()
    best_roc_auc = df['roc_auc'].idxmax()

    print(f"\n🎯 KEY FINDINGS:")
    print(f"   • Best Overall Model: {best_model} (Accuracy: {best_accuracy:.4f})")
    print(f"   • Most Efficient Model: {most_efficient} (Efficiency: {df.loc[most_efficient, 'efficiency']:.2f})")
    print(f"   • Best ROC-AUC: {best_roc_auc} (ROC-AUC: {df.loc[best_roc_auc, 'roc_auc']:.4f})")

    # Improvement analysis
    baseline_acc = df.loc['Baseline', 'test_accuracy']
    print(f"\n📈 IMPROVEMENTS OVER BASELINE:")
    for model in df.index:
        if model != 'Baseline':
            improvement = (df.loc[model, 'test_accuracy'] - baseline_acc) * 100
            print(f"   • {model}: {improvement:+.2f}% improvement")

    # Parameter efficiency ranking
    print(f"\n⚡ PARAMETER EFFICIENCY RANKING:")
    efficiency_ranking = df.sort_values('efficiency', ascending=False)
    for i, model in enumerate(efficiency_ranking.index, 1):
        eff = efficiency_ranking.loc[model, 'efficiency']
        params = efficiency_ranking.loc[model, 'total_params']
        print(f"   {i}. {model}: {eff:.2f} (Acc/M params) - {params:,} total parameters")

    # Training convergence analysis
    if histories:
        print(f"\n🔍 TRAINING CONVERGENCE ANALYSIS:")
        for model_name, history in histories.items():
            epochs_trained = len(history['accuracy'])
            final_train_acc = history['accuracy'][-1]
            final_val_acc = history['val_accuracy'][-1]
            best_val_acc = max(history['val_accuracy'])
            best_epoch = history['val_accuracy'].index(best_val_acc) + 1
            overfitting = final_train_acc - final_val_acc

            print(f"\n   {model_name}:")
            print(f"     • Epochs Trained: {epochs_trained}")
            print(f"     • Final Train Accuracy: {final_train_acc:.4f}")
            print(f"     • Final Validation Accuracy: {final_val_acc:.4f}")
            print(f"     • Best Validation Accuracy: {best_val_acc:.4f} (Epoch {best_epoch})")
            print(f"     • Overfitting Indicator: {overfitting:.4f}")

            if overfitting > 0.05:
                print(f"     ⚠️  Potential overfitting detected!")
            elif overfitting < -0.02:
                print(f"     📈 Model may benefit from more training")
            else:
                print(f"     ✅ Good training convergence")

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    try:
        print("🚀 ABLATION STUDY - INDIVIDUAL PLOTS GENERATOR (UPDATED COLORS)")
        print("="*80)
        print("Loading results from all models...")

        results, histories = load_all_results()

        if not results:
            print("❌ No results found! Make sure you've run all model training scripts first.")
            print("Expected directory structure:")
            print(f"  {results_dir}/")
            print("    ├── baseline/")
            print("    ├── batchnorm/")
            print("    ├── attention/")
            print("    └── enhanced_full/")
        else:
            print(f"✅ Successfully loaded results for {len(results)} models")

            # Create all individual plots with new colors
            create_all_individual_plots(results, histories)

            # Generate summary report
            generate_summary_report(results, histories)

            # Save summary report
            summary_path = f"{results_dir}/individual_plots_summary_updated.txt"
            with open(summary_path, 'w') as f:
                f.write("INDIVIDUAL PLOTS GENERATED (UPDATED COLORS)\n")
                f.write("="*50 + "\n\n")
                f.write("Generated 12 individual plots with new color scheme\n")
                f.write("Each plot saved as high-resolution PNG file\n\n")
                f.write("New Color Scheme:\n")
                f.write("• Baseline: Red (#E74C3C)\n")
                f.write("• BatchNorm: Blue (#3498DB)\n")
                f.write("• Attention: Green (#2ECC71)\n")
                f.write("• Enhanced_Full: Purple (#9B59B6)\n\n")
                f.write("Plot List:\n")
                plot_names = [
                    "01_test_accuracy_comparison.png",
                    "02_roc_auc_comparison.png",
                    "03_training_accuracy_history.png",
                    "04_training_loss_history.png",
                    "05_model_complexity_comparison.png",
                    "06_precision_recall_f1_comparison.png",
                    "07_accuracy_vs_complexity_scatter.png",
                    "08_performance_improvement_analysis.png",
                    "09_parameter_efficiency_analysis.png",
                    "10_comprehensive_performance_heatmap.png",
                    "11_overall_performance_radar_chart.png",
                    "12_training_convergence_analysis.png"
                ]
                for plot_name in plot_names:
                    f.write(f"{plot_name}\n")

            print(f"\n🎉 ANALYSIS COMPLETE WITH NEW COLORS!")
            print(f"📁 All {12} individual plots saved in: {results_dir}")
            print(f"📄 Summary report saved to: {summary_path}")
            print("\n🎨 Color Changes Applied:")
            print("   • Each model now has a distinct, vibrant color")
            print("   • Better visual separation between models")
            print("   • Consistent color scheme across all plots")
            print("   • Enhanced readability and professional appearance")

    except Exception as e:
        print(f"❌ Error during analysis: {str(e)}")
        print("Please check that all model training scripts have been run successfully.")

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, classification_report
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Paths
results_dir = '/content/drive/MyDrive/ablation_study_results'
model_path = f'{results_dir}/enhanced/best_model.keras'

# ===== CONFIGURE YOUR DATASET PATH HERE =====
# For testing on original test set:
TEST_DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'

# For testing on new dataset (uncomment and change path):
# TEST_DATA_PATH = '/content/new_dataset'  # Must have fake/ and real/ folders

# Load the trained model
print("Loading trained model...")
model = load_model(model_path)
print("✅ Model loaded successfully!\n")

# Setup data generator
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    TEST_DATA_PATH,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

print(f"Found {test_generator.samples} images")
print(f"Classes: {test_generator.class_indices}\n")

# Get predictions
print("Generating predictions...")
predictions = model.predict(test_generator, verbose=1)
y_pred = np.argmax(predictions, axis=1)
y_true = test_generator.classes

print("\n" + "="*80)
print("CONFUSION MATRIX")
print("="*80 + "\n")

# Calculate confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Print confusion matrix in text format
print("Confusion Matrix (Raw Numbers):")
print("-" * 40)
print(f"                Predicted")
print(f"              Fake    Real")
print(f"Actual Fake   {cm[0][0]:6d}  {cm[0][1]:6d}")
print(f"       Real   {cm[1][0]:6d}  {cm[1][1]:6d}")
print("-" * 40)

# Calculate metrics from confusion matrix
tn, fp, fn, tp = cm.ravel()
total = tn + fp + fn + tp

accuracy = (tp + tn) / total
precision_fake = tn / (tn + fn) if (tn + fn) > 0 else 0
precision_real = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_fake = tn / (tn + fp) if (tn + fp) > 0 else 0
recall_real = tp / (tp + fn) if (tp + fn) > 0 else 0

print(f"\nInterpretation:")
print(f"  • True Negatives (Fake correctly predicted):  {tn:6d} ({tn/total*100:.2f}%)")
print(f"  • False Positives (Fake predicted as Real):   {fp:6d} ({fp/total*100:.2f}%)")
print(f"  • False Negatives (Real predicted as Fake):   {fn:6d} ({fn/total*100:.2f}%)")
print(f"  • True Positives (Real correctly predicted):  {tp:6d} ({tp/total*100:.2f}%)")
print(f"\n  Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

print("\n" + "="*80)
print("DETAILED CLASSIFICATION REPORT")
print("="*80 + "\n")
print(classification_report(y_true, y_pred, target_names=['Fake', 'Real'], digits=4))

# Visualize confusion matrix
print("\nGenerating confusion matrix visualizations...\n")
print("="*80)
print("DISPLAYING CONFUSION MATRIX FIGURES")
print("="*80 + "\n")

# Figure 1: Detailed comparison (counts and normalized)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Plot 1: Raw numbers
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Fake', 'Real'],
            yticklabels=['Fake', 'Real'],
            cbar_kws={'label': 'Count'},
            ax=ax1,
            annot_kws={"fontsize": 16, "fontweight": "bold"})
ax1.set_title('Confusion Matrix (Counts)', fontsize=16, fontweight='bold', pad=15)
ax1.set_ylabel('Actual Label', fontsize=14, fontweight='bold')
ax1.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')

# Add totals
for i in range(2):
    for j in range(2):
        percentage = cm[i, j] / cm[i].sum() * 100
        ax1.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',
                ha='center', va='center', fontsize=11, color='darkred')

# Plot 2: Normalized (percentages)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens',
            xticklabels=['Fake', 'Real'],
            yticklabels=['Fake', 'Real'],
            cbar_kws={'label': 'Percentage'},
            ax=ax2,
            annot_kws={"fontsize": 16, "fontweight": "bold"})
ax2.set_title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold', pad=15)
ax2.set_ylabel('Actual Label', fontsize=14, fontweight='bold')
ax2.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced_full/confusion_matrix_detailed.png',
            dpi=300, bbox_inches='tight')
plt.show()

# Create a single larger confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn',
            xticklabels=['Fake (0)', 'Real (1)'],
            yticklabels=['Fake (0)', 'Real (1)'],
            cbar_kws={'label': 'Number of Images'},
            square=True,
            linewidths=2,
            linecolor='black',
            annot_kws={"fontsize": 20, "fontweight": "bold"})

plt.title('Confusion Matrix ',
          fontsize=18, fontweight='bold', pad=20)
plt.ylabel('Actual Label', fontsize=16, fontweight='bold')
plt.xlabel('Predicted Label', fontsize=16, fontweight='bold')

# Add percentage annotations
for i in range(2):
    for j in range(2):
        percentage = cm[i, j] / cm[i].sum() * 100
        plt.text(j + 0.5, i + 0.75, f'({percentage:.1f}%)',
                ha='center', va='center', fontsize=14,
                color='darkblue', fontweight='bold')

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced/confusion_matrix_single.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("="*80)
print("✅ CONFUSION MATRIX ANALYSIS COMPLETE!")
print("="*80)
print(f"\n📊 Visualizations saved:")
print(f"   • {results_dir}/enhanced/confusion_matrix_detailed.png")
print(f"   • {results_dir}/enhanced/confusion_matrix_single.png")
print("\n📈 Summary:")
print(f"   Total Images Tested: {total:,}")
print(f"   Correctly Classified: {tp + tn:,} ({(tp+tn)/total*100:.2f}%)")
print(f"   Misclassified: {fp + fn:,} ({(fp+fn)/total*100:.2f}%)")
print("="*80)

import os
import numpy as np
import matplotlib.pyplot as plt
import pickle
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Results directory
results_dir = '/content/drive/MyDrive/ablation_study_results'

# Load history for Proposed Model (Enhanced Full)
history_file = f'{results_dir}/enhanced/history.pkl'

print("="*80)
print("LOADING PROPOSED MODEL TRAINING HISTORY")
print("="*80)

try:
    with open(history_file, 'rb') as f:
        history = pickle.load(f)
    print("✅ Successfully loaded training history for Proposed Model\n")
except Exception as e:
    print(f"❌ Error loading history: {str(e)}")
    print("Make sure the enhanced_full model has been trained!")
    exit()

# Extract data
epochs = range(1, len(history['accuracy']) + 1)
train_accuracy = history['accuracy']
val_accuracy = history['val_accuracy']
train_loss = history['loss']
val_loss = history['val_loss']

# Print summary statistics
print("="*80)
print("TRAINING SUMMARY STATISTICS")
print("="*80)
print(f"Total Epochs Trained: {len(epochs)}")
print(f"\nFinal Training Accuracy:   {train_accuracy[-1]:.4f}")
print(f"Final Validation Accuracy: {val_accuracy[-1]:.4f}")
print(f"Best Validation Accuracy:  {max(val_accuracy):.4f} (Epoch {val_accuracy.index(max(val_accuracy)) + 1})")
print(f"\nFinal Training Loss:       {train_loss[-1]:.4f}")
print(f"Final Validation Loss:     {val_loss[-1]:.4f}")
print(f"Best Validation Loss:      {min(val_loss):.4f} (Epoch {val_loss.index(min(val_loss)) + 1})")
print(f"\nOverfitting Gap (Train - Val Accuracy): {train_accuracy[-1] - val_accuracy[-1]:.4f}")
print("="*80 + "\n")

# Create figure with both plots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))

# ==================== PLOT 1: Training & Validation Accuracy ====================
ax1.plot(epochs, train_accuracy, label='Training Accuracy',
         color='#2ECC71', linestyle='-', linewidth=3, marker='o', markersize=6, alpha=0.9)
ax1.plot(epochs, val_accuracy, label='Validation Accuracy',
         color='#E74C3C', linestyle='--', linewidth=3, marker='s', markersize=6, alpha=0.9)

ax1.set_title('Training & Validation Accuracy\nProposed Model',
              fontsize=16, fontweight='bold', pad=15)
ax1.set_xlabel('Epochs', fontsize=14, fontweight='bold')
ax1.set_ylabel('Accuracy', fontsize=14, fontweight='bold')
ax1.legend(fontsize=12, loc='lower right')
ax1.grid(True, alpha=0.3, linestyle='--')
ax1.tick_params(axis='both', labelsize=12)

# Add annotations for best values
best_val_acc_idx = val_accuracy.index(max(val_accuracy))
ax1.annotate(f'Best Val Acc: {max(val_accuracy):.4f}',
             xy=(best_val_acc_idx + 1, max(val_accuracy)),
             xytext=(10, -15), textcoords='offset points',
             fontsize=11, fontweight='bold', color='#E74C3C',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),
             arrowprops=dict(arrowstyle='->', color='#E74C3C', lw=2))

# ==================== PLOT 2: Training & Validation Loss ====================
ax2.plot(epochs, train_loss, label='Training Loss',
         color='#3498DB', linestyle='-', linewidth=3, marker='o', markersize=6, alpha=0.9)
ax2.plot(epochs, val_loss, label='Validation Loss',
         color='#E67E22', linestyle='--', linewidth=3, marker='s', markersize=6, alpha=0.9)

ax2.set_title('Training & Validation Loss\nProposed Model',
              fontsize=16, fontweight='bold', pad=15)
ax2.set_xlabel('Epochs', fontsize=14, fontweight='bold')
ax2.set_ylabel('Loss', fontsize=14, fontweight='bold')
ax2.legend(fontsize=12, loc='upper right')
ax2.grid(True, alpha=0.3, linestyle='--')
ax2.tick_params(axis='both', labelsize=12)

# Add annotations for best values
best_val_loss_idx = val_loss.index(min(val_loss))
ax2.annotate(f'Best Val Loss: {min(val_loss):.4f}',
             xy=(best_val_loss_idx + 1, min(val_loss)),
             xytext=(10, 15), textcoords='offset points',
             fontsize=11, fontweight='bold', color='#E67E22',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),
             arrowprops=dict(arrowstyle='->', color='#E67E22', lw=2))

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced_full/training_validation_plots.png',
            dpi=300, bbox_inches='tight')
print("Figure 1: Training & Validation Accuracy and Loss (Side-by-Side)")
plt.show()

print("\n" + "-"*80 + "\n")

# ==================== SEPARATE PLOTS ====================

# PLOT: Training & Validation Accuracy (Separate)
print("Figure 2: Training & Validation Accuracy (Standalone)")
plt.figure(figsize=(12, 7))

plt.plot(epochs, train_accuracy, label='Training Accuracy',
         color='#2ECC71', linestyle='-', linewidth=3.5, marker='o', markersize=7, alpha=0.9)
plt.plot(epochs, val_accuracy, label='Validation Accuracy',
         color='#E74C3C', linestyle='--', linewidth=3.5, marker='s', markersize=7, alpha=0.9)

plt.title('Training & Validation Accuracy - Proposed Model',
          fontsize=18, fontweight='bold', pad=20)
plt.xlabel('Epochs', fontsize=15, fontweight='bold')
plt.ylabel('Accuracy', fontsize=15, fontweight='bold')
plt.legend(fontsize=13, loc='lower right', framealpha=0.9)
plt.grid(True, alpha=0.3, linestyle='--', linewidth=1)
plt.tick_params(axis='both', labelsize=13)

# Add best validation accuracy annotation
best_val_acc_idx = val_accuracy.index(max(val_accuracy))
plt.annotate(f'Best Validation Accuracy\n{max(val_accuracy):.4f} (Epoch {best_val_acc_idx + 1})',
             xy=(best_val_acc_idx + 1, max(val_accuracy)),
             xytext=(15, -25), textcoords='offset points',
             fontsize=12, fontweight='bold', color='#E74C3C',
             bbox=dict(boxstyle='round,pad=0.6', facecolor='yellow', alpha=0.8, edgecolor='black', linewidth=2),
             arrowprops=dict(arrowstyle='->', color='#E74C3C', lw=2.5))

# Add final accuracy values
plt.text(0.02, 0.98, f'Final Training Acc: {train_accuracy[-1]:.4f}\nFinal Validation Acc: {val_accuracy[-1]:.4f}',
         transform=plt.gca().transAxes, fontsize=11, fontweight='bold',
         verticalalignment='top',
         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8, edgecolor='black'))

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced_full/training_validation_accuracy.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "-"*80 + "\n")

# PLOT: Training & Validation Loss (Separate)
print("Figure 3: Training & Validation Loss (Standalone)")
plt.figure(figsize=(12, 7))

plt.plot(epochs, train_loss, label='Training Loss',
         color='#3498DB', linestyle='-', linewidth=3.5, marker='o', markersize=7, alpha=0.9)
plt.plot(epochs, val_loss, label='Validation Loss',
         color='#E67E22', linestyle='--', linewidth=3.5, marker='s', markersize=7, alpha=0.9)

plt.title('Training & Validation Loss - Proposed Model',
          fontsize=18, fontweight='bold', pad=20)
plt.xlabel('Epochs', fontsize=15, fontweight='bold')
plt.ylabel('Loss', fontsize=15, fontweight='bold')
plt.legend(fontsize=13, loc='upper right', framealpha=0.9)
plt.grid(True, alpha=0.3, linestyle='--', linewidth=1)
plt.tick_params(axis='both', labelsize=13)

# Add best validation loss annotation
best_val_loss_idx = val_loss.index(min(val_loss))
plt.annotate(f'Best Validation Loss\n{min(val_loss):.4f} (Epoch {best_val_loss_idx + 1})',
             xy=(best_val_loss_idx + 1, min(val_loss)),
             xytext=(15, 25), textcoords='offset points',
             fontsize=12, fontweight='bold', color='#E67E22',
             bbox=dict(boxstyle='round,pad=0.6', facecolor='yellow', alpha=0.8, edgecolor='black', linewidth=2),
             arrowprops=dict(arrowstyle='->', color='#E67E22', lw=2.5))

# Add final loss values
plt.text(0.02, 0.98, f'Final Training Loss: {train_loss[-1]:.4f}\nFinal Validation Loss: {val_loss[-1]:.4f}',
         transform=plt.gca().transAxes, fontsize=11, fontweight='bold',
         verticalalignment='top',
         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8, edgecolor='black'))

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced_full/training_validation_loss.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "="*80)
print("✅ ALL TRAINING PLOTS GENERATED SUCCESSFULLY!")
print("="*80)
print(f"\n📊 3 Visualizations Created and Saved:")
print(f"   1. {results_dir}/enhanced_full/training_validation_plots.png")
print(f"      (Combined: Accuracy + Loss side-by-side)")
print(f"   2. {results_dir}/enhanced_full/training_validation_accuracy.png")
print(f"      (Standalone: Training & Validation Accuracy)")
print(f"   3. {results_dir}/enhanced_full/training_validation_loss.png")
print(f"      (Standalone: Training & Validation Loss)")
print("\n📈 Key Metrics:")
print(f"   • Total Epochs: {len(epochs)}")
print(f"   • Best Validation Accuracy: {max(val_accuracy):.4f} (Epoch {best_val_acc_idx + 1})")
print(f"   • Best Validation Loss: {min(val_loss):.4f} (Epoch {best_val_loss_idx + 1})")
print(f"   • Final Training Accuracy: {train_accuracy[-1]:.4f}")
print(f"   • Final Validation Accuracy: {val_accuracy[-1]:.4f}")
print(f"   • Overfitting Gap: {train_accuracy[-1] - val_accuracy[-1]:.4f}")
print("="*80)

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Paths
results_dir = '/content/drive/MyDrive/ablation_study_results'
model_path = f'{results_dir}/enhanced/best_model.keras'

# ===== CONFIGURE YOUR DATASET PATH HERE =====
# For testing on original test set:
TEST_DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'

# For testing on new dataset (uncomment and change path):
# TEST_DATA_PATH = '/content/new_dataset'  # Must have fake/ and real/ folders

# Load the trained model
print("Loading trained model...")
model = load_model(model_path)
print("✅ Model loaded successfully!\n")

# Setup data generator
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    TEST_DATA_PATH,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

print(f"Found {test_generator.samples} images")
print(f"Classes: {test_generator.class_indices}\n")

# Get predictions
print("Generating predictions...")
predictions = model.predict(test_generator, verbose=1)
y_pred = np.argmax(predictions, axis=1)
y_true = test_generator.classes

print("\n" + "="*80)
print("CONFUSION MATRIX")
print("="*80 + "\n")

# Calculate confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Print confusion matrix in text format
print("Confusion Matrix (Raw Numbers):")
print("-" * 40)
print(f"                Predicted")
print(f"              Fake    Real")
print(f"Actual Fake   {cm[0][0]:6d}  {cm[0][1]:6d}")
print(f"       Real   {cm[1][0]:6d}  {cm[1][1]:6d}")
print("-" * 40)

# Calculate metrics from confusion matrix
tn, fp, fn, tp = cm.ravel()
total = tn + fp + fn + tp

accuracy = (tp + tn) / total
precision_fake = tn / (tn + fn) if (tn + fn) > 0 else 0
precision_real = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_fake = tn / (tn + fp) if (tn + fp) > 0 else 0
recall_real = tp / (tp + fn) if (tp + fn) > 0 else 0

print(f"\nInterpretation:")
print(f"  • True Negatives (Fake correctly predicted):  {tn:6d} ({tn/total*100:.2f}%)")
print(f"  • False Positives (Fake predicted as Real):   {fp:6d} ({fp/total*100:.2f}%)")
print(f"  • False Negatives (Real predicted as Fake):   {fn:6d} ({fn/total*100:.2f}%)")
print(f"  • True Positives (Real correctly predicted):  {tp:6d} ({tp/total*100:.2f}%)")
print(f"\n  Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

print("\n" + "="*80)
print("DETAILED CLASSIFICATION REPORT")
print("="*80 + "\n")
print(classification_report(y_true, y_pred, target_names=['Fake', 'Real'], digits=4))

# Visualize confusion matrix
print("\nGenerating confusion matrix visualizations...\n")
print("="*80)
print("DISPLAYING CONFUSION MATRIX FIGURES")
print("="*80 + "\n")

# Figure 1: Detailed comparison (counts and normalized)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Plot 1: Raw numbers
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Fake', 'Real'],
            yticklabels=['Fake', 'Real'],
            cbar_kws={'label': 'Count'},
            ax=ax1,
            annot_kws={"fontsize": 16, "fontweight": "bold"})
ax1.set_title('Confusion Matrix (Counts)', fontsize=16, fontweight='bold', pad=15)
ax1.set_ylabel('Actual Label', fontsize=14, fontweight='bold')
ax1.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')

# Add totals
for i in range(2):
    for j in range(2):
        percentage = cm[i, j] / cm[i].sum() * 100
        ax1.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',
                ha='center', va='center', fontsize=11, color='darkred')

# Plot 2: Normalized (percentages)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens',
            xticklabels=['Fake', 'Real'],
            yticklabels=['Fake', 'Real'],
            cbar_kws={'label': 'Percentage'},
            ax=ax2,
            annot_kws={"fontsize": 16, "fontweight": "bold"})
ax2.set_title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold', pad=15)
ax2.set_ylabel('Actual Label', fontsize=14, fontweight='bold')
ax2.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced_full/confusion_matrix_detailed.png',
            dpi=300, bbox_inches='tight')
print("Figure 1: Detailed Confusion Matrix (Counts + Normalized)")
plt.show()
print("\n" + "-"*80 + "\n")

# Create a single larger confusion matrix
print("Figure 2: Single Large Confusion Matrix")
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn',
            xticklabels=['Fake (0)', 'Real (1)'],
            yticklabels=['Fake (0)', 'Real (1)'],
            cbar_kws={'label': 'Number of Images'},
            square=True,
            linewidths=2,
            linecolor='black',
            annot_kws={"fontsize": 20, "fontweight": "bold"})

plt.title('Confusion Matrix - Enhanced Full Model\n',
          fontsize=18, fontweight='bold', pad=20)
plt.ylabel('Actual Label', fontsize=16, fontweight='bold')
plt.xlabel('Predicted Label', fontsize=16, fontweight='bold')

# Add percentage annotations
for i in range(2):
    for j in range(2):
        percentage = cm[i, j] / cm[i].sum() * 100
        plt.text(j + 0.5, i + 0.75, f'({percentage:.1f}%)',
                ha='center', va='center', fontsize=14,
                color='darkblue', fontweight='bold')

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced/confusion_matrix_single.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "="*80)

# Figure 3: Beautiful publication-ready confusion matrix
print("\nFigure 3: Publication-Ready Confusion Matrix (Enhanced)")
plt.figure(figsize=(12, 9))

# Create custom colormap
from matplotlib.colors import LinearSegmentedColormap
colors_cm = ['#FFFFFF', '#E8F5E9', '#A5D6A7', '#66BB6A', '#43A047', '#2E7D32']
custom_cmap = LinearSegmentedColormap.from_list('custom_green', colors_cm, N=256)

# Plot heatmap
ax = sns.heatmap(cm, annot=False, cmap=custom_cmap,
                xticklabels=['Fake (Class 0)', 'Real (Class 1)'],
                yticklabels=['Fake (Class 0)', 'Real (Class 1)'],
                cbar_kws={'label': 'Number of Images', 'shrink': 0.8},
                square=True,
                linewidths=3,
                linecolor='black',
                cbar=True)

# Add large count annotations
for i in range(2):
    for j in range(2):
        # Main count
        plt.text(j + 0.5, i + 0.4, f'{cm[i, j]:,}',
                ha='center', va='center',
                fontsize=32, fontweight='bold', color='black')

        # Percentage
        percentage = cm[i, j] / cm[i].sum() * 100
        plt.text(j + 0.5, i + 0.65, f'({percentage:.1f}%)',
                ha='center', va='center',
                fontsize=16, fontweight='bold', color='darkblue')

        # Row percentage indicator
        row_pct = cm[i, j] / cm.sum() * 100
        plt.text(j + 0.5, i + 0.85, f'{row_pct:.1f}% of total',
                ha='center', va='center',
                fontsize=11, style='italic', color='darkgreen')

plt.title('Confusion Matrix - Enhanced Full Model\nDeepfake Detection Performance',
          fontsize=20, fontweight='bold', pad=25)
plt.ylabel('Actual Label (Ground Truth)', fontsize=16, fontweight='bold', labelpad=15)
plt.xlabel('Predicted Label (Model Output)', fontsize=16, fontweight='bold', labelpad=15)

# Add accuracy box
accuracy_text = f'Overall Accuracy: {accuracy:.2%}\n'
accuracy_text += f'Total Images: {total:,}\n'
accuracy_text += f'Correct: {tp+tn:,} | Wrong: {fp+fn:,}'
plt.text(1.5, -0.3, accuracy_text,
         ha='center', va='top',
         fontsize=14, fontweight='bold',
         bbox=dict(boxstyle='round,pad=0.8', facecolor='lightblue',
                  edgecolor='black', linewidth=2))

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced_full/confusion_matrix_publication.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "="*80)
print("✅ CONFUSION MATRIX ANALYSIS COMPLETE!")
print("="*80)
print(f"\n📊 Visualizations saved:")
print(f"   • {results_dir}/enhanced_full/confusion_matrix_detailed.png")
print(f"   • {results_dir}/enhanced_full/confusion_matrix_single.png")
print("\n📈 Summary:")
print(f"   Total Images Tested: {total:,}")
print(f"   Correctly Classified: {tp + tn:,} ({(tp+tn)/total*100:.2f}%)")
print(f"   Misclassified: {fp + fn:,} ({(fp+fn)/total*100:.2f}%)")
print("="*80)

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Paths
results_dir = '/content/drive/MyDrive/ablation_study_results'
model_path = f'{results_dir}/enhanced/best_model.keras'

# ===== CONFIGURE YOUR DATASET PATH HERE =====
# For testing on original test set:
TEST_DATA_PATH = '/content/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test'

# For testing on new dataset (uncomment and change path):
# TEST_DATA_PATH = '/content/new_dataset'  # Must have fake/ and real/ folders

# Load the trained model
print("Loading trained model...")
model = load_model(model_path)
print("✅ Model loaded successfully!\n")

# Setup data generator
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    TEST_DATA_PATH,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

print(f"Found {test_generator.samples} images")
print(f"Classes: {test_generator.class_indices}\n")

# Get predictions
print("Generating predictions...")
predictions = model.predict(test_generator, verbose=1)
y_pred = np.argmax(predictions, axis=1)
y_true = test_generator.classes

print("\n" + "="*80)
print("CONFUSION MATRIX")
print("="*80 + "\n")

# Calculate confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Print confusion matrix in text format
print("Confusion Matrix (Raw Numbers):")
print("-" * 40)
print(f"                Predicted")
print(f"              Fake    Real")
print(f"Actual Fake   {cm[0][0]:6d}  {cm[0][1]:6d}")
print(f"       Real   {cm[1][0]:6d}  {cm[1][1]:6d}")
print("-" * 40)

# Calculate metrics from confusion matrix
tn, fp, fn, tp = cm.ravel()
total = tn + fp + fn + tp

accuracy = (tp + tn) / total
precision_fake = tn / (tn + fn) if (tn + fn) > 0 else 0
precision_real = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_fake = tn / (tn + fp) if (tn + fp) > 0 else 0
recall_real = tp / (tp + fn) if (tp + fn) > 0 else 0

print(f"\nInterpretation:")
print(f"  • True Negatives (Fake correctly predicted):  {tn:6d} ({tn/total*100:.2f}%)")
print(f"  • False Positives (Fake predicted as Real):   {fp:6d} ({fp/total*100:.2f}%)")
print(f"  • False Negatives (Real predicted as Fake):   {fn:6d} ({fn/total*100:.2f}%)")
print(f"  • True Positives (Real correctly predicted):  {tp:6d} ({tp/total*100:.2f}%)")
print(f"\n  Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

print("\n" + "="*80)
print("DETAILED CLASSIFICATION REPORT")
print("="*80 + "\n")
print(classification_report(y_true, y_pred, target_names=['Fake', 'Real'], digits=4))

# Calculate ROC-AUC
print("\n" + "="*80)
print("ROC-AUC ANALYSIS")
print("="*80 + "\n")

# Get probability predictions for ROC curve
y_pred_proba = predictions[:, 1]  # Probability of positive class (Real)
roc_auc = roc_auc_score(y_true, y_pred_proba)
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)

print(f"ROC-AUC Score: {roc_auc:.4f}")
print(f"This means the model has a {roc_auc*100:.2f}% chance of distinguishing between Real and Fake images.")
print("="*80 + "\n")

# Visualize confusion matrix
print("\nGenerating visualizations...\n")
print("="*80)
print("DISPLAYING ALL FIGURES")
print("="*80 + "\n")

# Figure 1: Detailed comparison (counts and normalized)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Plot 1: Raw numbers
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Fake', 'Real'],
            yticklabels=['Fake', 'Real'],
            cbar_kws={'label': 'Count'},
            ax=ax1,
            annot_kws={"fontsize": 16, "fontweight": "bold"})
ax1.set_title('Confusion Matrix (Counts)', fontsize=16, fontweight='bold', pad=15)
ax1.set_ylabel('Actual Label', fontsize=14, fontweight='bold')
ax1.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')

# Add totals
for i in range(2):
    for j in range(2):
        percentage = cm[i, j] / cm[i].sum() * 100
        ax1.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',
                ha='center', va='center', fontsize=11, color='darkred')

# Plot 2: Normalized (percentages)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens',
            xticklabels=['Fake', 'Real'],
            yticklabels=['Fake', 'Real'],
            cbar_kws={'label': 'Percentage'},
            ax=ax2,
            annot_kws={"fontsize": 16, "fontweight": "bold"})
ax2.set_title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold', pad=15)
ax2.set_ylabel('Actual Label', fontsize=14, fontweight='bold')
ax2.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced/confusion_matrix_detailed.png',
            dpi=300, bbox_inches='tight')
print("Figure 1: Detailed Confusion Matrix (Counts + Normalized)")
plt.show()
print("\n" + "-"*80 + "\n")

# Figure 2: Single larger confusion matrix
print("Figure 2: Single Large Confusion Matrix")
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn',
            xticklabels=['Fake (0)', 'Real (1)'],
            yticklabels=['Fake (0)', 'Real (1)'],
            cbar_kws={'label': 'Number of Images'},
            square=True,
            linewidths=2,
            linecolor='black',
            annot_kws={"fontsize": 20, "fontweight": "bold"})

plt.title('Confusion Matrix - Enhanced Full Model',
          fontsize=18, fontweight='bold', pad=20)
plt.ylabel('Actual Label', fontsize=16, fontweight='bold')
plt.xlabel('Predicted Label', fontsize=16, fontweight='bold')

# Add percentage annotations
for i in range(2):
    for j in range(2):
        percentage = cm[i, j] / cm[i].sum() * 100
        plt.text(j + 0.5, i + 0.75, f'({percentage:.1f}%)',
                ha='center', va='center', fontsize=14,
                color='darkblue', fontweight='bold')

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced/confusion_matrix_single.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "-"*80 + "\n")

# ==================== FIGURE 3: ROC-AUC CURVE ====================
print("Figure 3: ROC-AUC Curve")
plt.figure(figsize=(10, 8))

# Plot ROC curve
plt.plot(fpr, tpr, color='#2ECC71', linewidth=3.5, label=f'ROC Curve (AUC = {roc_auc:.4f})')

# Plot diagonal reference line (random classifier)
plt.plot([0, 1], [0, 1], color='#E74C3C', linewidth=2.5, linestyle='--', label='Random Classifier (AUC = 0.5)')

# Fill area under curve
plt.fill_between(fpr, tpr, alpha=0.3, color='#2ECC71', label='Area Under Curve')

# Add optimal threshold point (closest to top-left corner)
# Optimal point is where TPR - FPR is maximum (Youden's Index)
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]
optimal_fpr = fpr[optimal_idx]
optimal_tpr = tpr[optimal_idx]

plt.scatter(optimal_fpr, optimal_tpr, color='red', s=250, zorder=5,
           edgecolors='black', linewidth=2.5, marker='*')
plt.annotate(f'Optimal Point\nThreshold: {optimal_threshold:.3f}\nTPR: {optimal_tpr:.3f}\nFPR: {optimal_fpr:.3f}',
            xy=(optimal_fpr, optimal_tpr), xytext=(optimal_fpr + 0.2, optimal_tpr - 0.2),
            fontsize=12, fontweight='bold',
            bbox=dict(boxstyle='round,pad=0.7', facecolor='yellow', alpha=0.9, edgecolor='black', linewidth=2),
            arrowprops=dict(arrowstyle='->', color='red', lw=2.5))

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=15, fontweight='bold')
plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=15, fontweight='bold')
plt.title('ROC Curve - Enhanced Full Model\nReceiver Operating Characteristic',
         fontsize=18, fontweight='bold', pad=20)
plt.legend(loc='lower right', fontsize=13, framealpha=0.95, edgecolor='black', fancybox=True)
plt.grid(True, alpha=0.3, linestyle='--', linewidth=1)

# Add performance interpretation box
if roc_auc >= 0.9:
    performance = "Excellent"
    color_box = '#2ECC71'
elif roc_auc >= 0.8:
    performance = "Good"
    color_box = '#3498DB'
elif roc_auc >= 0.7:
    performance = "Fair"
    color_box = '#F39C12'
else:
    performance = "Poor"
    color_box = '#E74C3C'

plt.text(0.98, 0.02, f'Model Performance: {performance}\nAUC Score: {roc_auc:.4f}',
        transform=plt.gca().transAxes, fontsize=13, fontweight='bold',
        verticalalignment='bottom', horizontalalignment='right',
        bbox=dict(boxstyle='round,pad=0.7', facecolor=color_box, alpha=0.8, edgecolor='black', linewidth=2))

plt.tight_layout()
plt.savefig(f'{results_dir}/enhanced/roc_auc_curve.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "-"*80 + "\n")

# ==================== FIGURE 4: COMBINED METRICS VISUALIZATION ====================
print("Figure 4: Combined Performance Metrics Dashboard")
from sklearn.metrics import precision_score, recall_score, f1_score

fig = plt.figure(figsize=(16, 10))
gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)

# Subplot 1: Confusion Matrix
ax1 = fig.add_subplot(gs[0, 0])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Fake', 'Real'],
            yticklabels=['Fake', 'Real'],
            cbar_kws={'label': 'Count'},
            ax=ax1,
            annot_kws={"fontsize": 14, "fontweight": "bold"})
ax1.set_title('Confusion Matrix', fontsize=14, fontweight='bold', pad=10)
ax1.set_ylabel('Actual', fontsize=12, fontweight='bold')
ax1.set_xlabel('Predicted', fontsize=12, fontweight='bold')

# Subplot 2: ROC Curve
ax2 = fig.add_subplot(gs[0, 1])
ax2.plot(fpr, tpr, color='#2ECC71', linewidth=3, label=f'AUC = {roc_auc:.4f}')
ax2.plot([0, 1], [0, 1], color='#E74C3C', linewidth=2, linestyle='--')
ax2.fill_between(fpr, tpr, alpha=0.3, color='#2ECC71')
ax2.scatter(optimal_fpr, optimal_tpr, color='red', s=150, zorder=5, edgecolors='black', linewidth=2)
ax2.set_xlim([0.0, 1.0])
ax2.set_ylim([0.0, 1.05])
ax2.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')
ax2.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')
ax2.set_title('ROC Curve', fontsize=14, fontweight='bold', pad=10)
ax2.legend(loc='lower right', fontsize=11)
ax2.grid(True, alpha=0.3)

# Subplot 3: Performance Metrics Bar Chart
ax3 = fig.add_subplot(gs[1, 0])
metrics_names = ['Accuracy', 'Precision\n(Weighted)', 'Recall\n(Weighted)', 'F1-Score\n(Weighted)']
precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')
metrics_values = [accuracy, precision, recall, f1]
colors_bars = ['#3498DB', '#2ECC71', '#F39C12', '#9B59B6']

bars = ax3.bar(metrics_names, metrics_values, color=colors_bars, alpha=0.8, edgecolor='black', linewidth=2)
for bar, value in zip(bars, metrics_values):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
            f'{value:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
ax3.set_ylim([0, 1.1])
ax3.set_ylabel('Score', fontsize=12, fontweight='bold')
ax3.set_title('Performance Metrics', fontsize=14, fontweight='bold', pad=10)
ax3.grid(True, alpha=0.3, axis='y')
ax3.set_xticklabels(metrics_names, fontsize=10)

# Subplot 4: Class-wise Performance
ax4 = fig.add_subplot(gs[1, 1])
class_names = ['Fake (Class 0)', 'Real (Class 1)']
precision_per_class = [tn / (tn + fn) if (tn + fn) > 0 else 0,
                       tp / (tp + fp) if (tp + fp) > 0 else 0]
recall_per_class = [tn / (tn + fp) if (tn + fp) > 0 else 0,
                   tp / (tp + fn) if (tp + fn) > 0 else 0]

x = np.arange(len(class_names))
width = 0.35

bars1 = ax4.bar(x - width/2, precision_per_class, width, label='Precision',
               color='#FF7F50', alpha=0.8, edgecolor='black', linewidth=1.5)
bars2 = ax4.bar(x + width/2, recall_per_class, width, label='Recall',
               color='#20B2AA', alpha=0.8, edgecolor='black', linewidth=1.5)

for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

ax4.set_ylabel('Score', fontsize=12, fontweight='bold')
ax4.set_title('Class-wise Performance', fontsize=14, fontweight='bold', pad=10)
ax4.set_xticks(x)
ax4.set_xticklabels(class_names, fontsize=10)
ax4.legend(fontsize=11)
ax4.set_ylim([0, 1.1])
ax4.grid(True, alpha=0.3, axis='y')

plt.suptitle('Enhanced Full Model - Comprehensive Performance Dashboard',
            fontsize=18, fontweight='bold', y=0.98)

plt.savefig(f'{results_dir}/enhanced/comprehensive_metrics_dashboard.png',
            dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "="*80)
print("✅ COMPLETE ANALYSIS FINISHED!")
print("="*80)
print(f"\n📊 4 Visualizations Created and Saved:")
print(f"   1. {results_dir}/enhanced/confusion_matrix_detailed.png")
print(f"      (Side-by-side: Counts + Normalized percentages)")
print(f"   2. {results_dir}/enhanced/confusion_matrix_single.png")
print(f"      (Single matrix with counts and percentages)")
print(f"   3. {results_dir}/enhanced/roc_auc_curve.png")
print(f"      (ROC-AUC Curve with optimal threshold) ⭐")
print(f"   4. {results_dir}/enhanced/comprehensive_metrics_dashboard.png")
print(f"      (Complete 4-panel performance dashboard)")
print("\n📈 Summary Statistics:")
print(f"   Total Images Tested: {total:,}")
print(f"   Correctly Classified: {tp + tn:,} ({(tp+tn)/total*100:.2f}%)")
print(f"   Misclassified: {fp + fn:,} ({(fp+fn)/total*100:.2f}%)")
print("\n🎯 Confusion Matrix Breakdown:")
print(f"   True Negatives (Fake → Fake):   {tn:>6,} ({tn/total*100:>5.2f}%)")
print(f"   False Positives (Fake → Real):  {fp:>6,} ({fp/total*100:>5.2f}%)")
print(f"   False Negatives (Real → Fake):  {fn:>6,} ({fn/total*100:>5.2f}%)")
print(f"   True Positives (Real → Real):   {tp:>6,} ({tp/total*100:>5.2f}%)")
print("\n🔥 ROC-AUC Metrics:")
print(f"   ROC-AUC Score: {roc_auc:.4f}")
print(f"   Model Performance: {performance}")
print(f"   Optimal Threshold: {optimal_threshold:.4f}")
print(f"   TPR at Optimal: {optimal_tpr:.4f}")
print(f"   FPR at Optimal: {optimal_fpr:.4f}")
print("="*80)

"""# R1-Scientific Report (Results Section Data)

1. Training and Validation Performance (Accuracy vs epoch, loss vs epoch)
"""

import matplotlib.pyplot as plt

# Data from training logs
epochs = list(range(1, 21))

# Training loss values
train_loss = [
    0.9240, 0.6125, 0.4506, 0.3564, 0.2813,
    0.2239, 0.1848, 0.1628, 0.1372, 0.1164,
    0.1063, 0.0918, 0.0854, 0.0761, 0.0695,
    0.0646, 0.0601, 0.0475, 0.0495, 0.0436
]

# Validation loss values
val_loss = [
    0.6099, 0.4799, 0.4211, 0.5387, 0.3474,
    0.2021, 0.3804, 0.2947, 0.1927, 0.1466,
    0.1146, 0.7350, 0.1146, 0.1394, 0.1135,
    0.1614, 0.0902, 0.1616, 0.0670, 0.0990
]

# Training accuracy values (converted to percentage)
train_acc = [
    55.93, 69.26, 79.26, 84.36, 88.14,
    90.88, 92.60, 93.48, 94.61, 95.56,
    95.82, 96.57, 96.74, 97.21, 97.43,
    97.58, 97.78, 98.23, 98.18, 98.44
]

# Validation accuracy values (converted to percentage)
val_acc = [
    68.41, 77.15, 79.96, 78.84, 85.30,
    91.83, 82.88, 87.37, 92.25, 94.34,
    95.59, 78.12, 95.64, 94.57, 95.76,
    94.43, 96.47, 94.45, 97.71, 96.82
]

# Set figure with high DPI and large font sizes
plt.figure(figsize=(11, 5), dpi=300)

# Plot Training & Validation Loss
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, 'o-', label="Train Loss", linewidth=2)
plt.plot(epochs, val_loss, 's-', label="Validation Loss", linewidth=2)
plt.xlabel("Epochs", fontsize=14, fontweight='bold')
plt.ylabel("Loss", fontsize=14, fontweight='bold')
plt.title("Training vs Validation Loss", fontsize=14, fontweight='bold')
plt.legend(fontsize=12)
plt.grid(True)

# Plot Training & Validation Accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, 'o-', label="Train Accuracy", linewidth=2)
plt.plot(epochs, val_acc, 's-', label="Validation Accuracy", linewidth=2)
plt.xlabel("Epochs", fontsize=14, fontweight='bold')
plt.ylabel("Accuracy (%)", fontsize=14, fontweight='bold')
plt.title("Training vs Validation Accuracy", fontsize=14, fontweight='bold')
plt.legend(fontsize=12)
plt.grid(True)

# Adjust layout
plt.tight_layout()

# Save the figure at 300 DPI
output_filename = 'training_validation_metrics.png'
plt.savefig(output_filename, dpi=300, bbox_inches='tight')
print(f"✓ Image saved as '{output_filename}' at 300 DPI")
print(f"✓ Please download the file from your notebook environment")

# Show the plot
plt.show()

"""2. Classification Performance Metrics"""

import matplotlib.pyplot as plt
import numpy as np

# Performance metrics
metrics = {
    'Test Accuracy': 0.9779,
    'Test Loss': 0.0618,
    'ROC-AUC': 0.9979,
    'Precision': 0.9781,
    'Recall': 0.9780,
    'F1-Score': 0.9780
}

# Create figure with custom style
fig = plt.figure(figsize=(10, 7), dpi=300)
fig.patch.set_facecolor('#f8f9fa')

ax = plt.gca()
ax.set_facecolor('#ffffff')

# Prepare data (include all metrics)
metric_names = list(metrics.keys())
metric_values = list(metrics.values())

# Create gradient colors
colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(metric_names)))

# Create bars with custom styling
bars = ax.barh(metric_names, metric_values, color=colors,
                edgecolor='black', linewidth=1.5, alpha=0.85)

# Add value labels on bars
for i, (bar, value) in enumerate(zip(bars, metric_values)):
    if value > 0.5:  # For most metrics
        ax.text(value - 0.01, i, f'{value:.4f}',
                va='center', ha='right', fontsize=12,
                fontweight='bold', color='white')
    else:  # For Test Loss (smaller value)
        ax.text(value + 0.005, i, f'{value:.4f}',
                va='center', ha='left', fontsize=12,
                fontweight='bold', color='black')

# Styling
ax.set_xlim(0, 1.02)
ax.set_xlabel('Score', fontsize=14, fontweight='bold')
ax.grid(axis='x', alpha=0.3, linestyle='--')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

plt.tight_layout()

# Save the figure
output_filename = 'model_performance_metrics.png'
plt.savefig(output_filename, dpi=300, bbox_inches='tight', facecolor='#f8f9fa')
print(f"✓ Image saved as '{output_filename}' at 300 DPI")
print(f"✓ Resolution: {14*300} x {6*300} pixels (4200 x 1800)")
print(f"✓ Please download the file from your notebook environment")

plt.show()

"""# Extra Graphs"""

import matplotlib.pyplot as plt
import numpy as np

# Your data
attack_types = ['Gaussian Blur \n(kernel=7x7)', 'JPEG Compression \n(quality=30)',
                'Contrast Adjustment \n(1.5x)', 'Gaussian Noise \n(σ=25)', 'Original \n(No attack)',
                'Image Resizing \n(down 50%, then back up)',
                'Brightness Adjustment \n(1.5x)', 'Zoom (1.2x)']
accuracy = [88.89, 90.28, 88.33, 77.39, 97.79, 95.45, 90.41, 95.41]

# Sort data by accuracy (optional, for better visualization)
sorted_indices = np.argsort(accuracy)
attack_types_sorted = [attack_types[i] for i in sorted_indices]
accuracy_sorted = [accuracy[i] for i in sorted_indices]

# Create figure with specific DPI for publication quality
fig, ax = plt.subplots(figsize=(8, 5), dpi=300)

# Create lollipop chart with enhanced styling
# Plot horizontal lines (stems)
ax.hlines(y=range(len(accuracy_sorted)), xmin=0, xmax=accuracy_sorted,
          color='#2563eb', alpha=0.7, linewidth=2.5)

# Plot circles (lollipop heads) - highlight "Original (No attack)" differently
colors = ['#ef4444' if 'Original' in attack else '#2563eb'
          for attack in attack_types_sorted]
ax.scatter(accuracy_sorted, range(len(accuracy_sorted)),
           c=colors, s=120, zorder=3, alpha=0.9, edgecolors='white', linewidth=1.5)

# Customize the plot with research paper standards
ax.set_yticks(range(len(attack_types_sorted)))
ax.set_yticklabels(attack_types_sorted, fontsize=10, fontfamily='serif')
ax.set_xlabel('Accuracy (%)', fontsize=11, fontweight='bold', fontfamily='serif')
ax.set_ylabel('Attack Types', fontsize=11, fontweight='bold', fontfamily='serif')  # Added Y-axis label
ax.set_title('Robustness Analysis: Model Accuracy Under Different Attack Types',
             fontsize=12, fontweight='bold', pad=15, fontfamily='serif')

# Add subtle grid for better readability
ax.grid(axis='x', linestyle='--', alpha=0.4, linewidth=0.8, color='gray')
ax.set_axisbelow(True)

# Set x-axis limits with some padding
ax.set_xlim(70, 102)

# Add accuracy values at the end of each lollipop
for i, (acc, attack) in enumerate(zip(accuracy_sorted, attack_types_sorted)):
    ax.text(acc + 0.8, i, f'{acc:.2f}%', va='center', fontsize=9,
            fontweight='bold', fontfamily='serif')

# Add a subtle border
for spine in ax.spines.values():
    spine.set_edgecolor('#cccccc')
    spine.set_linewidth(1.2)

# Remove top and right spines for cleaner look
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# Add legend if you highlighted "Original (No attack)"
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor='#2563eb', label='With Attack'),
    Patch(facecolor='#ef4444', label='No Attack')
]
ax.legend(handles=legend_elements, loc='lower right', fontsize=9,
          frameon=True, fancybox=True, shadow=True)

# Adjust layout to prevent label cutoff
plt.tight_layout()

# Save with high quality for publication
plt.savefig('lollipop_chart_research.png', dpi=300, bbox_inches='tight',
            facecolor='white', edgecolor='none')
plt.savefig('lollipop_chart_research.pdf', bbox_inches='tight',
            facecolor='white', edgecolor='none')  # PDF for LaTeX

# Show plot
plt.show()

print("Charts saved as PNG and PDF for research paper use!")

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd

# Your data
models = ['No Seed', 'Seed=41',
          'Seed=52', 'Seed=62']
avg_training_time = [20.3, 21.0, 20.8, 20.8]  # in minutes
total_test_time = [36.67, 38.80, 30.22, 30.21]
avg_time_per_epoch = [1.82, 1.94, 1.51, 1.51]
throughput = [549.92, 515.53, 661.78, 662.10]

# Create a DataFrame for easier manipulation
df = pd.DataFrame({
    'Model': models,
    'Avg Training \nTime/Epoch (min)': avg_training_time,
    'Total Test Time \n(sec)': total_test_time,
    'Avg Time/Image \n(ms)': avg_time_per_epoch,
    'Throughput \n(images/sec)': throughput
})

# Short model names for better display
short_models = ['No Seed', 'Seed=41',
          'Seed=52', 'Seed=62']

# Set publication-quality style
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.size'] = 10


# ============================================================================
# 2. HEATMAP
# ============================================================================
fig, ax = plt.subplots(figsize=(12, 5), dpi=300)

# Prepare data for heatmap (normalize for better color visualization)
heatmap_data = df.set_index('Model').T
heatmap_data.columns = short_models

# Create heatmap with annotations
sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='YlOrRd',
            linewidths=0.5, linecolor='white', cbar_kws={'label': 'Value'},
            ax=ax, square=False)

ax.set_title('Computational Cost Analysis: Seeds Comparison',
             fontweight='bold', fontsize=13, pad=15)
ax.set_xlabel('Model Configuration', fontweight='bold', fontsize=11)
ax.set_ylabel('Performance Metrics', fontweight='bold', fontsize=11)

# Rotate labels
plt.xticks(rotation=0, ha='center')
plt.yticks(rotation=0)

plt.tight_layout()
plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')
plt.savefig('heatmap.pdf', bbox_inches='tight')
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd

# Your data - Updated from Table 11
models = ['VGG16', 'VGG19', 'InceptionV3', 'ResNet50', 'MobileNetV2', 'Proposed Model']
avg_training_time = [20.7, 20.7, 20.7, 20.7, 20.7, 20.3]  # in minutes
total_test_time = [32.36, 27.37, 36.22, 34.46, 46.87, 36.67]  # in seconds
avg_time_per_image = [1.62, 1.37, 1.81, 1.72, 2.34, 1.82]  # in ms
throughput = [618.06, 730.74, 552.18, 580.45, 426.7, 549.92]  # images/second

# Create a DataFrame for easier manipulation
df = pd.DataFrame({
    'Model': models,
    'Avg Training \nTime/Epoch (min)': avg_training_time,
    'Total Test Time \n(sec)': total_test_time,
    'Avg Time/Image \n(ms)': avg_time_per_image,
    'Throughput \n(images/sec)': throughput
})

# Set publication-quality style
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.size'] = 10

# ============================================================================
# HEATMAP with different color scheme
# ============================================================================
fig, ax = plt.subplots(figsize=(12, 5), dpi=300)

# Prepare data for heatmap
heatmap_data = df.set_index('Model').T

# Create heatmap with annotations - Changed colormap
# Options: 'RdYlGn', 'RdYlBu', 'coolwarm', 'viridis', 'plasma', 'Blues', 'Greens', 'PuBuGn'
sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='coolwarm',
            linewidths=0.5, linecolor='white', cbar_kws={'label': 'Value'},
            ax=ax, square=False)

ax.set_title('Computational Cost Analysis: Pre-trained Model Comparison',
             fontweight='bold', fontsize=13, pad=15)
ax.set_xlabel('Model Name', fontweight='bold', fontsize=11)
ax.set_ylabel('Performance Metrics', fontweight='bold', fontsize=11)

# Rotate labels
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

plt.tight_layout()
plt.savefig('pretrained_cost_heatmap.png', dpi=300, bbox_inches='tight')
plt.savefig('pretrained_cost_heatmap.pdf', bbox_inches='tight')
plt.show()

print("Heatmap saved successfully!")

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd

# Your data - Updated from Table 16
models = ['Proposed Model',
          'Without Multiple\nResidual Blocks and\nMulti-Scale Feature\nFusion',
          'Without Squeeze-\nExcitation and\nResidual Block',
          'Without Batch\nNormalization']

avg_training_time = [20.3, 20.3, 20.3, 20.3]  # in minutes
total_test_time = [36.67, 31.74, 28.08, 29.03]  # in seconds
avg_time_per_image = [1.82, 1.59, 1.4, 1.45]  # in ms
throughput = [549.92, 630.06, 712.2, 688.9]  # images/second

# Create a DataFrame for easier manipulation
df = pd.DataFrame({
    'Model': models,
    'Avg Training \nTime/Epoch (min)': avg_training_time,
    'Total Test Time \n(sec)': total_test_time,
    'Avg Time/Image \n(ms)': avg_time_per_image,
    'Throughput \n(images/sec)': throughput
})

# Set publication-quality style
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.size'] = 10

# ============================================================================
# HEATMAP with different color scheme
# ============================================================================
fig, ax = plt.subplots(figsize=(12, 5), dpi=300)

# Prepare data for heatmap
heatmap_data = df.set_index('Model').T

# Create heatmap with annotations - Using 'RdYlGn' colormap
sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdYlGn_r',
            linewidths=0.5, linecolor='white', cbar_kws={'label': 'Value'},
            ax=ax, square=False, annot_kws={'fontsize': 9})

ax.set_title('Computational Cost Analysis: Ablation Study',
             fontweight='bold', fontsize=13, pad=15)
ax.set_xlabel('Model Configuration', fontweight='bold', fontsize=11)
ax.set_ylabel('Performance Metrics', fontweight='bold', fontsize=11)

# Rotate labels for better readability
plt.xticks(rotation=0, ha='center', fontsize=9)
plt.yticks(rotation=0, fontsize=9)

plt.tight_layout()
plt.savefig('ablation_cost_heatmap.png', dpi=300, bbox_inches='tight')
plt.savefig('ablation_cost_heatmap.pdf', bbox_inches='tight')
plt.show()

print("Ablation study heatmap saved successfully!")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Confusion matrix data
data = {
    'TP': [8018, 8014, 8233, 6184, 8723, 9715],
    'FP': [1668, 2146, 1843, 2416, 2060, 156],
    'TN': [8332, 7854, 8157, 7584, 7940, 9844],
    'FN': [1982, 1986, 1767, 3816, 1277, 285]
}

models = ['VGG16', 'VGG19', 'InceptionV3', 'ResNet50', 'MobileNetV2', 'Proposed Model']

# Create DataFrame
df = pd.DataFrame(data, index=models)

# Transpose so models are on X-axis
df_t = df.T

# Set style for research papers
sns.set_style("white")

# Plot heatmap
plt.figure(figsize=(10, 6),dpi=300)
ax = sns.heatmap(
    df_t,
    annot=True,
    fmt='d',
    cmap='Blues',
    #linewidths=0.8,
    linecolor='black',
    annot_kws={'size': 11},
    cbar_kws={'label': 'Number of Samples'}
)

# Titles and labels
plt.title(
    'Confusion Matrix Heatmap ',
    fontsize=16,
    fontweight='bold',
    pad=15
)
plt.xlabel('Model', fontweight='bold', fontsize=14)
plt.ylabel('Performance Metrics', fontweight='bold', fontsize=14)

# Tick formatting
plt.xticks(rotation=30, ha='right', fontsize=12)
plt.yticks(fontsize=12)

# Tight layout for paper formatting
plt.tight_layout()

# Save figure in 300 DPI (recommended for journals)
plt.savefig(
    'CM1.png',
    dpi=300,
    bbox_inches='tight'
)

plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Confusion matrix data
data = {
    'TP': [9715, 9311, 9668, 9713],
    'FP': [156, 46, 319, 441],
    'TN': [9844, 9954, 9681, 9559],
    'FN': [285, 689, 332, 287]
}

models = [
    'Proposed Model',
    'Without Multiscale \nResidual Blocks',
    'Without Squeeze-and\n-Excitation',
    'Without Batch \nNormalization'
]


# Create DataFrame
df = pd.DataFrame(data, index=models)

# Transpose so models are on X-axis
df_t = df.T

# Set style for research papers
sns.set_style("white")

# Plot heatmap
plt.figure(figsize=(10, 6),dpi=300)
ax = sns.heatmap(
    df_t,
    annot=True,
    fmt='d',
    cmap='Blues',
    #linewidths=0.8,
    linecolor='black',
    annot_kws={'size': 11},
    cbar_kws={'label': 'Number of Samples'}
)

# Titles and labels
plt.title(
    'Confusion Matrix Heatmap ',
    fontsize=16,
    fontweight='bold',
    pad=15
)
plt.xlabel('Model', fontweight='bold', fontsize=14)
plt.ylabel('Performance Metrics', fontweight='bold', fontsize=14)

# Tick formatting
plt.xticks(rotation=30, ha='right', fontsize=12)
plt.yticks(fontsize=12)

# Tight layout for paper formatting
plt.tight_layout()

# Save figure in 300 DPI (recommended for journals)
plt.savefig(
    'CM2.png',
    dpi=300,
    bbox_inches='tight'
)

plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Confusion matrix data
data = {
    'TP': [9715, 9545, 9724, 9577],
    'FP': [156, 51, 88, 236],
    'TN': [9844, 9949, 9912, 9764],
    'FN': [285, 455, 276, 423]
}

models = [
    'Proposed Model',
    'Seed-42',
    'Seed-52',
    'Seed-62'
]



# Create DataFrame
df = pd.DataFrame(data, index=models)

# Transpose so models are on X-axis
df_t = df.T

# Set style for research papers
sns.set_style("white")

# Plot heatmap
plt.figure(figsize=(10, 6),dpi=300)
ax = sns.heatmap(
    df_t,
    annot=True,
    fmt='d',
    cmap='Blues',
    #linewidths=0.8,
    linecolor='black',
    annot_kws={'size': 11},
    cbar_kws={'label': 'Number of Samples'}
)

# Titles and labels
plt.title(
    'Confusion Matrix Heatmap ',
    fontsize=16,
    fontweight='bold',
    pad=15
)
plt.xlabel('Configuration', fontweight='bold', fontsize=14)
plt.ylabel('Performance Metrics', fontweight='bold', fontsize=14)

# Tick formatting
plt.xticks(rotation=30, ha='right', fontsize=12)
plt.yticks(fontsize=12)

# Tight layout for paper formatting
plt.tight_layout()

# Save figure in 300 DPI (recommended for journals)
plt.savefig(
    'CM3.png',
    dpi=300,
    bbox_inches='tight'
)

plt.show()

"""# Cryptographic Method"""

pip install opencv-python numpy

import numpy as np
import cv2
import hashlib
import time
import os

# ===================================================
# Session Key Generator (Network-Security Inspired)
# ===================================================
def session_key(img, master_key=123456):
    """
    Generates a dynamic session key from image hash
    """
    h = hashlib.sha256(img.tobytes()).hexdigest()
    return (int(h[:8], 16) ^ master_key) % 256


# ===================================================
# Lightweight DNA-Inspired Arithmetic (Safe)
# ===================================================
def dna_add(a, b):
    """
    Safe modulo-256 addition
    """
    return ((a.astype(np.uint16) + b) % 256).astype(np.uint8)


# ===================================================
# Encryption Function
# ===================================================
def encrypt_rgb(img, master_key=123456):
    """
    Lightweight RGB Image Encryption
    """
    key = session_key(img, master_key)

    R = img[:, :, 0]
    G = img[:, :, 1]
    B = img[:, :, 2]

    # Cross-channel chained diffusion (NOVEL)
    E_R = dna_add(R, key)
    E_G = dna_add(G, E_R)
    E_B = dna_add(B, E_G)

    encrypted = np.stack([E_R, E_G, E_B], axis=2)
    return encrypted


# ===================================================
# Decryption Function
# ===================================================
def decrypt_rgb(enc, master_key=123456):
    """
    Correct inverse operation of encrypt_rgb()
    """
    key = session_key(enc, master_key)

    E_R = enc[:, :, 0]
    E_G = enc[:, :, 1]
    E_B = enc[:, :, 2]

    R = ((E_R.astype(np.int16) - key) % 256).astype(np.uint8)
    G = ((E_G.astype(np.int16) - E_R.astype(np.int16)) % 256).astype(np.uint8)
    B = ((E_B.astype(np.int16) - E_G.astype(np.int16)) % 256).astype(np.uint8)

    decrypted = np.stack([R, G, B], axis=2)
    return decrypted


# ===================================================
# Security Metrics
# ===================================================
def entropy(channel):
    hist = np.histogram(channel.flatten(), bins=256)[0]
    p = hist / np.sum(hist)
    p = p[p > 0]
    return -np.sum(p * np.log2(p))


def correlation(channel):
    x = channel[:, :-1].flatten()
    y = channel[:, 1:].flatten()
    return np.corrcoef(x, y)[0, 1]


def npcr(img1, img2):
    diff = img1 != img2
    return np.sum(diff) / diff.size * 100


def uaci(img1, img2):
    return np.mean(np.abs(
        img1.astype(np.int16) - img2.astype(np.int16)
    ) / 255) * 100


# ===================================================
# MAIN TEST (Single Image)
# ===================================================
if __name__ == "__main__":

    # 🔴 CHANGE THIS PATH
    image_path = "/content/images.png"

    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image not found: {image_path}")

    # Load and convert image
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Encryption
    start = time.time()
    encrypted = encrypt_rgb(img)
    enc_time = time.time() - start

    # Decryption
    start = time.time()
    decrypted = decrypt_rgb(encrypted)
    dec_time = time.time() - start

    # Save results
    cv2.imwrite("encrypted.png", cv2.cvtColor(encrypted, cv2.COLOR_RGB2BGR))
    cv2.imwrite("decrypted.png", cv2.cvtColor(decrypted, cv2.COLOR_RGB2BGR))

    # ===================================================
    # Metrics
    # ===================================================
    print("\n========== PERFORMANCE & SECURITY METRICS ==========")

    print(f"Encryption Time (s): {enc_time:.6f}")
    print(f"Decryption Time (s): {dec_time:.6f}")

    print("\nEntropy (Encrypted Image):")
    print(f"R: {entropy(encrypted[:,:,0]):.4f}")
    print(f"G: {entropy(encrypted[:,:,1]):.4f}")
    print(f"B: {entropy(encrypted[:,:,2]):.4f}")

    print("\nCorrelation (Encrypted Image):")
    print(f"R: {correlation(encrypted[:,:,0]):.6f}")
    print(f"G: {correlation(encrypted[:,:,1]):.6f}")
    print(f"B: {correlation(encrypted[:,:,2]):.6f}")

    print("\nDifferential Attack Metrics:")
    print(f"NPCR (%): {npcr(img, encrypted):.2f}")
    print(f"UACI (%): {uaci(img, encrypted):.2f}")

    # Verification
    if np.array_equal(img, decrypted):
        print("\n✔ Decryption Successful (Perfect Recovery)")
    else:
        print("\n✘ Decryption Failed")
